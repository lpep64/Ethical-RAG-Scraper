{"id": "1j683ag", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/", "author": "thalissonvs", "created_utc": 1741404301, "score": 635, "title": "The library I built because I hate Selenium, CAPTCHAS and my own life", "content": "After countless hours spent automating tasks only to get blocked by Cloudflare, rage-quitting over reCAPTCHA v3 (why is there no button to click?), and nearly throwing my laptop out the window, I built PyDoll. GitHub: It\u2019s not magic, but it solves what matters: - Native bypass for reCAPTCHA v3 & Cloudflare Turnstile (just click in the checkbox). - 100% async \u2013 because nobody has time to wait for requests. - Currently running in a critical project at work (translation: if it breaks, I get fired). FAQ (For the Skeptical): - \u201cIs this illegal?\u201d \u2192 No, but I\u2019m not your lawyer. - \u201cDoes it actually work?\u201d \u2192 It\u2019s been in production for 3 months, and I\u2019m still employed. - \u201cWhy open-source?\u201d \u2192 Because I suffered through building it, so you don\u2019t have to (or you can help make it better). For those struggling with hCAPTCHA, native support is coming soon \u2013 drop a star \u2b50 to support the cause"}
{"id": "mgmmttv", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgmmttv/", "author": "Historical-City-7708", "created_utc": 1741404764, "score": 15, "content": "Wow. Let me test with site which has v3. Does it work in headless mode"}
{"id": "mgnfaby", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnfaby/", "author": "Illustrious_Comb_216", "created_utc": 1741418724, "score": 9, "content": "Is it compatible with Chromium?"}
{"id": "mgnqv95", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnqv95/", "author": "PawsAndRecreation", "created_utc": 1741425959, "score": 9, "content": "Also interested how it differs from nodriver? Looks like based on same tech."}
{"id": "mgnrwe8", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnrwe8/", "author": "whodadada", "created_utc": 1741426636, "score": 6, "content": "I\u2019m a big advocate of open source, thanks for sharing. Just be careful when sharing code you\u2019ve created for a company - be sure you\u2019re not breaching your contract. Code written on company time normally belongs to the company contractually."}
{"id": "mgncdty", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgncdty/", "author": "d0lern", "created_utc": 1741417032, "score": 4, "content": "Can it scrape js powered webpages?"}
{"id": "mgom83j", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgom83j/", "author": "UserOfTheReddits", "created_utc": 1741442322, "score": 4, "content": "Leaving comment here to note this"}
{"id": "mgnofpz", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnofpz/", "author": "pownedjojo", "created_utc": 1741424398, "score": 3, "content": "Thanks. I\u2019ll try it soon"}
{"id": "mgtqr8h", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgtqr8h/", "author": "PM_Me_anything_Bored", "created_utc": 1741510526, "score": 3, "content": "Wow Amazing work dude ! Oe question, Now you have open sourced it don't you think cloudflare and other captcha providers will figure out your way of bypassing it and render your hardwork useless?"}
{"id": "mgnmk5p", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnmk5p/", "author": "Giraffe889", "created_utc": 1741423192, "score": 2, "content": "Thanks man, maybe will use this in future."}
{"id": "mgnpkkx", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnpkkx/", "author": "SEC_INTERN", "created_utc": 1741425123, "score": 2, "content": "What's the difference between this and Nodriver?"}
{"id": "mgt9l99", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgt9l99/", "author": "kofikwakye", "created_utc": 1741500695, "score": 2, "content": "I\u2019ll have to test it on my project, my prayers might probably be answered."}
{"id": "mhqkfb9", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhqkfb9/", "author": "AcedWorld", "created_utc": 1741955772, "score": 2, "content": "How can I simulate pressing the enter key, spacebar and other keys please"}
{"id": "mnkrqa8", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mnkrqa8/", "author": "0xReaper", "created_utc": 1744894260, "score": 2, "content": "Hey, that's a great tool. I have a small suggestion. I see you are using bs4 in the code. If you are very interested in increasing speed, why not have a look at replacing it with Scrapling? Scrapling parsing engine is ~600 times faster than bs4, as the benchmark shows, uses less memory, and offers a very similar API while having a lot more features. This page is about moving from bs4 to Scrapling, a comparison of line by line:"}
{"id": "mgns3v8", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgns3v8/", "author": "ViperAMD", "created_utc": 1741426772, "score": 2, "content": "Just use seleniumbase"}
{"id": "mgmwm8o", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgmwm8o/", "author": "boklos", "created_utc": 1741408968, "score": 1, "content": "Thanks"}
{"id": "mgmwxwz", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgmwxwz/", "author": "InternationalUse4228", "created_utc": 1741409119, "score": 1, "content": "Thanks for sharing"}
{"id": "mgnitj1", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnitj1/", "author": "tysonwjl", "created_utc": 1741420865, "score": 1, "content": "What a bloody legend, I was looking at making something like this shortly for the exact same reasons!"}
{"id": "mgniyfi", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgniyfi/", "author": "openwidecomeinside", "created_utc": 1741420947, "score": 1, "content": "Does this have the ability to output html of the page it loads? I can see it can scrape, what does it output here? Can you specify specific tags only to scrape?"}
{"id": "mgnuyyj", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnuyyj/", "author": "RaiseLopsided5049", "created_utc": 1741428581, "score": 1, "content": "Your code is very clean, I love it !"}
{"id": "mgnvvty", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnvvty/", "author": "None", "created_utc": 1741429144, "score": 1, "content": "[removed]"}
{"id": "mgnz5bx", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnz5bx/", "author": "SteveMatai", "created_utc": 1741431167, "score": 1, "content": "Thanks mate, this looks gold. Can\u2019t wait to give it a run\u2026"}
{"id": "mgnzdbv", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnzdbv/", "author": "FeralFanatic", "created_utc": 1741431303, "score": 1, "content": "What method are you using to bypass ReCaptcha?"}
{"id": "mgo19lr", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo19lr/", "author": "lakot1", "created_utc": 1741432442, "score": 1, "content": "Looks amazing, thanks. Gonna try it!!"}
{"id": "mgo385b", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo385b/", "author": "planetearth80", "created_utc": 1741433591, "score": 1, "content": "Does it support network capture to capture api responses?"}
{"id": "mgo459r", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo459r/", "author": "SykenZy", "created_utc": 1741434117, "score": 1, "content": "Did you check if you can operate X or other social media automatically with it? Maybe create multiple tabs and each operates a social media account"}
{"id": "mgo4abj", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo4abj/", "author": "JCPLee", "created_utc": 1741434195, "score": 1, "content": "Great work!!"}
{"id": "mgo8cqq", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo8cqq/", "author": "None", "created_utc": 1741436338, "score": 1, "content": "[removed]"}
{"id": "mgoadbe", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgoadbe/", "author": "oleksandrb", "created_utc": 1741437323, "score": 1, "content": "That's very cool. Thank you so much for contributing to open source. Amazing job!"}
{"id": "mgomrq7", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgomrq7/", "author": "SerhatOzy", "created_utc": 1741442531, "score": 1, "content": "'Not legal, but I am not your lawyer' Thanks for the script."}
{"id": "mgozk2t", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgozk2t/", "author": "Glad-Bandicoot-8030", "created_utc": 1741447048, "score": 1, "content": "Looks clean. I will try it later."}
{"id": "mgqjjyl", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgqjjyl/", "author": "d0lern", "created_utc": 1741464459, "score": 1, "content": "Whats wrong with webdriver?"}
{"id": "mgri5c6", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgri5c6/", "author": "drose1490", "created_utc": 1741475841, "score": 1, "content": "Gas"}
{"id": "mgs5835", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgs5835/", "author": "Houd_Ammari", "created_utc": 1741484122, "score": 1, "content": "Remindme!"}
{"id": "mgv6fpn", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgv6fpn/", "author": "Scary_Mad_Scientist", "created_utc": 1741534724, "score": 1, "content": "Wow, this is great. I'll give it a try during the week. Specially handy now that some of the most renowned projects that deal with Cloudflare's CAPTCHAS are now abandoned or barely active."}
{"id": "mh6n89i", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mh6n89i/", "author": "ian_k93", "created_utc": 1741691703, "score": 1, "content": "Awesome, will check it out!"}
{"id": "mh6qec8", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mh6qec8/", "author": "junai-", "created_utc": 1741693225, "score": 1, "content": "Awsome!! will try it for cloudfare captcha!"}
{"id": "mhaszlp", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhaszlp/", "author": "LorSt4r", "created_utc": 1741737575, "score": 1, "content": "This looks very gamechanger"}
{"id": "mhc873s", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhc873s/", "author": "Quirky-Dependent-474", "created_utc": 1741757021, "score": 1, "content": "this is dope as hell! i\u2019ve been banging my head against the wall with selenium and those damn captchas too, so I feel your pain bro. Pydoll sounds like a friggin lifesaver native bypass for recaptcha AND cloudflare? AND async? sign me up! gonna check out that github link for sure. props for open-sourcing it too, takes guts to put it out there like that. i\u2019m def dropping a star, can\u2019t wait for that hcaptcha support cuz that ones been kicking my ass lately. keep us posted man, you\u2019re a legend for this!"}
{"id": "mhxsceg", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhxsceg/", "author": "Ok_Map_2755", "created_utc": 1742054384, "score": 1, "content": "How is this vs. nodriver? I'm gonna test out both yours and nodriver and see which I'll end up using in prod."}
{"id": "n6ixvts", "type": "comment", "parent_id": "t3_1j683ag", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/n6ixvts/", "author": "decipher_this", "created_utc": 1754142240, "score": 1, "content": "Very cool, I like how you took control of the technology instead of suffering through another solution. I'm new to the scraping scene but trying to follow along. I set up a basic demo using the pydoll example code from the repo but it gets blocked every time by DataDome on etsy.com. I'm using my fairly good reputation residential IP to connect so it seems like it's pydoll. Not asking you to troubleshoot my setup but I wanted to see if you've had success with datadome protected sites in the past?"}
{"id": "mgp0p47", "type": "comment", "parent_id": "t1_mgmmttv", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgp0p47/", "author": "FeralFanatic", "created_utc": 1741447424, "score": 3, "content": "What was the result?"}
{"id": "mgmnaou", "type": "comment", "parent_id": "t1_mgmmttv", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgmnaou/", "author": "thalissonvs", "created_utc": 1741404955, "score": 5, "content": "Yes! Just tested it on a work project, and it works like a charm."}
{"id": "mgnwvoq", "type": "comment", "parent_id": "t1_mgnfaby", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnwvoq/", "author": "thalissonvs", "created_utc": 1741429766, "score": 4, "content": "yes, it's compatible with any chromium-based browser :)"}
{"id": "mgp0qvr", "type": "comment", "parent_id": "t1_mgnqv95", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgp0qvr/", "author": "FeralFanatic", "created_utc": 1741447440, "score": 2, "content": "I\u2019m curious too"}
{"id": "mgnweln", "type": "comment", "parent_id": "t1_mgnrwe8", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnweln/", "author": "thalissonvs", "created_utc": 1741429466, "score": 15, "content": "I wrote this code outside of working hours, and the company is already aware that the intention has always been to make it open source. In fact, we have a fork within the company with some additional features."}
{"id": "mgnwg43", "type": "comment", "parent_id": "t1_mgncdty", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnwg43/", "author": "thalissonvs", "created_utc": 1741429492, "score": 8, "content": "Yes, you can scrape any kind of webpages"}
{"id": "mgut145", "type": "comment", "parent_id": "t1_mgtqr8h", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgut145/", "author": "thalissonvs", "created_utc": 1741530203, "score": 4, "content": "I don't think giants like Cloudflare and Google will pay attention to a small library haha. But anyway, I can adapt if needed."}
{"id": "mgnwre6", "type": "comment", "parent_id": "t1_mgnpkkx", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnwre6/", "author": "thalissonvs", "created_utc": 1741429691, "score": 1, "content": "I didn't know this library, I'll take a look"}
{"id": "mi6egj2", "type": "comment", "parent_id": "t1_mhqkfb9", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mi6egj2/", "author": "thalissonvs", "created_utc": 1742169491, "score": 1, "content": "Hi! please open an issue, I'll respond you there"}
{"id": "mgnw3mk", "type": "comment", "parent_id": "t1_mgniyfi", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnw3mk/", "author": "thalissonvs", "created_utc": 1741429276, "score": 2, "content": "Yes, it looks like selenium. You can view the output html with page.page_source or element.page_source"}
{"id": "mgny17n", "type": "comment", "parent_id": "t1_mgnuyyj", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgny17n/", "author": "thalissonvs", "created_utc": 1741430490, "score": 1, "content": "Thank's :)"}
{"id": "mgnxyqv", "type": "comment", "parent_id": "t1_mgnvvty", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnxyqv/", "author": "thalissonvs", "created_utc": 1741430449, "score": 3, "content": "But if you don\u2019t want to wait, just do the following: from pydoll.browser.options import Options from pydoll.browser.chrome import Chrome options = Options() options.binary_location = \"/your/path/to/chrome\" browser = Chrome(options=options)"}
{"id": "mgnwjjq", "type": "comment", "parent_id": "t1_mgnvvty", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgnwjjq/", "author": "thalissonvs", "created_utc": 1741429553, "score": 2, "content": "Hi, could you open an issue? I don't have a Mac, so I couldn't implement and test it"}
{"id": "mgoptbi", "type": "comment", "parent_id": "t1_mgnzdbv", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgoptbi/", "author": "thalissonvs", "created_utc": 1741443667, "score": 4, "content": "Both of these captchas measure a score\u2014that is, how human-like your behavior appears. Large tools like Selenium and Playwright are probably required to indicate that automation is being used (which we can see in the flag that appears when using Selenium). A clean implementation on top of CDP, combined with more realistic scripts that simulate clicks with hover, mouse press, mouse release, and all the events of a real user, ensures a high score and, consequently, bypasses the captcha"}
{"id": "mgo9gg6", "type": "comment", "parent_id": "t1_mgo385b", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo9gg6/", "author": "thalissonvs", "created_utc": 1741436879, "score": 2, "content": "yes, you just have to enable: page.enable_network_events(), then, access the logs: page.network_logs"}
{"id": "mgo9ixn", "type": "comment", "parent_id": "t1_mgo459r", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo9ixn/", "author": "thalissonvs", "created_utc": 1741436913, "score": 2, "content": "yes, but you'll have to automate this process"}
{"id": "mgopyfp", "type": "comment", "parent_id": "t1_mgo8cqq", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgopyfp/", "author": "thalissonvs", "created_utc": 1741443721, "score": 2, "content": "Yes, you can :)"}
{"id": "mgr409t", "type": "comment", "parent_id": "t1_mgqjjyl", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgr409t/", "author": "thalissonvs", "created_utc": 1741471073, "score": 1, "content": "It's just very easy to detect by any decent CAPTCHA system, even in patches like undetected_chromedriver."}
{"id": "mgs5c98", "type": "comment", "parent_id": "t1_mgs5835", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgs5c98/", "author": "RemindMeBot", "created_utc": 1741484164, "score": 1, "content": "**Defaulted to one day.** I will be messaging you on [**2025-03-10 01:35:22 UTC**]( to remind you of [**this link**]( [**1 OTHERS CLICKED THIS LINK**]( to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)]( ***** |[^(Info)]( Reminders)]( |-|-|-|-|"}
{"id": "n7d9anq", "type": "comment", "parent_id": "t1_n6ixvts", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/n7d9anq/", "author": "thalissonvs", "created_utc": 1754544857, "score": 1, "content": "I'm working on a few improvements in captcha bypass. A new release is coming soon :)"}
{"id": "mgsju2m", "type": "comment", "parent_id": "t1_mgp0p47", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgsju2m/", "author": "Historical-City-7708", "created_utc": 1741489602, "score": 5, "content": "Works great"}
{"id": "mgo3h55", "type": "comment", "parent_id": "t1_mgnwvoq", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgo3h55/", "author": "Illustrious_Comb_216", "created_utc": 1741433735, "score": 3, "content": "I'll give it a try"}
{"id": "mgplngs", "type": "comment", "parent_id": "t1_mgnweln", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgplngs/", "author": "0x13A0F", "created_utc": 1741453910, "score": 5, "content": "Just be careful, open sourcing a work project that is running in prod is risky, not necessarily for you. because there are people out there (from other companies) constantly monitoring open source projects and writing protections and detections against them."}
{"id": "mgpifu3", "type": "comment", "parent_id": "t1_mgnwg43", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgpifu3/", "author": "DETWOS", "created_utc": 1741452919, "score": 1, "content": "Gamechanger ty"}
{"id": "mhb5ht1", "type": "comment", "parent_id": "t1_mgut145", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhb5ht1/", "author": "Livid-Reality-3186", "created_utc": 1741741656, "score": 1, "content": "Thank you. Can it emulate realistic moves, like mouse moves etc, or this tricks are don't needed? Also, can it work with extension?"}
{"id": "mgp0jcn", "type": "comment", "parent_id": "t1_mgoptbi", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgp0jcn/", "author": "FeralFanatic", "created_utc": 1741447370, "score": 2, "content": "Sounds good! I know the chrome driver usually has a flag set which can be detected. Used to have to use a hex editor to change the value within the binary. Will give this a try. Glad to see that this has the ability to get the cookies."}
{"id": "mgp9hyq", "type": "comment", "parent_id": "t1_mgo9ixn", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mgp9hyq/", "author": "SykenZy", "created_utc": 1741450172, "score": 1, "content": "Great!"}
{"id": "mhbai5l", "type": "comment", "parent_id": "t1_mhb5ht1", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhbai5l/", "author": "thalissonvs", "created_utc": 1741743337, "score": 1, "content": "Yes, it works with extensions Take a look at the readme"}
{"id": "mhdgmpe", "type": "comment", "parent_id": "t1_mhbai5l", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhdgmpe/", "author": "Livid-Reality-3186", "created_utc": 1741782398, "score": 1, "content": "Thank you very much! Can I ask more questions please?"}
{"id": "mhe7m8t", "type": "comment", "parent_id": "t1_mhdgmpe", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhe7m8t/", "author": "thalissonvs", "created_utc": 1741791591, "score": 1, "content": "sure, don't worry"}
{"id": "mhgt9fb", "type": "comment", "parent_id": "t1_mhe7m8t", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhgt9fb/", "author": "None", "created_utc": 1741818788, "score": 1, "content": "[removed]"}
{"id": "mhhk76m", "type": "comment", "parent_id": "t1_mhgt9fb", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhhk76m/", "author": "Gistix", "created_utc": 1741827317, "score": 3, "content": "Just took a deep dive, it seems pydoll launches Chrome with a blank user, meaning all your settings and preferences aren't used/saved. By using add\\_argument you can either: A. specify a path to an Chrome user which contains such extension already installed or maybe already logged into a website. or B. specify an extension folder or whatever file format they accept (like CRX) to load. For both you'll need to use 'Options' to configure the browser: from pydoll.browser.options import Options options = Options() For method A that would be: `options.add_argument('--user-data-dir=C:/YourProfile')` For method B: `options.add_argument('--load-extension=C:/YourExtensionFolderOrFile')` Apply options to your Chrome instance just like in the docs `async with Chrome(options=options) as browser:` Make sure there are no spaces in the path, and maybe use absolute paths as well, good luck!"}
{"id": "mhoi4nl", "type": "comment", "parent_id": "t1_mhhk76m", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhoi4nl/", "author": "None", "created_utc": 1741918398, "score": 1, "content": "[removed]"}
{"id": "mhu2pe8", "type": "comment", "parent_id": "t1_mhoi4nl", "permalink": "https://www.reddit.com/r/webscraping/comments/1j683ag/the_library_i_built_because_i_hate_selenium/mhu2pe8/", "author": "webscraping-ModTeam", "created_utc": 1741994560, "score": 1, "content": "Please review the sub rules"}
{"id": "w1ve97", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/", "author": "lmilano10", "created_utc": 1658137995, "score": 477, "title": "Virgin API consumer vs Chad third-party scraper", "content": ""}
{"id": "ignbpz9", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/ignbpz9/", "author": "the_trentfrazier", "created_utc": 1658155302, "score": 31, "content": "Don't you fucking delete this mods"}
{"id": "igmnwz1", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igmnwz1/", "author": "v_maria", "created_utc": 1658143096, "score": 11, "content": "fucking love it"}
{"id": "ii52scd", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/ii52scd/", "author": "GullibleEngineer4", "created_utc": 1659109588, "score": 5, "content": "I find the private apis of web apps and use them, instead of scraping HTML. I get to have to the best of both worlds that way. Where does that put me?"}
{"id": "igmxm46", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igmxm46/", "author": "chaos_battery", "created_utc": 1658148825, "score": 2, "content": "This graphic is awesome."}
{"id": "igpis7w", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igpis7w/", "author": "amemingfullife", "created_utc": 1658187128, "score": 1, "content": "Scraping can be ethical tho\u2026"}
{"id": "igt8pz9", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igt8pz9/", "author": "taewoo", "created_utc": 1658255652, "score": 1, "content": "FYI, cloudflare is kicking most headless browser scrapers' asses"}
{"id": "ign6nh2", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/ign6nh2/", "author": "interneti", "created_utc": 1658153133, "score": 1, "content": "Lmao"}
{"id": "ignwt16", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/ignwt16/", "author": "SnooChipmunks8648", "created_utc": 1658163737, "score": 1, "content": "It's literally me"}
{"id": "igohn4d", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igohn4d/", "author": "Angrydroid21", "created_utc": 1658171975, "score": 1, "content": "The one time I was a Chad for pay\u2026 do miss it corporate programming gigs are boring in comparison"}
{"id": "igr2rk7", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igr2rk7/", "author": "AndroidePsicokiller", "created_utc": 1658217437, "score": 1, "content": "Hahaha finally... I am the chad :')"}
{"id": "jicj7s8", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/jicj7s8/", "author": "None", "created_utc": 1682886050, "score": 1, "content": "Can anyone explain to me the \"promising career at high-frequency trading firm\" part. How exactly?"}
{"id": "jnzykvj", "type": "comment", "parent_id": "t3_w1ve97", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/jnzykvj/", "author": "Atomkraft98", "created_utc": 1686667828, "score": 1, "content": "This has become really relevant these past few weeks"}
{"id": "igqap3a", "type": "comment", "parent_id": "t1_ignbpz9", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igqap3a/", "author": "stets", "created_utc": 1658199687, "score": 6, "content": "We need to make it the banner tbh"}
{"id": "il708pg", "type": "comment", "parent_id": "t1_ii52scd", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/il708pg/", "author": "Hamzikbande", "created_utc": 1661091174, "score": 3, "content": "How do you find the private apis?"}
{"id": "ih46s1m", "type": "comment", "parent_id": "t1_igpis7w", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/ih46s1m/", "author": "kn_kry", "created_utc": 1658444128, "score": 5, "content": "with 0.0000009 USD per captcha its very ethical in my opinion its not like you have practically slaves clicking im not a robot buttons all day long for like 20 cents idk what are you talking about"}
{"id": "kh6iju8", "type": "comment", "parent_id": "t1_igt8pz9", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/kh6iju8/", "author": "phking1337", "created_utc": 1704874086, "score": 2, "content": "Just become a TLS fingerprint spoofing chad using curl-impersonate"}
{"id": "igtodf6", "type": "comment", "parent_id": "t1_igt8pz9", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igtodf6/", "author": "Certain-Ad827", "created_utc": 1658261538, "score": 1, "content": "Excuse me sir, I am using selenium to scrape most of my data, I dont understand what is wrong with that. When I googled \"cloudflare\" it told me something called 1.1.1.1 and I never heard about it before. And I tried to googled FYI and I found related to webscraping."}
{"id": "igu1jf6", "type": "comment", "parent_id": "t1_igt8pz9", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/igu1jf6/", "author": "the_trentfrazier", "created_utc": 1658266499, "score": 1, "content": "When you say headless are you just referring to user agents? If so that shld be easily remedied I would think"}
{"id": "joqq3tv", "type": "comment", "parent_id": "t1_jicj7s8", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/joqq3tv/", "author": "MaxwellsMilkies", "created_utc": 1687203229, "score": 1, "content": "Trading algorithms can rely on scraped social media data for sentiment analysis or other patterns."}
{"id": "juvamd6", "type": "comment", "parent_id": "t1_jnzykvj", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/juvamd6/", "author": "Robokopf", "created_utc": 1691219724, "score": 1, "content": "Why?"}
{"id": "isbl2fp", "type": "comment", "parent_id": "t1_il708pg", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/isbl2fp/", "author": "FalseStructure", "created_utc": 1665772687, "score": 12, "content": "Chrome devtools -> network -> reload page Then just look through fetch/xhr until you find what you need"}
{"id": "ih4707q", "type": "comment", "parent_id": "t1_ih46s1m", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/ih4707q/", "author": "amemingfullife", "created_utc": 1658444224, "score": 3, "content": "\u2018Scrapes so fast the backend crashes\u2019"}
{"id": "juyhd42", "type": "comment", "parent_id": "t1_juvamd6", "permalink": "https://www.reddit.com/r/webscraping/comments/w1ve97/virgin_api_consumer_vs_chad_thirdparty_scraper/juyhd42/", "author": "Atomkraft98", "created_utc": 1691274982, "score": 1, "content": "Between Reddit closing down their APIs and demanding an exorbitant troll toll for access, and Twitter doing the same"}
{"id": "1k2ttln", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/", "author": "shajid-dev", "created_utc": 1745060864, "score": 478, "title": "I built data scraping AI agents with n8n", "content": ""}
{"id": "mnx9x5u", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnx9x5u/", "author": "OvdjeZaBolesti", "created_utc": 1745069447, "score": 34, "content": "What kind of scraping do you do? This is great for entity relationship extraction (eg. names that appear, organizations and sentiment surrounding them and frequency of their occurrences, used for indexing in FAISS-like or BM25-like algos). This is good enough for named entity information extraction (names of people and information mentioned about them, used for knowledge aggregation over multiple sources, like CIA does in movies). This is actually quite bad for complete content extraction (with data ordering and hierarchy kept). This is more for machine learnings and data science subreddits, but i will explain this here, especially since i learned it the hard way by making a similar pipeline in Python. It seems weird, the complete content extraction should be simpler than the relationship/entity info extraction. And it is, it is SIMPLER, but not EASIER, given you can do the complete data extraction one without LLMs. What is important here is that LLMs are actually the reason similar data extraction pipelines fail - the moment when you replace regex matching and algorithmically relying on HTML tree-like hierarchy with LLMs, the quality drops and information is lost. Models i was using were gpt-4o (not mini), and o1 and o3, all accessed through API, so you can see i brought out the big guns. The later two spent too much tokens on their \"thinking\" (hate being forced to use human-related nomenclature to describe inference and COT algos) so content of the webpage had to be split into three or four parts for them to do their job. The first one worked with the entirety of the text. I used markdownify (preferred) and markitdown (by Microsoft) for data extraction. AND A LOT OF BEAUTIFUL SOUP. I tried both ways - first LLM, then markdown formatting and first mardown fromatting from HTML and then LLMs. First, depending on what markown converter for html you use, it can obscure information. Plus, the markdown formatting is kept (using ** for bold, ## for titles etc), but the data ordering can be improper. If the webpage has a strong reliance on custom types in its HTML code (higher JS reliance, common for drag-and-drop web building apps, which account for a lot of content), the markdown tool can just drop it or place it in weird places. It messes up (technically messes up, you could argue they do exactly what they need to do) the title-description-alt text-src extraction for images so I advise manual formatting either as a part of the preprocessing or postprocessing pipeline. Second, lost in the middle is still a problem. No, needle-in-the-haystock tests that these model builders use to show that it was solved do not test for lost-in-the-middle effect. So data will be dropped. Third, LLM expect ideal conditions. which is funny, because that is actually not the case for almost all webpages. You will have a large, well established webpage, like Forbes or something, and the styles and classes of types in HTML code will not be standardized or ID system will change between pages, and LLM will try to prepare a pipeline for data extraction and be confident it did a good job, but actually fail. Finally, if you pass HTML code to the LLM instead of markdowned text, you are even more screwed. the depth of type nesting (div inside a p inside another div object inside a table) can be wild, and it just confused the machine - it is not all powerful. So it just drops a lot of information, not detecting it. If the programmer preferred to stylize the types inline a lot, and not through classes, which is common for smaller sites and projects (again, smaller sites and projects make 70% of the net), you are passing a huge number of unnecessary tokens to the machine, confusing it a lot. You will notice i approached this with a sentiment you were building a \"one-size-fits-all\" solution for data extraction when you could have been building some custom pipeline for a predefined collection of webpages that works flawlessly for them. I have an example of a more complex webpage that I approached with regex+beautiful soup and did the flawless extraction+formatting (flawlessly in a sense it looks like someone did it by manually rewriting the content into .txt format, which is the gold standard) if you want to compare results and maybe test for weaknesses But, in general, if you plan on selling this as a general service, don't expect much until you add preprocessing and postprocessing steps to every time LLM and markdown are used, given that tools similar to this already exist as python libraries. Good luck, man."}
{"id": "mnww76m", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnww76m/", "author": "unhinged_peasant", "created_utc": 1745063707, "score": 16, "content": "What is exactly scraping with AI? Do you just dump page source codes and ask AI to extract data from the elements? Does it blows API tokens ?"}
{"id": "mnx5oxd", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnx5oxd/", "author": "Practical-Hat-3943", "created_utc": 1745067814, "score": 6, "content": "So you are sending every URL to Gemini for it to extract the data you are looking for, and then a second time to... do what exactly? Sorry, just trying to follow the workflow to learn for myself. Or is it that the first time around you are using Gemini Chat for a different purpose than finding the data you are looking for, and the second time you call Gemini Pro that's when it extracts the data you are looking for? If not, what are the reasons you use one over the other at one specific moment? Also, how many pages per second can you manage with a workflow like this? I'm assuming performance is not the main concern? But I do like the approach since a lot of the code we write end up being boiler code that doesn't really add to the true objective of the algorithm you are implementing, so doing it this way I see how it can save time not only in the implementation (maybe not the first time if you are not familiar with this approach of development, but eventually) but also in the maintenance over time."}
{"id": "mnwrwpp", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnwrwpp/", "author": "None", "created_utc": 1745061546, "score": 15, "content": "[deleted]"}
{"id": "mnwulmg", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnwulmg/", "author": "Stochasticlife700", "created_utc": 1745062929, "score": 3, "content": "How much time did it take? (Overall)"}
{"id": "mnxbrsz", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxbrsz/", "author": "Hot-Carrot-994", "created_utc": 1745070139, "score": 2, "content": "what advantage does AI webscraping have over regular web scraping?"}
{"id": "mnwuw7c", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnwuw7c/", "author": "Still_Steve1978", "created_utc": 1745063075, "score": 2, "content": "I love this idea. Similar to stuff I\u2019ve been (slowly) working on. Are you able to share the n8n workflow? Ps ignore the haters. Some people just don\u2019t seem to want to move with the times, times are changing. Automation, ai, workflows and agents are the now and the future."}
{"id": "mnxmb4j", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxmb4j/", "author": "Vegetable_Sun_9225", "created_utc": 1745073763, "score": 1, "content": "Can you share the actual workflow?"}
{"id": "mny1gsk", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mny1gsk/", "author": "ravindra91", "created_utc": 1745078608, "score": 1, "content": "Which platforms it can scrape?"}
{"id": "mnyknca", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyknca/", "author": "Feisty_Stress_7193", "created_utc": 1745084655, "score": 1, "content": "Can you share the project ?"}
{"id": "mnym8hk", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnym8hk/", "author": "youdig_surf", "created_utc": 1745085136, "score": 1, "content": "I was thinking on something striping html first and using llm to pin point and navigate to the information i wanted but i just learn in the first post it was complicated therefore i wonder if using vision first to pinpoint html element is a better practice ?"}
{"id": "mo18vmg", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo18vmg/", "author": "None", "created_utc": 1745118365, "score": 1, "content": "[removed]"}
{"id": "mo6tm7n", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo6tm7n/", "author": "rednlsn", "created_utc": 1745200364, "score": 1, "content": "why are you google shiting it? why dont you use a proper postgres?"}
{"id": "mof0ffs", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mof0ffs/", "author": "CrashingAtom", "created_utc": 1745323180, "score": 1, "content": "N8N. Woof. Nightmares return. It\u2019s wild that this has 400 upvotes just because the visual looks like how neophytes think coding should look. N8N sucks and I doubt this works, but literally posting what looks complicated makes people amazed so here we are. You just drag and attach and whatever, and boom people are utterly blown away with an unproven flow. Not that you shouldn\u2019t try different ways to get things done, but N8N just proved over a year to be really slow to implement anything. A"}
{"id": "mnwwvxn", "type": "comment", "parent_id": "t3_1k2ttln", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnwwvxn/", "author": "salmanmapkar", "created_utc": 1745064031, "score": 1, "content": "I am sorry but why do you need AI for scraping? I understand visual implementation helps in maintenance, but didnt understand the need of this?"}
{"id": "mnxq40y", "type": "comment", "parent_id": "t1_mnx9x5u", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxq40y/", "author": "shajid-dev", "created_utc": 1745075006, "score": 7, "content": "I totally agree with your points. The knowledge you've shared is very valuable. You're right about the limitations, especially how LLMs struggle with complex HTML structures and the lost-in-the-middle problem. I'm actually using N8N specifically because it lets me implement those crucial pre/post-processing steps you mentioned. My workflows combine HTML parsing with targeted extraction rather than purely relying on LLMs. However, I was using N8N purely for data-enrichment on already scraped data. I'm working towards creating a data-enrichment aggregator website that works on top of the scraped data. This workflow has been working flawlessly, but I'm interested in trying the approach you suggested. I'd be interested in seeing your regex+BS4 approach that achieved that 'manual rewrite' quality. Always looking to improve my pipeline. Thanks! Let's keep in touch. ;)"}
{"id": "mnynec2", "type": "comment", "parent_id": "t1_mnx9x5u", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnynec2/", "author": "Visual-Librarian6601", "created_utc": 1745085482, "score": 3, "content": "We are also using LLM for extraction. We pass markdown to it too. For needle in haystack problem, you can solve it by splitting the markdown into chunks and extract each chunk separately before combining them."}
{"id": "movrf0f", "type": "comment", "parent_id": "t1_mnx9x5u", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/movrf0f/", "author": "britax12", "created_utc": 1745538727, "score": 1, "content": "Maybe you can combine multimodal Gemini for data extraction together with some parsing engine that parses html into structurized data. I believe it could help better"}
{"id": "mnxod16", "type": "comment", "parent_id": "t1_mnww76m", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxod16/", "author": "thecowmilk_", "created_utc": 1745074436, "score": 7, "content": "Well, AI or ML removes the ambiguity of having to scrap by using css selectors or IDs. And yeah most likely will blow API Tokens and will be very, very costly."}
{"id": "mnxhh1q", "type": "comment", "parent_id": "t1_mnww76m", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxhh1q/", "author": "None", "created_utc": 1745072159, "score": 1, "content": "[removed]"}
{"id": "mnxjgm5", "type": "comment", "parent_id": "t1_mnx5oxd", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxjgm5/", "author": "None", "created_utc": 1745072828, "score": 1, "content": "[removed]"}
{"id": "mnws6x4", "type": "comment", "parent_id": "t1_mnwrwpp", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnws6x4/", "author": "shajid-dev", "created_utc": 1745061694, "score": 7, "content": "I chose this visual workflow approach because it's efficient for my specific needs. I can code yes in JS (puppeteer, cheerio), but sometimes visual tools offer faster implementation and easier maintenance for certain projects. Different tools for different situations buddy!"}
{"id": "mnxc67s", "type": "comment", "parent_id": "t1_mnwrwpp", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxc67s/", "author": "No_River_8171", "created_utc": 1745070285, "score": -7, "content": "Coding makes you smarter !"}
{"id": "mnwvn8u", "type": "comment", "parent_id": "t1_mnwulmg", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnwvn8u/", "author": "shajid-dev", "created_utc": 1745063440, "score": 4, "content": "Ahem! it took me less than 1-2 hours maybe, Without understanding what you want to build might consume more time. I was seeing my vision clearly and built on top of that, and like adding extra tahini though. There are lots of combination involves here, understanding of your requirement and n8n plays major role."}
{"id": "mnyv3gz", "type": "comment", "parent_id": "t1_mnxbrsz", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyv3gz/", "author": "Visual-Librarian6601", "created_utc": 1745087919, "score": 3, "content": "For us, it really shines on: 1. Extracting hidden information or structured answer from context (not extracting as it is) 2. Easily enriching from different websites with variable layout, no manual maintenance, won't break on site changes"}
{"id": "mnxqf6u", "type": "comment", "parent_id": "t1_mnxbrsz", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxqf6u/", "author": "shajid-dev", "created_utc": 1745075106, "score": 0, "content": "I say that it's totally depends on your requirement. If the requirement is similar to mine, then yeah obviously AI is better in this case, or otherwise, better go with regular or advanced web scraping with Python or bS"}
{"id": "mnwvv0c", "type": "comment", "parent_id": "t1_mnwuw7c", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnwvv0c/", "author": "None", "created_utc": 1745063544, "score": 1, "content": "[removed]"}
{"id": "mnxneiy", "type": "comment", "parent_id": "t1_mnxmb4j", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxneiy/", "author": "shajid-dev", "created_utc": 1745074121, "score": 1, "content": "you mean clear version eh"}
{"id": "mnyk8ck", "type": "comment", "parent_id": "t1_mny1gsk", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyk8ck/", "author": "None", "created_utc": 1745084526, "score": 1, "content": "[removed]"}
{"id": "mnyvnrw", "type": "comment", "parent_id": "t1_mnym8hk", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyvnrw/", "author": "Visual-Librarian6601", "created_utc": 1745088101, "score": 2, "content": "Html -> markdown -> llm is the common practice (llm is well trained on markdown data). Vision most of the time is not necessary for simple webpages. It really shines on PDF tables and sometimes handling interactions."}
{"id": "mo1xpfz", "type": "comment", "parent_id": "t1_mo18vmg", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo1xpfz/", "author": "shajid-dev", "created_utc": 1745131123, "score": 2, "content": "Nah, It will not happen as long as you control your billings, I think I have extracted around hundred thousands of data, and costs me around less than a $2 though. However, Workflow is optimized btw. I'm stating the fact that is controllable."}
{"id": "mof9za2", "type": "comment", "parent_id": "t1_mof0ffs", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mof9za2/", "author": "shajid-dev", "created_utc": 1745326906, "score": 0, "content": "lol oh is it burning for you though? DUDE! Listen here. You can criticize and that's your opinion, and I'm open to as well, perhaps you should know that the workflow I built is actually working fine. You think you wanted to code, and make it happen, ah yes. You can go ahead show what you can do though. Ahh! 400 Upvotes, do you think this will make me money? NOPE! just a validation for myself. THAT'S IT. I think you're a quite beginner to understand. I can create with BS4, N8N, Make, or zapier. You know what, I can build an entire system same as N8N or zapier through coding by choosing the correct programming language. It is time consuming, END OF THE DAY! I wanted to minimize my time and earn money lol. I knew, people like you getting burnt badly like a script kiddie. uk what i meant there? AND PERIOD. BYE! :)"}
{"id": "mnxo9ku", "type": "comment", "parent_id": "t1_mnwwvxn", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxo9ku/", "author": "shajid-dev", "created_utc": 1745074405, "score": 1, "content": "It's totally depends on the project requirement tbh. I was needed the data, and the dead-line of this project not seems cooperative when I hard-coded everything, Instead I use n8n, and created appropriate nodes to make things happen. I use AI to get helped in certain sub-tasks. It's much easier tbh, but everything possible with correct prompt engineering otherwise you will get into a rabbit hole though. that's a minus from AI side."}
{"id": "mo1w6o6", "type": "comment", "parent_id": "t1_mnynec2", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo1w6o6/", "author": "shajid-dev", "created_utc": 1745130203, "score": 1, "content": "Aye, This is a good approach btw."}
{"id": "mnxra1k", "type": "comment", "parent_id": "t1_mnxod16", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxra1k/", "author": "None", "created_utc": 1745075382, "score": 6, "content": "[deleted]"}
{"id": "mnzz3tc", "type": "comment", "parent_id": "t1_mnxhh1q", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnzz3tc/", "author": "webscraping-ModTeam", "created_utc": 1745101410, "score": 2, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mnzz4cc", "type": "comment", "parent_id": "t1_mnxjgm5", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnzz4cc/", "author": "webscraping-ModTeam", "created_utc": 1745101415, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mo0a5zc", "type": "comment", "parent_id": "t1_mnws6x4", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo0a5zc/", "author": "Twenty8cows", "created_utc": 1745105283, "score": 1, "content": "What website is this?"}
{"id": "mnxhpkb", "type": "comment", "parent_id": "t1_mnxc67s", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnxhpkb/", "author": "shajid-dev", "created_utc": 1745072240, "score": 2, "content": "Hmm yeah but time is money :) Even for coding, you need to think, but you can invest on something that does the job means you need to play smart and wise. Vibe coding is not my thing, but yeah! I would do the same workflow in JS though, thinking of it."}
{"id": "n06vr8c", "type": "comment", "parent_id": "t1_mnwvn8u", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/n06vr8c/", "author": "Infinite-Ask5534", "created_utc": 1751085678, "score": 1, "content": "fake news prob took you the whole day. If it only took 1-2 hours you would know how many hours"}
{"id": "mnx9757", "type": "comment", "parent_id": "t1_mnwvv0c", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnx9757/", "author": "webscraping-ModTeam", "created_utc": 1745069171, "score": 2, "content": "Please review the sub rules"}
{"id": "mny7sb7", "type": "comment", "parent_id": "t1_mnxneiy", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mny7sb7/", "author": "Vegetable_Sun_9225", "created_utc": 1745080625, "score": 1, "content": "Kinda hard to replicate using just that image"}
{"id": "mnzz5di", "type": "comment", "parent_id": "t1_mnyk8ck", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnzz5di/", "author": "webscraping-ModTeam", "created_utc": 1745101425, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mo1y9c1", "type": "comment", "parent_id": "t1_mo1xpfz", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo1y9c1/", "author": "None", "created_utc": 1745131455, "score": 1, "content": "[removed]"}
{"id": "mofgmqp", "type": "comment", "parent_id": "t1_mof9za2", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mofgmqp/", "author": "CrashingAtom", "created_utc": 1745329236, "score": 0, "content": "No need, my point is that the validation is the sad part. And that people who don\u2019t know what they\u2019re looking at might be impressed, but you literally just posted for the updoots. It\u2019s fine, nobody cares. Just stands out as kinda funny."}
{"id": "mo6v34p", "type": "comment", "parent_id": "t1_mo1w6o6", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo6v34p/", "author": "rednlsn", "created_utc": 1745200884, "score": 1, "content": "I actually dont think the llm hallucinates with html. In a recent research i made, i had a benchmark saying that the better the model, the less differences in processing different structures like html, xml, json or markdown. Also, some sources stated that xml and html are better for structuring complex data, while markdown is yet simpler. Also, notice that if you have a prompt that mixes instructions and with some content in markdown format, you need a good separator to disguise one from the other. Otherwise, the markdown content can be interpreted as an instruction."}
{"id": "mnyud6w", "type": "comment", "parent_id": "t1_mnxra1k", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyud6w/", "author": "Visual-Librarian6601", "created_utc": 1745087680, "score": 0, "content": "The cost depends on model use. If using a cost effectively model like gpt4o-mini, the cost is like $0.3 per 1000 pages (assuming \\~800 tokens or 600 words per page). I also agree that for pages with static layout and css selectors, AI is not necessary here. But in our case, it really shines when: 1. Extracting hidden information or structured answer from context (not extracting as it is) 2. Enriching from different websites with variable layout"}
{"id": "mnyuptp", "type": "comment", "parent_id": "t1_mnxhpkb", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyuptp/", "author": "Visual-Librarian6601", "created_utc": 1745087795, "score": 2, "content": "\\+1 maintaining a manual scraping pipeline takes time and effort that can be spent also on automation and data enrichment if the goal is to prototype and find value"}
{"id": "mo2ihuc", "type": "comment", "parent_id": "t1_mnxhpkb", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo2ihuc/", "author": "No_River_8171", "created_utc": 1745144319, "score": 0, "content": "I Know time is Money but let me Tell you something: I have Scraped 3 Websites got over 3560 Objekts that im gon hardcode into Videos with pure Math Crazy Right ! Meaning i will make over 3000 Videos and turn them into content that i will again automatically Upload thrue the year while i can Focus on Stocks , Girls and Music All with cost of Knowledge and wisdom Now i can do that with any Kind of content because i know the Math and structure behind a Video for intertainment Ps: \u2014\u2014 my last Paycheck was 3751,77 $ No Money have Been Spended for this project All the Cash i did will be used for Promotions and follower ingagment I know time is Money Thats why i Code \u200d And automate my Job"}
{"id": "n0q8ko3", "type": "comment", "parent_id": "t1_n06vr8c", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/n0q8ko3/", "author": "shajid-dev", "created_utc": 1751361002, "score": 1, "content": "How much time did it take to build this workflow? and I have given the transparent answer, explicitly mentioned that I have got the vision and understanding. without those, yeah It could be like couple of days maybe."}
{"id": "n06vu0u", "type": "comment", "parent_id": "t1_mny7sb7", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/n06vu0u/", "author": "Infinite-Ask5534", "created_utc": 1751085717, "score": 1, "content": "\"no my n8n workflow is too good too share with u peasants\""}
{"id": "mo1zh6a", "type": "comment", "parent_id": "t1_mo1y9c1", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo1zh6a/", "author": "shajid-dev", "created_utc": 1745132185, "score": 1, "content": "I use various models, inclusing gemini 1.5, 2.5 PRO, claude for different tasks. I think you can check it out in there section though. idk why I cant explicitly mention a name that gets something like a mod message."}
{"id": "n06w0az", "type": "comment", "parent_id": "t1_mofgmqp", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/n06w0az/", "author": "Infinite-Ask5534", "created_utc": 1751085802, "score": 1, "content": "lol updoots = redditcoin = real money. get with the times get or get lost in the dust before i give you a whirlie. Lol"}
{"id": "mofk952", "type": "comment", "parent_id": "t1_mofgmqp", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mofk952/", "author": "shajid-dev", "created_utc": 1745330418, "score": 0, "content": "haha. Looking up at a building, whether tall or short, can be impressive. The person gazing upward might be a good architect or just an ordinary person, that's all there is to it. If others find this behavior funny, it may indicate a serious mental issue on their part. So! No comments, simply waste. OR else, I would rather say build and show us."}
{"id": "mo6vjyz", "type": "comment", "parent_id": "t1_mo6v34p", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo6vjyz/", "author": "rednlsn", "created_utc": 1745201051, "score": 1, "content": "Also, if you want to have an LLM to process html from a web page, you can cleanup scripts, styles, img, svg, html props and other data that is meaningless against text content and page structure."}
{"id": "mnzlgz4", "type": "comment", "parent_id": "t1_mnyud6w", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnzlgz4/", "author": "Unlikely_Track_5154", "created_utc": 1745096693, "score": 1, "content": "I think I have been doing the second one, not really sure, but using the 2 websites html from the same page ( say Elon musk bio) and using the llm to find what is common among pages what is missing and getting a way to make the bio into one big bio so to speak and then cross referencing ( I think that is what you call it) the data that is the same."}
{"id": "mnyv06i", "type": "comment", "parent_id": "t1_mnyuptp", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mnyv06i/", "author": "shajid-dev", "created_utc": 1745087889, "score": 1, "content": "Exactly. When it comes to enriching the scraped data, May god bless us all."}
{"id": "mo2j0lf", "type": "comment", "parent_id": "t1_mo2ihuc", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mo2j0lf/", "author": "No_River_8171", "created_utc": 1745144648, "score": 1, "content": "And now the hacking part I builded a Rat using fireware api of Google A fishing Location Tracker Page Man i love coding"}
{"id": "mofl7jr", "type": "comment", "parent_id": "t1_mofk952", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mofl7jr/", "author": "CrashingAtom", "created_utc": 1745330728, "score": 0, "content": "N8N stinks, we gave up on it last year because it\u2019s so time consuming. And you\u2019re comparing yourself to an architect of great buildings? lol wtf"}
{"id": "mofnp4g", "type": "comment", "parent_id": "t1_mofl7jr", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mofnp4g/", "author": "None", "created_utc": 1745331515, "score": 0, "content": "[removed]"}
{"id": "mofp5a3", "type": "comment", "parent_id": "t1_mofnp4g", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mofp5a3/", "author": "None", "created_utc": 1745331962, "score": 0, "content": "[removed]"}
{"id": "mofqqxf", "type": "comment", "parent_id": "t1_mofp5a3", "permalink": "https://www.reddit.com/r/webscraping/comments/1k2ttln/i_built_data_scraping_ai_agents_with_n8n/mofqqxf/", "author": "None", "created_utc": 1745332452, "score": 0, "content": "[removed]"}
{"id": "1kqhucx", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/", "author": "Firstboy11", "created_utc": 1747676221, "score": 401, "title": "How do big companies like Amazon hide their API calls", "content": "Hello, I am learning web scrapping and tried beautifulsoup and selenium to scrape. With bot detection and resources, I realized they aren't the most efficient ones and I can try using API calls instead to get the data. I, however, noticed that big companies like Amazon hide their API calls unlike small companies where I can see the JSON file from the request. I have looked at a few post, and some mentioned about encryption. How does it work? Is there any way to get around this? If so, how do I do that? I would appreciate if you could also point me out to any articles to improve my understanding on this matter. Thank you."}
{"id": "mt5p38j", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt5p38j/", "author": "AndiCover", "created_utc": 1747676795, "score": 94, "content": "Probably server side rendering. The frontend server does the API call and provides the rendered HTML to the client."}
{"id": "mt5ucii", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt5ucii/", "author": "HermaeusMora0", "created_utc": 1747678303, "score": 13, "content": "JS or WASM. Look at the sources on the Dev Tools, you'll probably see something under WASM or a bunch of minified/obfuscated JS code, usually it's what will generate anti-bot tokens that will be used somewhere as a cookie or in the payload. For example, Cloudflare UAM does a JS challenge that outputs a string. The string is used in the cf_clearance cookie. So, if you'd wish to generate the string in-house, without a browser, you'd need to understand the heavily obfuscated JS and generate the string yourself. The bigger the site, the harder it is to do that."}
{"id": "mt5p4si", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt5p4si/", "author": "None", "created_utc": 1747676807, "score": 22, "content": "[removed]"}
{"id": "mtb3lbm", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtb3lbm/", "author": "ScraperAPI", "created_utc": 1747753953, "score": 6, "content": "Most e-commerce websites use SSR (Server-Side Rendering), as it makes their websites faster and ensures that all pages can be indexed by Google. If you use Chrome DevTools, you\u2019ll notice that product pages typically don\u2019t make any API calls, except for those related to traffic tracking and analytics tools. Therefore, if you need data from Amazon, the easiest method is to scrape the raw HTML and parse it. If you really want to use their internal APIs, you might be able to intercept them by logging all the API calls made by the Amazon mobile app. Since apps can't use server-side rendering, you'll likely find the API calls you need there. Hope this helps!"}
{"id": "mt5tqyq", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt5tqyq/", "author": "vinilios", "created_utc": 1747678131, "score": 9, "content": "encryption makes things more complex and harder to mimic client behaviour but it's not a way to hide an api endpoint and client calls to that endpoint. A common pattern that indirectly hides access to raw, and formally structured endpoints, is backend for frontend. See here for more details,"}
{"id": "mt9hjuc", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9hjuc/", "author": "None", "created_utc": 1747729493, "score": 1, "content": "[removed]"}
{"id": "mt9inb2", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9inb2/", "author": "chautob0t", "created_utc": 1747730211, "score": 1, "content": "Everything is SSR since inception, at least for the website and most of the mobile app. Very few calls are Ajax calls from the browser. That said, we have millions of bot requests everyday. I assumed all of them scrape the details from the frontend."}
{"id": "mtf2jlp", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtf2jlp/", "author": "divided_capture_bro", "created_utc": 1747800093, "score": 1, "content": "Depending on your use case you can just use GET requests."}
{"id": "mtimlqc", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtimlqc/", "author": "Technical-General578", "created_utc": 1747851858, "score": 1, "content": "Modern frontend application leverage server side rendering"}
{"id": "mto9lgq", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mto9lgq/", "author": "Andriyo", "created_utc": 1747928963, "score": 1, "content": "Amazon website is essentially 90s tech where the server produces complete HTML that includes data being rendered. All API calls or whatever is needed to get the data for page happens on the server side."}
{"id": "mtzw5c8", "type": "comment", "parent_id": "t3_1kqhucx", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtzw5c8/", "author": "reosanchiz", "created_utc": 1748088813, "score": 1, "content": "Use PHP ![gif](giphy|FZaNpQrCtyQms)"}
{"id": "mt7439s", "type": "comment", "parent_id": "t1_mt5p38j", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7439s/", "author": "caprica71", "created_utc": 1747692136, "score": 20, "content": "Amazon are heavy on serverside rendering. It is why their site performs so well"}
{"id": "mt9p9rc", "type": "comment", "parent_id": "t1_mt5p38j", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9p9rc/", "author": "ErikThiart", "created_utc": 1747734428, "score": 1, "content": "and still they don't seem to support PHP well especially in lamdas"}
{"id": "mtg4oj8", "type": "comment", "parent_id": "t1_mt5p38j", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtg4oj8/", "author": "ign1000", "created_utc": 1747821869, "score": 1, "content": "Yes for GET methods this is the way. But you can still see POST endpoints"}
{"id": "mtfoswo", "type": "comment", "parent_id": "t1_mt5ucii", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtfoswo/", "author": "None", "created_utc": 1747811934, "score": 3, "content": "I may be misunderstanding the post, but how does that hide the network calls? Afaik if you do a network call it WILL show up in dev tools regardless if you use wasm or not. I believe it\u2019s way simpler than that, they\u2019re just doing SSR."}
{"id": "mtkogm3", "type": "comment", "parent_id": "t1_mt5ucii", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtkogm3/", "author": "finah1995", "created_utc": 1747874609, "score": 1, "content": "Yeah also Web Socket can be used like when using .net and Blazor with Blazor Server option."}
{"id": "mto7de1", "type": "comment", "parent_id": "t1_mt5ucii", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mto7de1/", "author": "A_parisian", "created_utc": 1747928332, "score": 1, "content": "I remember scraping google maps like 8 years ago and regex was the only practical way to pull data and surprisingly it worked very well for a while to my surprise. Oddly enough that put me on track to find out about their spatial index (S2) which was not really well known back then apart from a few specialists and that opened a lot of new perspectives. Scrapping lets you stumble on plenty of amazing stuff and reverse engineering is really stimulating especially on hardened targets."}
{"id": "mt7aivu", "type": "comment", "parent_id": "t1_mt5p4si", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7aivu/", "author": "someonesopranos", "created_utc": 1747694363, "score": 3, "content": "I inspected again and yes it is server side rendered. I made a small script where extracting product information by chrome extension. For something scalable needed to work with api (canopy) or needed build puppeteer workflow. The repo: ["}
{"id": "mt75gkb", "type": "comment", "parent_id": "t1_mt5p4si", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt75gkb/", "author": "webscraping-ModTeam", "created_utc": 1747692603, "score": 0, "content": "Please review the sub rules"}
{"id": "mtbeare", "type": "comment", "parent_id": "t1_mtb3lbm", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtbeare/", "author": "ChaoticShadows", "created_utc": 1747757090, "score": 2, "content": "Could you explain \"scrape the raw html and parse it\"? I understand getting the raw html (scraping). I'm not sure what you mean, in this context, by parsing it. An example would be helpful."}
{"id": "mt9wcad", "type": "comment", "parent_id": "t1_mt9hjuc", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9wcad/", "author": "webscraping-ModTeam", "created_utc": 1747738327, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mtuvsuu", "type": "comment", "parent_id": "t1_mt9inb2", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtuvsuu/", "author": "ai-tacocat-ia", "created_utc": 1748016584, "score": 1, "content": "I haven't seen a literal AJAX (Asynchronous JavaScript And XML) request in probably a decade."}
{"id": "mu0bnkn", "type": "comment", "parent_id": "t1_mtimlqc", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mu0bnkn/", "author": "RRumpleTeazzer", "created_utc": 1748094771, "score": 2, "content": "how is this modern? this used to be the case for e.g. PHP or ASP."}
{"id": "mt7hok8", "type": "comment", "parent_id": "t1_mt7439s", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7hok8/", "author": "None", "created_utc": 1747696856, "score": 62, "content": "[deleted]"}
{"id": "mt7gp44", "type": "comment", "parent_id": "t1_mt7439s", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7gp44/", "author": "Consibl", "created_utc": 1747696515, "score": 3, "content": "I\u2019ve never used SSR \u2014 wouldn\u2019t it make a site slower?"}
{"id": "mt9c8eb", "type": "comment", "parent_id": "t1_mt7439s", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9c8eb/", "author": "None", "created_utc": 1747726083, "score": 3, "content": "[deleted]"}
{"id": "mtgadvh", "type": "comment", "parent_id": "t1_mtg4oj8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtgadvh/", "author": "AndiCover", "created_utc": 1747824923, "score": 1, "content": "Works also with POST."}
{"id": "mtbo1eu", "type": "comment", "parent_id": "t1_mtbeare", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtbo1eu/", "author": "DOMNode", "created_utc": 1747759953, "score": 3, "content": "Parsing means extracting the data from the DOM. For example Get the list of products: `const productElements = document.querySelectorAll('.product-list-item')` Extract the product name: `const productNames = [...productElements].map(element=>element.innerText)`"}
{"id": "mtsrs7y", "type": "comment", "parent_id": "t1_mtbeare", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtsrs7y/", "author": "fr3nch13702", "created_utc": 1747986793, "score": 1, "content": "Or beautifulsoup for python. Most languages have a dom parser."}
{"id": "mtuw9j8", "type": "comment", "parent_id": "t1_mtuvsuu", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtuw9j8/", "author": "chautob0t", "created_utc": 1748016717, "score": 1, "content": "At least for Amazon, they're pretty common. Just click on a product variation like a different colour or size etc and see the network tab on the detail page. Plus tons of calls for logging metrics etc."}
{"id": "mt9y3ku", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9y3ku/", "author": "barmz75", "created_utc": 1747739176, "score": 19, "content": "As a boomer dev I\u2019m just starting to discover that a whole new generation of devs assume everything is client side with APIs. Terrifying."}
{"id": "mt7txny", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7txny/", "author": "HelloWorldMisericord", "created_utc": 1747701243, "score": 8, "content": "I loved SHTML and was a master at it back in the day (not that it was particularly complex or difficult language)."}
{"id": "mtdcl92", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtdcl92/", "author": "flippakitten", "created_utc": 1747777795, "score": 6, "content": "I'm just waiting for them to discover you can host hundreds of sites on a \u00a35 lamp stack and each app will will function 100% the same. If one app grows, put it on its own server. If it's a unicorn, then you can dockerize it. P.s. I'm a rails developer but my routes are php."}
{"id": "mt7jxni", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7jxni/", "author": "recursing_noether", "created_utc": 1747697635, "score": 2, "content": "Yeah with templates"}
{"id": "mtmqxnv", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtmqxnv/", "author": "_MrJamesBomb", "created_utc": 1747909858, "score": 2, "content": "I agree that specific patterns are reused repeatedly, but to the uninformed, it seems revolutionary. The best examples are HTML and CSS in JS, as in React. I am still undergoing heavy PTSD flashes, coming from PHP 3 and 4, where you mixed and matched everything and called it a day. Even here, the parallel between JS and PHP is striking: PHP went into strict mode, stopped being a dynamically typed language, and aspired to become type-safe. At the same time, JS had to undergo the same exorcism by cloaking it in the god-send TypeScript. In conclusion, we have also repeatedly used the exact solutions. PHP became massively cluttered, and the same goes for the once versatile JS language standard. JS is massively bloated, like its predecessors down this road have been and still are: Java, PHP, and C#. John Resig's book \"JavaScript Ninja\" was mindblowing, but you can only understand its magic if you consider JS ES5. JS ES5 is like assembler/c. Under the hood, it still is."}
{"id": "mtqwgrr", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtqwgrr/", "author": "DocHolligray", "created_utc": 1747957585, "score": 2, "content": "Until I read your post, I legit thought I was having a Mandela effect type moment."}
{"id": "mtcfft0", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtcfft0/", "author": "fftommi", "created_utc": 1747767837, "score": 1, "content": "I LOVE HYPERMEDIA"}
{"id": "mtvalg9", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtvalg9/", "author": "biocin", "created_utc": 1748020793, "score": 1, "content": "Oh they call it server side rendering now. I am old."}
{"id": "mu06rpz", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mu06rpz/", "author": "gwawr", "created_utc": 1748093022, "score": 1, "content": "Except partials and islands and asynchronous loading weren't so much of a thing back then it was mainly one round trip, generating html with a bunch of perl and cgi"}
{"id": "mu1pgzs", "type": "comment", "parent_id": "t1_mt7hok8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mu1pgzs/", "author": "halfxdeveloper", "created_utc": 1748110679, "score": 1, "content": "Preach. I am amazed at how little developers know about how computer systems actually work."}
{"id": "mt7onb6", "type": "comment", "parent_id": "t1_mt7gp44", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7onb6/", "author": "NexusBoards", "created_utc": 1747699297, "score": 9, "content": "No, it does make it faster. When a user visits the website, instead of downloading for example the whole of react, all the dependancies installed with react and then making an api call to get the pages data, a server Amazon owns will do all that then only send the already built html, a far smaller download for the user when they visit the site."}
{"id": "mt8s48j", "type": "comment", "parent_id": "t1_mt7gp44", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt8s48j/", "author": "nagol22", "created_utc": 1747715011, "score": 5, "content": "I work in this field and manage server infrastructure like this serving web traffic, for large sites it goes: content management server --> rendering servers --> cache servers 1 --> load balancer(s) --> cache servers 2 (Cloud Distribution Network or CDN) --> Web Firewall The initial page load from any user will hit the rendering layer which is slower but then be cached for all other users and be very fast. Cache can be controlled by a number of different mechanisms for example request headers such that unique pages can be rendered and cached by region or any other information that may be known about the visitor."}
{"id": "mta5n1f", "type": "comment", "parent_id": "t1_mt7gp44", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mta5n1f/", "author": "angelarose210", "created_utc": 1747742527, "score": 1, "content": "It's bad for seo purposes."}
{"id": "mtdgw6h", "type": "comment", "parent_id": "t1_mt7gp44", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtdgw6h/", "author": "vcaiii", "created_utc": 1747779190, "score": 1, "content": "it shifts the compute & network burden from the user to the server"}
{"id": "mt9ibse", "type": "comment", "parent_id": "t1_mt9c8eb", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9ibse/", "author": "caprica71", "created_utc": 1747729999, "score": 3, "content": "McMaster Carr is one of the fastest websites on the planet. It runs on ASP and uses serverside rendering."}
{"id": "mt9ix4z", "type": "comment", "parent_id": "t1_mt9c8eb", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9ix4z/", "author": "SIntLucifer", "created_utc": 1747730387, "score": 1, "content": "Well that depends on the hardware that is used by the user. I recently did some test and while on my hardware a csr page is loaded faster the moment i start throttling my pc they are almost the same. Also that comparison is mostly made against older SSR websites that load in all the JS and CSS and not only the necessary code you would get by using frameworks like vue/react/etc. But then there is something like AstroJS that doenst ship JS by default to the client and only send the necessary files needed for that page."}
{"id": "mtuxvzl", "type": "comment", "parent_id": "t1_mtuw9j8", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtuxvzl/", "author": "ai-tacocat-ia", "created_utc": 1748017187, "score": 2, "content": "Not saying I don't see API calls all the time. Was just a lighthearted ribbing for showing your age when you called it AJAX - which isn't actually a thing in modern JavaScript. AJAX was a hack we used back in the day when browsers didn't natively support fetch and JSON hadn't fully gained popularity. Later we'd use the same hack to pull json - but mostly leveraging jQuery. Then browsers started catching up (thanks, Chrome) and we didn't have to make janky-ass ajax calls except to support super old browsers like IE 6."}
{"id": "mtf6hyk", "type": "comment", "parent_id": "t1_mt9y3ku", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtf6hyk/", "author": "PM_ME_ALL_YOUR_THING", "created_utc": 1747801934, "score": 4, "content": "But where would you push the flash?"}
{"id": "mtogh3a", "type": "comment", "parent_id": "t1_mt9y3ku", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtogh3a/", "author": "broccollinear", "created_utc": 1747930975, "score": 1, "content": "Why make the trip to the server at all? Can\u2019t we just download a copy of the server to the client and then you\u2019ll have frictionless access."}
{"id": "mtlfa36", "type": "comment", "parent_id": "t1_mt7txny", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtlfa36/", "author": "aplarsen", "created_utc": 1747884106, "score": 2, "content": "Yeah, SSI on Apache was awesome."}
{"id": "mtm8ls4", "type": "comment", "parent_id": "t1_mtdcl92", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtm8ls4/", "author": "narasadow", "created_utc": 1747898827, "score": 1, "content": "![gif](giphy|1NVugSXiJGJZvWMOud)"}
{"id": "mt7wt55", "type": "comment", "parent_id": "t1_mt7onb6", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt7wt55/", "author": "Infamous_Land_1220", "created_utc": 1747702305, "score": 3, "content": "That\u2019s on the first visit tho, doesn\u2019t the stuff get cached and there are no subsequent downloads of react or any other libraries since now this info is cached on user side?"}
{"id": "mtwpauk", "type": "comment", "parent_id": "t1_mt7onb6", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtwpauk/", "author": "Brilliant_Corner7140", "created_utc": 1748036045, "score": 1, "content": "I use SSR only for full page reloads, eg if user types url and presses enter key. For user navigation in the browser, I'll use client side rendering since it's cheaper and faster. Why use and pay your server to do the rendering, when client browser can do it for free?"}
{"id": "mtdtxh4", "type": "comment", "parent_id": "t1_mta5n1f", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtdtxh4/", "author": "Motor_Line_5640", "created_utc": 1747783602, "score": 2, "content": "That's absolutely incorrect."}
{"id": "mt9nk58", "type": "comment", "parent_id": "t1_mt9ix4z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9nk58/", "author": "None", "created_utc": 1747733363, "score": 4, "content": "[deleted]"}
{"id": "mtuyhm7", "type": "comment", "parent_id": "t1_mtuxvzl", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtuyhm7/", "author": "chautob0t", "created_utc": 1748017358, "score": 1, "content": "Ah! Here I was wondering about your age. Thanks for the \"Ajax\" info, I never wondered about the history! I learned something new today."}
{"id": "mthwcsw", "type": "comment", "parent_id": "t1_mtf6hyk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mthwcsw/", "author": "frostfenix", "created_utc": 1747844416, "score": 1, "content": "Macromedia Flash?"}
{"id": "mttuinz", "type": "comment", "parent_id": "t1_mtf6hyk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mttuinz/", "author": "AcoustixAudio", "created_utc": 1748005542, "score": 1, "content": "That's what she said."}
{"id": "mtu7xzc", "type": "comment", "parent_id": "t1_mtf6hyk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtu7xzc/", "author": "alessandrawhocodes", "created_utc": 1748009811, "score": 1, "content": "Below your Applets, for course."}
{"id": "mtmy132", "type": "comment", "parent_id": "t1_mtlfa36", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtmy132/", "author": "Icy-Contact-7784", "created_utc": 1747913248, "score": 2, "content": "IIS baby ASP"}
{"id": "mt9jpiw", "type": "comment", "parent_id": "t1_mt7wt55", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mt9jpiw/", "author": "commercial-hippie", "created_utc": 1747730894, "score": 1, "content": "Any react components on the new page will have to be downloaded, and you'd still need the components data fetched from the server. Sometimes these component data fetches are the same speed or even slower than a full SSR page render."}
{"id": "mtarrab", "type": "comment", "parent_id": "t1_mt7wt55", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtarrab/", "author": "altfapper", "created_utc": 1747750447, "score": 1, "content": "Depends, dynamic data obviously doesn't get cached (well...it does...and it helps but its on a different level) but everything statically build or that what can be made static, yes that's cached. So it's not that's it constantly \"downloading\" and/or building the javascript app each time someone creates a session (updates on features and stuff are warmed up but those machines would be fast enough anyway to do this quick enough)."}
{"id": "mtm2snk", "type": "comment", "parent_id": "t1_mt7wt55", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtm2snk/", "author": "FalseRegister", "created_utc": 1747895441, "score": 1, "content": "Plus the huge cost of interpreting that React, running it, firing off the virtual dom, and injecting the result into the DOM. Now imagine all of that on a crappy phone. Downloading is not the highest cost of SPA. Read on \"the cost of Javascript\". With SSR, the browser received HTML ready to go. The only con is that if you have a lot of users, you may need a bigger server"}
{"id": "mtekm2y", "type": "comment", "parent_id": "t1_mtdtxh4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtekm2y/", "author": "angelarose210", "created_utc": 1747793051, "score": 2, "content": "Yeah idk I replied to the wrong comment or something lol. Client side is bad."}
{"id": "mtctyug", "type": "comment", "parent_id": "t1_mt9nk58", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtctyug/", "author": "campsafari", "created_utc": 1747772166, "score": 3, "content": "Yeah it\u2019s so funny, started building SPAs with actionscript / flex with all the bells and whistles like SEO, deeplinks, etc. Then the IPhone came out and Flash got killed. Moved on to html, css, js and php and built SSR ecommerce solutions. A couple years later, JS SPA frmeworks started popping up, backbonejs then react, angular etc. Best thing, they started facing all the same issues like SEO, deeplinking etc. It felt like Flash all over again, same shit different toilet. And now we are discussing SSR vs CSR vs island architecture, etc"}
{"id": "mta7m5k", "type": "comment", "parent_id": "t1_mt9nk58", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mta7m5k/", "author": "None", "created_utc": 1747743332, "score": 2, "content": "SSR is so cool. Now lets make SPAs send entire framework back to server to process it and render html back. /s"}
{"id": "mta7use", "type": "comment", "parent_id": "t1_mt9nk58", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mta7use/", "author": "HarmadeusZex", "created_utc": 1747743427, "score": 1, "content": "Yes but if you notice it is a common pattern, people rediscover old things all the time with variations"}
{"id": "mtd8a3r", "type": "comment", "parent_id": "t1_mt9nk58", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtd8a3r/", "author": "javix64", "created_utc": 1747776444, "score": 1, "content": "Did you hear about AstroJS? It is agnostic JS framework. it renderate automatically HTML files. It is like gatsby without loading any JS, it is super fast. Also what do you recommend? I am a React developer. I had never try NextJS, but i think it is ok, but i wont try it."}
{"id": "mtfu1j1", "type": "comment", "parent_id": "t1_mt9nk58", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtfu1j1/", "author": "SIntLucifer", "created_utc": 1747815228, "score": 1, "content": "Sorry i missed the part that your comment was sarcastic. But yeah you are right."}
{"id": "mtgqs7e", "type": "comment", "parent_id": "t1_mt9nk58", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtgqs7e/", "author": "kruhsoe", "created_utc": 1747831772, "score": 1, "content": "Wait until they figure out that it's possible to generate code without \"hallucinations\""}
{"id": "mti1heu", "type": "comment", "parent_id": "t1_mthwcsw", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mti1heu/", "author": "PM_ME_ALL_YOUR_THING", "created_utc": 1747845901, "score": 2, "content": "It\u2019s the future!"}
{"id": "mtujv3m", "type": "comment", "parent_id": "t1_mthwcsw", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtujv3m/", "author": "0xSnib", "created_utc": 1748013235, "score": 2, "content": "This content is no longer avaliable."}
{"id": "mtif8if", "type": "comment", "parent_id": "t1_mthwcsw", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtif8if/", "author": "dseven4evr", "created_utc": 1747849747, "score": 1, "content": "Wow, this just brought back the memories of Steve Jobs\u2019 letter. Damn, I\u2019m old."}
{"id": "my3yivl", "type": "comment", "parent_id": "t1_mthwcsw", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/my3yivl/", "author": "BadTouchUncle", "created_utc": 1750093090, "score": 1, "content": "Homestar Runner!!!"}
{"id": "mtrbmcp", "type": "comment", "parent_id": "t1_mtmy132", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtrbmcp/", "author": "justStupidFast", "created_utc": 1747963022, "score": 1, "content": "IIS and ColdFusion with an Access \"backend\""}
{"id": "mtt3bpb", "type": "comment", "parent_id": "t1_mtmy132", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtt3bpb/", "author": "HelloWorldMisericord", "created_utc": 1747993789, "score": 1, "content": "Oh man, IIS and all of the other services on a \"borrowed\" copy of Windows Server 2003. If I had enjoyed setting up and running my basement server, I probably would have become a network admin, but I was dumb and stumbled my way painfully through setup."}
{"id": "mtb2453", "type": "comment", "parent_id": "t1_mtarrab", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtb2453/", "author": "Infamous_Land_1220", "created_utc": 1747753521, "score": 1, "content": "Idk, I prefer static websites that dynamically load data. We aren\u2019t Amazon and we don\u2019t have our own cloud infrastructure so I prefer to leave fetching and computer to users devices. SSR especially for larger scale applications with like 1-10k concurrent users you save a lot more money by not doing SSR."}
{"id": "mtm5hii", "type": "comment", "parent_id": "t1_mtm2snk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtm5hii/", "author": "Infamous_Land_1220", "created_utc": 1747896975, "score": 1, "content": "If you optimize your application so that it doesnt re-render unnecessarily the app is pretty efficient. So if you use compiler or if you use memo, you should be fine. Like I said, in my use cases I have thousands of concurrent users, so for me it\u2019s easier to just render stuff on the client side. I host static websites in cdn and then have the clients make api calls."}
{"id": "mtu4mhu", "type": "comment", "parent_id": "t1_mti1heu", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtu4mhu/", "author": "BigBagaroo", "created_utc": 1748008804, "score": 1, "content": "I can finally leverage my Director skills!"}
{"id": "mtruckv", "type": "comment", "parent_id": "t1_mtrbmcp", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtruckv/", "author": "Icy-Contact-7784", "created_utc": 1747969939, "score": 0, "content": "Oh yah did that toooooo."}
{"id": "mtm5m87", "type": "comment", "parent_id": "t1_mtm5hii", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtm5m87/", "author": "FalseRegister", "created_utc": 1747897051, "score": 1, "content": "Sure. But it is still more processing on the client side and SSR pages will still be faster. Comparing two good implementations, ofc. This is important to some businesses and use cases, such as e-commerce."}
{"id": "mtm5rnv", "type": "comment", "parent_id": "t1_mtm5m87", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqhucx/how_do_big_companies_like_amazon_hide_their_api/mtm5rnv/", "author": "Infamous_Land_1220", "created_utc": 1747897137, "score": 1, "content": "No for sure, if you want better SEO SSR is a good idea."}
{"id": "1b77lzs", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/", "author": "GeekLifer", "created_utc": 1709651741, "score": 386, "title": "I created an open source tool for extracting data from websites", "content": ""}
{"id": "ktgkhq2", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktgkhq2/", "author": "GeekLifer", "created_utc": 1709651823, "score": 39, "content": "I'm the creator. I've made this project open source and plan on adding code generation using AI in the future. Thanks for watching! edit: Sorry forgot to link github"}
{"id": "kth687l", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kth687l/", "author": "JFC_Mx", "created_utc": 1709658442, "score": 5, "content": "Has any one tried it to scrape Twitter?"}
{"id": "ktgrkqc", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktgrkqc/", "author": "None", "created_utc": 1709654082, "score": 5, "content": "Git?"}
{"id": "ktimytx", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktimytx/", "author": "Emperor_Abyssinia", "created_utc": 1709675777, "score": 5, "content": "I\u2019d like to contribute"}
{"id": "ktgv52a", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktgv52a/", "author": "CryptoOdin99", "created_utc": 1709655125, "score": 3, "content": "Link to the project?"}
{"id": "kth4z55", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kth4z55/", "author": "illkeepthatinmind", "created_utc": 1709658046, "score": 3, "content": "Do you plan to monetize it at some point?"}
{"id": "kthzccm", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kthzccm/", "author": "nealcaffery_bored", "created_utc": 1709668036, "score": 3, "content": "Has anyone tried youtbe and other major social media apps ? when i tried to fect the youtube playlist it failed.or did i make something wrong process?"}
{"id": "ktgrgh2", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktgrgh2/", "author": "avg_skl", "created_utc": 1709654048, "score": 2, "content": "@op github?"}
{"id": "ktia0ww", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktia0ww/", "author": "lazynoob0503", "created_utc": 1709671538, "score": 2, "content": "Amazing work man, will following your work closely, and will help you build as well as I get some time. Do you know any other projects which are working on the same thing.? This will end the era of paid services , I love it. Loooking forward to testing and give you some suggestions, I am active user of similar low code solutions , I would love to change that with open source solution and I think you have the base ready. If you don\u2019t mind me asking how long have you been working on this!?"}
{"id": "ktul3m6", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktul3m6/", "author": "Sl33py_4est", "created_utc": 1709859885, "score": 2, "content": "this is great"}
{"id": "ktz341a", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktz341a/", "author": "FromAtoZen", "created_utc": 1709932085, "score": 2, "content": "Does it work against sites protected by CloudFlare?"}
{"id": "ku05y6t", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ku05y6t/", "author": "oldrocketscientist", "created_utc": 1709946630, "score": 2, "content": "Can it do a page from LinkedIn?"}
{"id": "ku81c2c", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ku81c2c/", "author": "Ms-Prada", "created_utc": 1710081000, "score": 2, "content": "I don't see this as useful. If you want the text or innerHTML of that tag on a website. Just highlight the text, right click, select inspect, then select copy, and then pick your poison. This also allows you to see the css of an element as well."}
{"id": "ktj05kz", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktj05kz/", "author": "barrard123", "created_utc": 1709680397, "score": 1, "content": "Cheerio is not the best at loading pages with lots of JavaScript, I found puppeteer works really well though"}
{"id": "ktj7ml6", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktj7ml6/", "author": "Nikastreams", "created_utc": 1709683196, "score": 1, "content": "Very cool! Can it also visit pages (I.e clicking on each product) and recursively grab info?"}
{"id": "ktjo32i", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktjo32i/", "author": "None", "created_utc": 1709689525, "score": 1, "content": "Very cool! Thanks so much for sharing. I\u2019ll check it out!"}
{"id": "ktndqz8", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktndqz8/", "author": "onroster", "created_utc": 1709751511, "score": 1, "content": "Is it only working with selectors vs. xpaths?"}
{"id": "ktnjxt3", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktnjxt3/", "author": "cdank", "created_utc": 1709753524, "score": 1, "content": "Neat"}
{"id": "ktopofq", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktopofq/", "author": "Heavy_Bluebird_1780", "created_utc": 1709767561, "score": 1, "content": "If you could add a sort button for the prices it would be awesome! it is an amazing project!"}
{"id": "ktsfrio", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktsfrio/", "author": "tbriz", "created_utc": 1709831018, "score": 1, "content": "Very cool. It would be nice / next level to scrape at the card level, then output json for each card. For example: { \"product\" : \"samsung galaxy\", \"price\" : \"$259.99\"} That data would be ready to pop into a database, and could do some other cool stuff with the json output."}
{"id": "ktsftk9", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktsftk9/", "author": "tbriz", "created_utc": 1709831037, "score": 1, "content": "Very cool. It would be nice / next level to scrape at the card level, then output json for each card. For example: { \"product\" : \"samsung galaxy\", \"price\" : \"$259.99\"} { \"product\" : \"iPhone 11\", \"price\" : \"$400.00\"} ...etc That data would be ready to pop into a database, and could do some other cool stuff with the json output."}
{"id": "kvxfjz8", "type": "comment", "parent_id": "t3_1b77lzs", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kvxfjz8/", "author": "myrainyday", "created_utc": 1711046575, "score": 1, "content": "This is interesting. Would be great to be able to feed an excel sheet with websites and get emails and phones from it."}
{"id": "kth088r", "type": "comment", "parent_id": "t1_ktgkhq2", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kth088r/", "author": "Rockets2TheMoon", "created_utc": 1709656615, "score": 9, "content": "very cool, how would you go about utilizing this in a scraping project?"}
{"id": "kth7tdg", "type": "comment", "parent_id": "t1_kth687l", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kth7tdg/", "author": "GeekLifer", "created_utc": 1709658960, "score": 7, "content": "Got a link? Oh wow, failed to get something like [ edit: oh so it's having trouble with javascript rendering"}
{"id": "ktirpg2", "type": "comment", "parent_id": "t1_ktimytx", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktirpg2/", "author": "GeekLifer", "created_utc": 1709677402, "score": 2, "content": "Feel free to open up a pull request. I'd be happy to add you to the contribution list"}
{"id": "kth5wax", "type": "comment", "parent_id": "t1_kth4z55", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kth5wax/", "author": "GeekLifer", "created_utc": 1709658337, "score": 3, "content": "Right now everything is free. If I do get code generation working (calling AI would cost money) and I would need to monetize the code generation part."}
{"id": "kti04ij", "type": "comment", "parent_id": "t1_kthzccm", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kti04ij/", "author": "GeekLifer", "created_utc": 1709668292, "score": 2, "content": "You're not doing anything wrong. It seems like pages with a lot of JavaScript is failing to load."}
{"id": "kti1xlu", "type": "comment", "parent_id": "t1_kthzccm", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kti1xlu/", "author": "GeekLifer", "created_utc": 1709668887, "score": 1, "content": "I just added a toggle for Javascript. Give it a try"}
{"id": "ktiesyh", "type": "comment", "parent_id": "t1_ktia0ww", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktiesyh/", "author": "GeekLifer", "created_utc": 1709673096, "score": 3, "content": "Thanks for checking it out. So the only ones that I know off are mostly browser extensions that lets you pick selectors and stuff. But never they all require a browser of some kind. Please do give it a try. I've had some really good feedback so far. Which I added a beta option to toggle loading javascript. Still a lot of issues to fix though. And the UI can be improved as well. So I've always wanted a quick and easy tool like this for a long time. Just haven't found one yet. So I started researching and building this about a month ago."}
{"id": "ktz5vk0", "type": "comment", "parent_id": "t1_ktz341a", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktz5vk0/", "author": "GeekLifer", "created_utc": 1709933028, "score": 2, "content": "Yes. Give those sites a try. Let me know if they don\u2019t work and I can take a look into it"}
{"id": "ku06pvh", "type": "comment", "parent_id": "t1_ku05y6t", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ku06pvh/", "author": "GeekLifer", "created_utc": 1709946942, "score": 1, "content": "Anything that requires logging in is not possible."}
{"id": "ku879lo", "type": "comment", "parent_id": "t1_ku81c2c", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ku879lo/", "author": "GeekLifer", "created_utc": 1710083402, "score": 1, "content": "Right, but say you have multiple items you want to parse on the page. You\u2019ll still have to play around with the css to get a generalized css that works. This lets you quickly visualize while you play with the css"}
{"id": "ktno7de", "type": "comment", "parent_id": "t1_ktndqz8", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktno7de/", "author": "GeekLifer", "created_utc": 1709754909, "score": 1, "content": "It should work with selectors and xpaths. Did xpath not work for you?"}
{"id": "ktz5zr2", "type": "comment", "parent_id": "t1_ktsftk9", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktz5zr2/", "author": "GeekLifer", "created_utc": 1709933069, "score": 1, "content": "So right now it is column focused. Might be easier to see in a spreadsheet"}
{"id": "kth2dc4", "type": "comment", "parent_id": "t1_kth088r", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/kth2dc4/", "author": "GeekLifer", "created_utc": 1709657255, "score": 10, "content": "Great question. I envision it being a tool to **help build scrapers quicker**. People can point and click at data to extract. They can verify that it is grabbing the right data. Then simply generate the code in Python/Javascript and any other language they want. (code generation is being worked on)"}
{"id": "ktphmma", "type": "comment", "parent_id": "t1_kth5wax", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktphmma/", "author": "D_a_f_f", "created_utc": 1709778396, "score": 3, "content": "You could use Ollama. It\u2019s open source, can be run locally, and provides access to numerous open source LLM and image generation models"}
{"id": "ktultni", "type": "comment", "parent_id": "t1_kth5wax", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktultni/", "author": "Sl33py_4est", "created_utc": 1709860170, "score": 1, "content": "what sort of code generation? (for what purpose?) for local models ollama is a good slot in, llamacpp is a good build in local models are far more stable than hosted models if this is to be a stable project, i would think a local model with a good framework would suffice if it's going to be hosted, what kind of code will it be generating? the hosted models are all going through iterative changes that might brick your code generation at any point unless it is super basic or broad at which point i loop back to why not local? (llamacpp + phi-2.gguf runs interactively on a raspberry pi)"}
{"id": "ktigi9v", "type": "comment", "parent_id": "t1_ktiesyh", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktigi9v/", "author": "lazynoob0503", "created_utc": 1709673652, "score": 1, "content": "I don\u2019t know js that well, I usually do this using scrapy and python, but I will fork and test out on my end as well. If time allows I can work on Python implementation of this. Keep doing the good work lots of value in this. I wonder why no one worked on this before. Will take some time understanding it better and will help you along the way in documenting as I will be using this instead of paid service going forward. Nice meeting you man, I will stay in touch."}
{"id": "lctxxi4", "type": "comment", "parent_id": "t1_ku879lo", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/lctxxi4/", "author": "saintshing", "created_utc": 1720790500, "score": 1, "content": "Aren't you using selectorgadget?"}
{"id": "ktk4csg", "type": "comment", "parent_id": "t1_kth2dc4", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktk4csg/", "author": "ReadSeparate", "created_utc": 1709695995, "score": 4, "content": "Another thing that might be worth considering is generating embeddings of the page source, and then asking, say, GPT-4, to write code to extract each of the features you care about. You often can't just copy and paste the page source into a prompt because it's waaaay too much html/js, but if you convert it to embeddings then it might be able to find the pieces it needs directly instead."}
{"id": "lcvw0g0", "type": "comment", "parent_id": "t1_lctxxi4", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/lcvw0g0/", "author": "GeekLifer", "created_utc": 1720813658, "score": 1, "content": "Yes sir"}
{"id": "ktnslnl", "type": "comment", "parent_id": "t1_ktk4csg", "permalink": "https://www.reddit.com/r/webscraping/comments/1b77lzs/i_created_an_open_source_tool_for_extracting_data/ktnslnl/", "author": "GeekLifer", "created_utc": 1709756346, "score": 2, "content": ">Another thing that might be worth considering is generating embeddings of the page source, and then asking, say, GPT-4, to write code to extract each of the features you care about. You often can't just copy and paste the page source into a prompt because it's waaaay too much html/js, but if you convert it to embeddings then it might be able to find the pieces it needs directly instead. Great idea. I'll see what I can do. Making the UI might be the hard part"}
{"id": "1i6tfec", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/", "author": "DM_Me_Summits_In_UAE", "created_utc": 1737493848, "score": 381, "title": "Why does webscraping cause this facial expression?", "content": ""}
{"id": "m8f76cb", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8f76cb/", "author": "fstring", "created_utc": 1737494916, "score": 130, "content": "Excessive amounts of Selenium can cause gastric discomfort."}
{"id": "m8f8vqz", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8f8vqz/", "author": "None", "created_utc": 1737495374, "score": 101, "content": "cause he knows his shit will be broken tomorrow and he\u2019s tired to fuck of fixing it"}
{"id": "m8f7be6", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8f7be6/", "author": "GETPILLSAGAINST", "created_utc": 1737494953, "score": 42, "content": "its what happens when you're making an entire scraping video around a sponsor.."}
{"id": "m8f6zpr", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8f6zpr/", "author": "hemogolobin", "created_utc": 1737494867, "score": 20, "content": "It's the fucking scrapy"}
{"id": "m8f97ju", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8f97ju/", "author": "Maleppe", "created_utc": 1737495464, "score": 14, "content": "They look like they just pooped their pants :3"}
{"id": "m8grzo4", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8grzo4/", "author": "None", "created_utc": 1737512130, "score": 14, "content": ""}
{"id": "m8fn6un", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8fn6un/", "author": "matty_fu", "created_utc": 1737499361, "score": 9, "content": "I think this goes beyond just web scraping! lots of tech channels test thumbnails where they ran different variations to see which brings the most views For some reason, having a dimwit expression on your face gets the most clicks. Just another example of impulse-driven enshittification"}
{"id": "m8gwdpt", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8gwdpt/", "author": "alepsan", "created_utc": 1737513577, "score": 8, "content": "Yup that was my face expression when cloudflare block me even using rotating proxies."}
{"id": "m8fco3m", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8fco3m/", "author": "Psychological-Egg122", "created_utc": 1737496397, "score": 10, "content": "The face you make when you accept the fact that you're gobblin the balls of some of the world's most scumbaggy and shitty proxy providers but >!atleast they paid you well!<"}
{"id": "m8hjcpi", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8hjcpi/", "author": "parthmty", "created_utc": 1737521937, "score": 3, "content": "I guess beautifulsoup is not that beautiful after all"}
{"id": "m8gg9c0", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8gg9c0/", "author": "emphieishere", "created_utc": 1737508377, "score": 2, "content": "You saw the rates on Upwork, didn't you?"}
{"id": "m8glrma", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8glrma/", "author": "HANEZ", "created_utc": 1737510118, "score": 2, "content": "The bags underneath the eyes are more telling."}
{"id": "m8jxwdp", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8jxwdp/", "author": "None", "created_utc": 1737561202, "score": 2, "content": "[deleted]"}
{"id": "m8qs0c3", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8qs0c3/", "author": "professorbasket", "created_utc": 1737648575, "score": 1, "content": "It is a contempt of self that causes this face. A self loathing if you will, deep below the surface. A shame that permeates, unable to be hidden by the face."}
{"id": "m8tjk6l", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8tjk6l/", "author": "NopeNotHB", "created_utc": 1737676348, "score": 1, "content": "Because it's really really hard to get paid doing web scraping. Lol"}
{"id": "m9am5s5", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m9am5s5/", "author": "None", "created_utc": 1737911624, "score": 1, "content": "He may need to try Playwright"}
{"id": "ma0u5x8", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/ma0u5x8/", "author": "Anxious-Ostrich-36", "created_utc": 1738250914, "score": 1, "content": "Signature look of superiority."}
{"id": "mb1h27m", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/mb1h27m/", "author": "Majestic_Nobody_9995", "created_utc": 1738723810, "score": 1, "content": "sorry, are these two the same person?"}
{"id": "m8p4hg7", "type": "comment", "parent_id": "t3_1i6tfec", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8p4hg7/", "author": "funnyDonaldTrump", "created_utc": 1737624218, "score": 0, "content": "the face when your beautifulsoup tastes meh"}
{"id": "m8h2eyk", "type": "comment", "parent_id": "t1_m8f76cb", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8h2eyk/", "author": "SponsoredByMLGMtnDew", "created_utc": 1737515586, "score": 8, "content": ""}
{"id": "m8hcam4", "type": "comment", "parent_id": "t1_m8f76cb", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8hcam4/", "author": "None", "created_utc": 1737519127, "score": 2, "content": "Also beautifulsoup"}
{"id": "m8fva22", "type": "comment", "parent_id": "t1_m8f8vqz", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8fva22/", "author": "Mpk_Paulin", "created_utc": 1737501808, "score": 14, "content": "The right fucking answer"}
{"id": "m99isen", "type": "comment", "parent_id": "t1_m8f8vqz", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m99isen/", "author": "amemingfullife", "created_utc": 1737899187, "score": 2, "content": "He tried using AI. Unfortunately it did not make things better."}
{"id": "mbfyb19", "type": "comment", "parent_id": "t1_m8f8vqz", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/mbfyb19/", "author": "das_war_ein_Befehl", "created_utc": 1738913059, "score": 1, "content": "Sometimes it just breaks because of a gentle breeze"}
{"id": "m8hosvb", "type": "comment", "parent_id": "t1_m8grzo4", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8hosvb/", "author": "GoatBass", "created_utc": 1737524334, "score": 5, "content": "slow down, Keegan-Michael Key"}
{"id": "m8fct0j", "type": "comment", "parent_id": "t1_m8fco3m", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8fct0j/", "author": "Psychological-Egg122", "created_utc": 1737496434, "score": 0, "content": "\u2665 tech with tim (no hate)"}
{"id": "m8hr3l9", "type": "comment", "parent_id": "t1_m8gg9c0", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8hr3l9/", "author": "Quantum_Rage", "created_utc": 1737525422, "score": 1, "content": "Only a problem if you hunt for freelance projects on Upwork."}
{"id": "m901oif", "type": "comment", "parent_id": "t1_m8jxwdp", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m901oif/", "author": "TimelessTrance", "created_utc": 1737760947, "score": 2, "content": "Scraping isn\u2019t illegal. Reasons for scraping or methods may have criminal or civil consequences though. Ddos is illegal, copyright infringement is illegal, violating TOS? Ask your lawyer. Violating robots.txt is actually not illegal. When it comes to scraping my experience is either cease and desist letters or emails to ask you to not abuse their servers."}
{"id": "mb2b1v8", "type": "comment", "parent_id": "t1_mb1h27m", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/mb2b1v8/", "author": "DM_Me_Summits_In_UAE", "created_utc": 1738735523, "score": 1, "content": "No"}
{"id": "m8p4oqf", "type": "comment", "parent_id": "t1_m8p4hg7", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8p4oqf/", "author": "DM_Me_Summits_In_UAE", "created_utc": 1737624349, "score": 0, "content": "Underrated comment"}
{"id": "m8igkb9", "type": "comment", "parent_id": "t1_m8hcam4", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8igkb9/", "author": "youdig_surf", "created_utc": 1737540063, "score": 4, "content": "Wait till you start nodriver with it's very thin documentation."}
{"id": "m8kc7qv", "type": "comment", "parent_id": "t1_m8hr3l9", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8kc7qv/", "author": "emphieishere", "created_utc": 1737565160, "score": 1, "content": "oh alright, so where do you hunt"}
{"id": "m90c2g4", "type": "comment", "parent_id": "t1_m901oif", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m90c2g4/", "author": "None", "created_utc": 1737764297, "score": 0, "content": "[deleted]"}
{"id": "m8m6mcs", "type": "comment", "parent_id": "t1_m8kc7qv", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8m6mcs/", "author": "domitori3", "created_utc": 1737582927, "score": 1, "content": "Sometimes, companies have an actual 9/5 position for a web scraping specialist. That's basically the only way to get a good pay out of web scraping, imo."}
{"id": "mbfym96", "type": "comment", "parent_id": "t1_m90c2g4", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/mbfym96/", "author": "das_war_ein_Befehl", "created_utc": 1738913238, "score": 1, "content": "Every company scrapes, lawyers come out when you try selling the good stuff a little too obviously"}
{"id": "m8pc53p", "type": "comment", "parent_id": "t1_m8m6mcs", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/m8pc53p/", "author": "emphieishere", "created_utc": 1737628999, "score": 1, "content": "AFAIK, this currently in decline too. I had my friend working in the scraping agency, and he suggested I do the same switch too, he told me that initial rate for newcomers is 7 dollars/h was back than, but it was almost full-time.. When I came, he told me that they do max 5$ now at the start (considering that I pass well) and even after some period of time he told me that even though I was good on the call, that they don't have much orders for now and they don't really need staff. that happened to me like 3-4 months ago"}
{"id": "mbfygge", "type": "comment", "parent_id": "t1_m8m6mcs", "permalink": "https://www.reddit.com/r/webscraping/comments/1i6tfec/why_does_webscraping_cause_this_facial_expression/mbfygge/", "author": "das_war_ein_Befehl", "created_utc": 1738913145, "score": 1, "content": "The money is to actually move up to where you are using the data to generate business outcomes and not just providing said data. Scraping is a skill set complimentary to other things. Or provide it in bulk and sell it to a niche"}
{"id": "1kceext", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/", "author": "Remote-Book-8616", "created_utc": 1746120399, "score": 372, "title": "What I've Learned After 5 Years in the Web Scraping Trenches", "content": "After spending the last 5 years working with web scraping projects, I wanted to share some insights that might help others who are just getting started or facing common challenges. **The biggest challenges I've faced:** **1. Website Anti-Bot Measures** These have gotten incredibly sophisticated. Simple requests with Python's requests library rarely work on modern sites anymore. I've had to adapt by using headless browsers, rotating proxies, and mimicking human behavior patterns. **2. Maintenance Nightmare** About 10-15% of my scrapers break EVERY WEEK due to website changes. This is the hidden cost nobody talks about - the ongoing maintenance. I've started implementing monitoring systems that alert me when data patterns change significantly. **3. Resource Consumption** Browser-based scraping (which is often necessary to handle JavaScript) is incredibly resource-intensive. What starts as a simple project can quickly require significant server resources when scaled. **4. Legal Gray Areas** Understanding what you can legally scrape vs what you can't is confusing. I've developed a personal framework: public data is generally ok, but respect robots.txt, don't overload servers, and never scrape personal information. **What's worked well for me:** **1. Proxy Management** Residential and mobile proxies are worth the investment for serious projects. I rotate IPs, use different user agents, and vary request patterns. **2. Modular Design** I build scrapers with separate modules for fetching, parsing, and storage. When a website changes, I usually only need to update the parsing module. **3. Scheduled Validation** Automated daily checks that compare today's data with historical patterns to catch breakages early. **4. Caching Strategies** Implementing smart caching to reduce requests and avoid getting blocked. Would love to hear others' experiences and strategies! What challenges have you faced with web scraping projects? Any clever solutions you've discovered?"}
{"id": "mq43a07", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq43a07/", "author": "DifferenceDull2948", "created_utc": 1746144847, "score": 52, "content": "You mention 10-15% of scrapers break weekly because of website changes. I am assuming this is due to changes in the selectors and so. I don\u2019t want to be that guy, but I will : have you tried using LLMs ? Just to be clear: I mean only for the broken ones, not everything. So, if you can\u2019t get the information / it breaks because of website changes, you could pass the html to Gemini and ask something like: which selector holds this information. You can configure it to reply in a structured way, JSON for example, and then you can automatically update your selectors/paths/whatever. It\u2019s a fairly easy way to do it, it\u2019s literally just an API call, it\u2019s cheap, and with the massive context window that Gemini has now, you can pretty much throw the whole html at it. I had a project in which I did something like that, I used it to scrap any website for e-commerce products automatically. Worked pretty decently"}
{"id": "mq4z7ox", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4z7ox/", "author": "mickspillane", "created_utc": 1746156593, "score": 6, "content": "When I'm making authenticated requests (i.e. logged in to the website I'm scraping), I have to sometimes throttle how fast I'm making requests from that account so the site doesn't get suspicious. Rotating IPs is not useful in this scenario because I'm logged in and sophisticated sites can track account-level behavior. Any advice on dealing with captchas?"}
{"id": "mq5z89w", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq5z89w/", "author": "Ill-Possession1", "created_utc": 1746176212, "score": 5, "content": "Can you write more details about how you avoid Anti-Bot measures?"}
{"id": "mq4ae4f", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4ae4f/", "author": "LowerDescription5759", "created_utc": 1746147393, "score": 3, "content": "i\u2019m just curious. what are you scraping and for what reason? i\u2019m just curious about a legit use case for scrapping."}
{"id": "mqcef20", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqcef20/", "author": "iotchain2", "created_utc": 1746263933, "score": 3, "content": "Congratulations on your journey, can you please give us a detailed operating procedure for scrapping: tools, script, proxy.... it will really help a lot of people"}
{"id": "mq3uqh9", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq3uqh9/", "author": "s_busso", "created_utc": 1746141907, "score": 2, "content": "Thanks for sharing. Could you tell more about how you do for \"Automated daily checks that compare today's data with historical patterns to catch breakages early.\""}
{"id": "mqa2mt7", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqa2mt7/", "author": "iotchain2", "created_utc": 1746225761, "score": 2, "content": "Is your work very profitable? Is information really worth gold to sell?"}
{"id": "mqac3uk", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqac3uk/", "author": "TheOriginalStig", "created_utc": 1746229008, "score": 2, "content": "Good points. This industry has changed since we started doing it 15+ years ago when it was easier. While some stuff still in perl and works the maintenance has made me move to modern frameworks. Your post was very accurate"}
{"id": "mr2t52f", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mr2t52f/", "author": "0xReaper", "created_utc": 1746630643, "score": 2, "content": "Hey there, great post! I have been in the same boat for the last 4 years, and that's why I created Scrapling to solve almost all the repeating issues with each update. For example, for the maintenance issue, there's the automatch feature: I even wrote an article about this since it seems like everyone who faces this issue runs to AI before thinking of any creative ideas: The library is still new, so I would love to hear your feedback."}
{"id": "mq3efjj", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq3efjj/", "author": "None", "created_utc": 1746136413, "score": 1, "content": "[removed]"}
{"id": "mq3zjp3", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq3zjp3/", "author": "mm_reads", "created_utc": 1746143544, "score": 1, "content": "Just posting agreement with this. A lot of sites that were open accessible a few years ago have become closed and added a lot of bot protection. I mainly interact with one or two sites. They had an API available several years ago before they revoked the API, despite still being an otherwise free site. Best I've come up with is mostly-automated with some human interaction to bypass bot gatekeeping. For some data, I can understand the need for bot protection, especially when it's users' content creations. For others, not so much..."}
{"id": "mq4g0ti", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4g0ti/", "author": "trashcan41", "created_utc": 1746149419, "score": 1, "content": "is it ok to ask scraping advice? the website i'm trying to scrape creating this weird semi pdf file where if i am using html to pdf the page format become messy but when i print and save the html as pdf the page look fine. what do you do in this situation?"}
{"id": "mq4ios3", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4ios3/", "author": "JurrasicBarf", "created_utc": 1746150363, "score": 1, "content": "You should turn it into a product or something?"}
{"id": "mq5f456", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq5f456/", "author": "LeewardLeeway", "created_utc": 1746164056, "score": 1, "content": "Something I've been wondering: since more and more websites force scrapers to mimick human behaviour, is there going to come a time when it is more effective to hire people from MTruk, for example, to do the scraping for you? Of course, multiple scrapers can be run and proxies can be rotated, but the increasing computational costs was already mentioned. These costs will only increase with growing scraping protections while mimickin human behaviour slows down scraping."}
{"id": "mq5fahy", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq5fahy/", "author": "VierFaeuste", "created_utc": 1746164153, "score": 1, "content": "Thanks for sharing, good to know. I am currently working on scraper for Lego Star Wars sets, but in an very early stage of development"}
{"id": "mq607hk", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq607hk/", "author": "None", "created_utc": 1746176832, "score": 1, "content": "[removed]"}
{"id": "mq7x2l0", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq7x2l0/", "author": "None", "created_utc": 1746202548, "score": 1, "content": "[removed]"}
{"id": "mqbg5jn", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqbg5jn/", "author": "magiiczman", "created_utc": 1746244376, "score": 1, "content": "I decided to create a web scraper as a my first project and I feel like I learned a lot within 2 days. Robots.txt, headers, get, setting up a .venv, pip installs, request, beautifulSoup, etc. However like you said I ran into so many issues while trying to get my program to not give me a 403 Client error. The only site I have it working is Wikipedia which I guess is fine for a beginner project since I\u2019m not trying to do anything crazy and just wanted to have at least one project that I did solo and wasn\u2019t a school project. You bring up headless browsers which I don\u2019t know what that is but it sounds like something I could maybe go more in depth on later. Depending on how complicated it is to have a web scraping application I might end up switching to figuring out how to implement AI and create a chat bot. I just finished a C++ class and wanted to build in Python a normal and fun language so bad. Made me remember that programming should be fun."}
{"id": "mqqjzxi", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqqjzxi/", "author": "InappropriatelyHard", "created_utc": 1746464890, "score": 1, "content": "Ive been struggling to reach path of exile api through cloudflare , constantly get 403 after a few hundred requests. Any suggestions?"}
{"id": "mqup4aw", "type": "comment", "parent_id": "t3_1kceext", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqup4aw/", "author": "BetterNotRelapse", "created_utc": 1746520132, "score": 1, "content": "Great write up! :) Could you maybe write a bit more how you smart caching works in your system?"}
{"id": "mq4hyum", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4hyum/", "author": "germs_smell", "created_utc": 1746150106, "score": 14, "content": "As someone new to this but not data in general, this is clever... so youre saying upon exception/broken script you invoke a second attempt by changing the approach and ask the LLM for help? your passing the LLM's JSON structured response via rest API back Into a variable in your script that refocuses how/where you are scraping then rerun against the target? That's fucking cool and clever..."}
{"id": "mq8ub8b", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq8ub8b/", "author": "KaleRevolutionary795", "created_utc": 1746212042, "score": 4, "content": "This is the answer. You can ask an LLM from this page: extract names, telefone numbers and it will find it. It works declaratively, not imperatively."}
{"id": "mq66iv9", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq66iv9/", "author": "Olschinger", "created_utc": 1746180675, "score": 3, "content": "Switched to llm for a short time then stopped after getting strange hallucinations, not many maybe 1 in a thousand but that\u2019s enough to destroy my trust in all of the data."}
{"id": "mr2s9vr", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mr2s9vr/", "author": "0xReaper", "created_utc": 1746630393, "score": 2, "content": "A more sustainable and cheaper way is to use Scrapling's automatch feature. That's why it was created in the first place:"}
{"id": "mq6vj9o", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq6vj9o/", "author": "None", "created_utc": 1746191405, "score": 1, "content": "[removed]"}
{"id": "mql56b1", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mql56b1/", "author": "BlackLands123", "created_utc": 1746387307, "score": 1, "content": "Can you suggest any AI scrapers? I'm interested in scraping some pages with paginations, but not all of them have it."}
{"id": "mqwuy3d", "type": "comment", "parent_id": "t1_mq43a07", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqwuy3d/", "author": "Mouse37dev", "created_utc": 1746550101, "score": 1, "content": "I am also doing this with LLMs, can write emails and stuff. Works well."}
{"id": "mq87zpt", "type": "comment", "parent_id": "t1_mq4z7ox", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq87zpt/", "author": "qundefined", "created_utc": 1746205645, "score": 4, "content": "Invest in a captcha solver. They've gotten really good and affordable. Not the answer you're probably looking for, but I also struggle with this. My \"cheap\" solution is to mark errorful URLs and set up a cron to re-try those errorful URLs after a while so that list shrinks throughout the day."}
{"id": "mqq27c4", "type": "comment", "parent_id": "t1_mqcef20", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqq27c4/", "author": "GoodLegal9346", "created_utc": 1746459652, "score": 3, "content": "Tools: puppeteer, playwright, botasaurus Proxies: residential and mobile are the best, but if the site supports ipv6 then its way cheaper (works to scrape google) Proxt providers: id stick with cheaper ones that work (bytezero, dataimpulse, nodemaven, etc.) Chatgpt and claude are your best friends :) use them to learn and understand what they're giving you in close. Embed them through api and pass DOM so that they do the pain in the a$$ work of parsing with xpath for you. Hope that helps"}
{"id": "mq3fhi8", "type": "comment", "parent_id": "t1_mq3efjj", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq3fhi8/", "author": "webscraping-ModTeam", "created_utc": 1746136753, "score": 3, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mq4mo9y", "type": "comment", "parent_id": "t1_mq4g0ti", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4mo9y/", "author": "alphabet_explorer", "created_utc": 1746151786, "score": 1, "content": "Basic OCR with simplecv/opencv? What is your task? Scraping the free text or graphics? Or what?"}
{"id": "mq637f2", "type": "comment", "parent_id": "t1_mq607hk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq637f2/", "author": "webscraping-ModTeam", "created_utc": 1746178717, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mq8v9rh", "type": "comment", "parent_id": "t1_mq7x2l0", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq8v9rh/", "author": "webscraping-ModTeam", "created_utc": 1746212326, "score": 1, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "mq4mbyb", "type": "comment", "parent_id": "t1_mq4hyum", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4mbyb/", "author": "alphabet_explorer", "created_utc": 1746151661, "score": 12, "content": "It\u2019s an interesting proposition but, but this seems like a fast way to break your script. You are trusting the LLM to modify your script accordingly and automatically. I can see this endless looping and you look back and your script is 100x longer with all these random routines and subroutines\u2026"}
{"id": "mqid9de", "type": "comment", "parent_id": "t1_mq8ub8b", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqid9de/", "author": "Technical_System_252", "created_utc": 1746350539, "score": 1, "content": "What do you send as a input precisely ?"}
{"id": "mqcgdwo", "type": "comment", "parent_id": "t1_mq66iv9", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqcgdwo/", "author": "Visual-Librarian6601", "created_utc": 1746265208, "score": 1, "content": "It also depend on which LLM u use. From my experience, Gemini 2.5 flash and GPT 4o mini are pretty good and also cost effective."}
{"id": "mq713sz", "type": "comment", "parent_id": "t1_mq6vj9o", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq713sz/", "author": "webscraping-ModTeam", "created_utc": 1746193268, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mqlw9ad", "type": "comment", "parent_id": "t1_mql56b1", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqlw9ad/", "author": "DifferenceDull2948", "created_utc": 1746395877, "score": 1, "content": "I\u2019ve not used any. I built this myself as a fallback for an already existing scraper that we had. So, if it broke because of css selectors change, it would try to fetch the new selectors by asking Gemini and storing them. Then try those in future scrapes."}
{"id": "mqb5cak", "type": "comment", "parent_id": "t1_mq87zpt", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqb5cak/", "author": "EloquentSyntax", "created_utc": 1746239836, "score": 2, "content": "Any recommendations?"}
{"id": "mr0f8wh", "type": "comment", "parent_id": "t1_mqq27c4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mr0f8wh/", "author": "gavin101", "created_utc": 1746591112, "score": 1, "content": "Could you expand on when you use the different tools? I'm currently using DrissionPage but looking at playwright vs botasaurus so I'm curious what you like/dislike about each. Appreciate it"}
{"id": "mq4oc5r", "type": "comment", "parent_id": "t1_mq4mo9y", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4oc5r/", "author": "trashcan41", "created_utc": 1746152387, "score": 1, "content": "Thanks for the answer My task is making that html into pdf with the same format and paging. When i use html to pdf library the page become messy so i scrape the page individually and change them into pdf. I will look up how ocr work with simplecv/opencv but the formating probably need some work."}
{"id": "mq60kae", "type": "comment", "parent_id": "t1_mq4mbyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq60kae/", "author": "ZnV1", "created_utc": 1746177059, "score": 10, "content": "Nah, this is a common pattern. I work with genAI professionally. You set a max retries, say 3. Every time you get an error, you loop back to the LLM saying \"I tried xyz, this is the error I'm getting: xyz. Fix it\" If it works, flag it to a human for review but continue the process. After 3 retries if it's still an error, flag it to the human to fix, cancel process."}
{"id": "mq4pfd0", "type": "comment", "parent_id": "t1_mq4mbyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq4pfd0/", "author": "PresidentHoaks", "created_utc": 1746152787, "score": 7, "content": "It's not that much longer. You can just simply cache a selector that the LLM chooses for something and try that selector for future scrapes. If it doesnt work, then you still need to fix it, but it reduces the work a little."}
{"id": "mqd8tha", "type": "comment", "parent_id": "t1_mq4mbyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqd8tha/", "author": "Kos---Mos", "created_utc": 1746278815, "score": 5, "content": "Smells vibe coding sh*t. They don't care if something is extremely resource inefficient. They trust the a.i blindly and are usually very proud of not caring about what the a.i is doing"}
{"id": "mq5359z", "type": "comment", "parent_id": "t1_mq4mbyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq5359z/", "author": "germs_smell", "created_utc": 1746158254, "score": 1, "content": "You could do something like assign the variable the LLM response (you'll probably need to parse something, clean or whatever), try it. If it fails, follow an exception path to clear the variable and loop to try again or move on to some other approach in the script. It won't blow it up if you code if/then exception handling, and use like while loop concept and a counter to limit the number of retries... then maybe after so many failures you move on to something new!?"}
{"id": "mqis674", "type": "comment", "parent_id": "t1_mqid9de", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqis674/", "author": "Yashugan00", "created_utc": 1746359085, "score": 4, "content": "You can give it the rendered html page if you want, and then ask it questions, for example: return a Json with the following attributes: name of person, date of birth etc. And it will do just that. No need to navigate to a particular DOM object and have it break when they change the html page. Note that not all LLM models from openapi are capable of responding in json format. But most do."}
{"id": "mqqc5x8", "type": "comment", "parent_id": "t1_mqlw9ad", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqqc5x8/", "author": "BlackLands123", "created_utc": 1746462610, "score": 1, "content": "Thanks a lot! Do you have any docs or YouTube videos in order to reproduce this workflow?"}
{"id": "mqq17v0", "type": "comment", "parent_id": "t1_mqb5cak", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqq17v0/", "author": "None", "created_utc": 1746459364, "score": 1, "content": "[removed]"}
{"id": "mqdhcp8", "type": "comment", "parent_id": "t1_mqd8tha", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqdhcp8/", "author": "alphabet_explorer", "created_utc": 1746281767, "score": 2, "content": "Exactly. I can\u2019t trust this thing. I would be watching every code iteration. I have seen some of the weird paths down the decision tree it takes and it gets stuck in these weird holes recursing into oblivion for an unrelated task doing absolute nonsense."}
{"id": "mq60ve3", "type": "comment", "parent_id": "t1_mq5359z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq60ve3/", "author": "alphabet_explorer", "created_utc": 1746177253, "score": 2, "content": "Okay but do you need an LLM for that though?. Sounds like basic error handling will manage this issue no?"}
{"id": "mqqi9l9", "type": "comment", "parent_id": "t1_mqq17v0", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mqqi9l9/", "author": "webscraping-ModTeam", "created_utc": 1746464386, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mq9id36", "type": "comment", "parent_id": "t1_mq60ve3", "permalink": "https://www.reddit.com/r/webscraping/comments/1kceext/what_ive_learned_after_5_years_in_the_web/mq9id36/", "author": "germs_smell", "created_utc": 1746219175, "score": 2, "content": "I think that the concept is query the LLM for the location in html/css or something that tells you where to scrape. If the LLM returns you garbage and your scrape fails, you do the exception handling with your python script. You can loop through a block of code a set number of times if you want to try the LLM response again before moving to a new section of the script if you want. You could add more logic that changes how you ask the LLM each retry attempt if want. Then something like if my retry attempt count = my max acceptable retries, send \"fuck you\" to the LLM and move on to next block in script. I think there is some point logically where you just need to step in and fix stuff if needed..."}
{"id": "1flgwup", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/", "author": "Sea_Cardiologist_212", "created_utc": 1726851591, "score": 355, "title": "After 2 months learning scraping, I'm sharing what I learned!", "content": "UPDATE: I built a tool to make this super easy: - no more scraping/AI headaches! 1. Don't try putting scraping tools in Lambda. Just admit defeat! 2. Selenium is cool and talked about a lot, but Playwright/Puppeteer/hrequests are new and better. 3. Don't feel like you have to go with Python. The Node.JS scraping community is huge! And more modern advice than Selenium. 4. AI will likely teach you old tricks because it's trained on a lot of old data. Use Medium/google search with timeframe < 1 year. 5. Scraping is about new tricks, as Cloudflare, etc block a lot of scraping tactics. 6. Playwright is super cool! A lot of MS coders brought on from Puppeteer, from what I heard. The stealth plugin doesn't work, however (most stealth plugins don't, in fact!) 7. Find out YOUR browser headers 8. Don't worry about fancy proxies, etc if you're scraping lots of sites at scale. Worry if you're scraping lots of data from one site, or regular data scraping from one site. 9. If you're going to use proxies, use residential ones! (**Update:** people have suggested using mobile proxies. I would suggest using data center, then residential, then mobile as a waterfall-like fallback to keep costs down.) 10. **Find out what your browser headers are (user agent, etc) and mimic the same settings in Playwright!** 11. Use checker tools like \"Am I Headless\" to find out some detection. 12. **Don't try putting things in Lambda! If you like happiness and a work/life balance.** 13. Don't learn scraping avoidance techniques from scraping sites. Learn from the sites that teach detecting these! 14. Put a random delay between requests, 800ms-2s. If the scraping errors, back off a little more and retry a few more seconds later. 15. Browser pools are great! A small EC2 instance will happily run about 5 at a time."}
{"id": "lo5gtpy", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo5gtpy/", "author": "ivanoski-007", "created_utc": 1726886084, "score": 40, "content": "Learn lxml for python , it's extremely fast Learn how to find apis in websites and how to get the data you want Learn threading, to do concurrent requests (on sites that handle it) Selenium is extremely unstable, and sucks resources, it is better to not rely in it, (I use it to get cookies and then I close it) Use your headers, or you'll get banned quickly Learn how to do request get and request post with headers"}
{"id": "lo4cbli", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo4cbli/", "author": "lopnax", "created_utc": 1726869700, "score": 12, "content": "Residential proxies are cheaper but some website/app have a database of those ones. If you don\u2019t want problems and you can pay a bit more go mobile proxies."}
{"id": "lo6vz5l", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6vz5l/", "author": "krimpenrik", "created_utc": 1726917104, "score": 7, "content": "Most important tip, before scraping the content see via chrome inspector where the frontend fetched the data from the backend. Tapping into that endpoint gives you the exact structure you are looking for. If that fails then reverse engineer that structure from the rendered content. If you find that endpoint you don't need puppeteer for the rendered pages."}
{"id": "lo3y3zy", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo3y3zy/", "author": "one-escape-left", "created_utc": 1726864902, "score": 6, "content": "What have you been scraping? Would you share any of the code you wrote?"}
{"id": "lo44tm1", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo44tm1/", "author": "chefkoch-24", "created_utc": 1726867124, "score": 4, "content": "What would you recommend instead of lambda for scheduled jobs?"}
{"id": "lo8jda8", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo8jda8/", "author": "DataShack", "created_utc": 1726939419, "score": 4, "content": "I've been doing web scraping for almost a decade. I admit that trying to do it in Lambda is the biggest mistake of my life."}
{"id": "lo548xw", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo548xw/", "author": "OP_will_deliver", "created_utc": 1726880687, "score": 2, "content": "Thank you! Can you share how to do this? > Find out YOUR browser headers > Find out what your browser meta data is (user agent, etc) and mimic the same settings in Playwright!"}
{"id": "locpiba", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locpiba/", "author": "None", "created_utc": 1727008319, "score": 2, "content": "What is the use of scrapping can you make money out of it?"}
{"id": "lo5skqn", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo5skqn/", "author": "khanosama783", "created_utc": 1726891616, "score": 1, "content": "please share some articles if possibly"}
{"id": "lo627f3", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo627f3/", "author": "Nora-AR", "created_utc": 1726897046, "score": 1, "content": "\u00a1Thank you for this! I have some experience with web scraping but I use the old tools cause I feel more comfortable. I am trying to change this and wanna learn more about the new tools"}
{"id": "lo63wok", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo63wok/", "author": "Prior_Meal_6228", "created_utc": 1726898097, "score": 1, "content": "Can you explain 13 and 14 a little bit"}
{"id": "lo67voa", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo67voa/", "author": "Best_Fish_2941", "created_utc": 1726900627, "score": 1, "content": "3. Do you mean nod js better than python or equal"}
{"id": "lo6gt19", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6gt19/", "author": "ReceptionRadiant6425", "created_utc": 1726906684, "score": 1, "content": "Why not lambda function any specific reason?"}
{"id": "lo6hfzi", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6hfzi/", "author": "Cultural-Arugula-894", "created_utc": 1726907133, "score": 1, "content": "Hello, Is there any way to run cheerio or playwright on Heroku server. I am facing issue with these scraping packages on heroku server. Doesn't work."}
{"id": "lo6nbu3", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6nbu3/", "author": "Salt-Page1396", "created_utc": 1726911363, "score": 1, "content": "\"If you're going to use proxies, use residential ones!\" Try datacenter proxies first. They are hell of a lot cheaper than residential and a lot of the times get the job done."}
{"id": "lo6vm43", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6vm43/", "author": "ConfusionHumble3061", "created_utc": 1726916891, "score": 1, "content": "How fast are the Puppeteer and the other compare to Selenium ? I'm trying to scrape a website but i cannot do with beautifulsoup and i found myself stuck because a link that i'm scrapping is using some X-Amz-Algorithm key who change everytime"}
{"id": "lo7dej9", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo7dej9/", "author": "Salt_Ant107s", "created_utc": 1726925247, "score": 1, "content": "Im so gonna put this in my zotero"}
{"id": "loay2l5", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loay2l5/", "author": "Purple-Control8336", "created_utc": 1726971295, "score": 1, "content": "Why Scrap when GTP is already has all data and Google Bard"}
{"id": "lob0hkh", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lob0hkh/", "author": "coinboi2012", "created_utc": 1726972344, "score": 1, "content": "Just to add, use a lambda framework like SST if you are gonna do serverless. Rawdogging AWS is a horrible idea always"}
{"id": "loclr19", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loclr19/", "author": "RacoonInThePool", "created_utc": 1727006467, "score": 1, "content": "Can you share more about 12 and 15, first time heard about browser pool"}
{"id": "lodkdlg", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lodkdlg/", "author": "pcuser522", "created_utc": 1727019836, "score": 1, "content": "Where the hell was this post months ago. Beautiful info. Been stuck on all of this issues personally litterally last night solved my proxy issue. W post. My one suggestion is this. If cloudflare is blocking you try to reinitialize the driver for each request and typically I\u2019ve been able to load the site and not get hit for a few seconds so I can scrape the data before getting cut off. Works on a fuck ton of sites."}
{"id": "loeyr5g", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loeyr5g/", "author": "boreagami", "created_utc": 1727034922, "score": 1, "content": "Lambda web scraping is working perfect for me with selenium at a large scale."}
{"id": "lofko4m", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lofko4m/", "author": "Limp_Charity4080", "created_utc": 1727041874, "score": 1, "content": "what tools did you use to manage browser pools?"}
{"id": "loh6b59", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loh6b59/", "author": "faz_Lay", "created_utc": 1727064285, "score": 1, "content": "Node.JS scraping community is huge -- seriously !!!"}
{"id": "lohhm4t", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lohhm4t/", "author": "Shad0w_spawn", "created_utc": 1727070637, "score": 1, "content": "I\u2019ve been learning with playwright and trying to make some \u2018generic\u2019 scrapers. Can you explain 7, 10, and 15 a little more? Do you need to regularly update the headers? Are the browser pools replicable in non AWS envs?"}
{"id": "lol2sth", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lol2sth/", "author": "None", "created_utc": 1727124245, "score": 1, "content": "[removed]"}
{"id": "lp2af6b", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lp2af6b/", "author": "Single_Tomato_6233", "created_utc": 1727376430, "score": 1, "content": "I've found a bit of a workaround to using Playwright in Lambda. The trick is to deploy a server with chrome/playwright on EC2 or a similar platform. Then you can connect to it over CDP from your Lambda: browser = await pw.chromium.connect_over_cdp(CDP_URL) Github repo with the Dockerized playwright server here: ["}
{"id": "lp2yzsr", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lp2yzsr/", "author": "dredav", "created_utc": 1727384181, "score": 1, "content": "One question about proxies: How often do you rotate them? Are u using a pool of proxies and pick one per request or are you using on proxy per browser pool instance?"}
{"id": "lroqkwv", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lroqkwv/", "author": "Holiday-Regret-1896", "created_utc": 1728803176, "score": 1, "content": "Need help please: Please check this - [ the problem is i cant scrape ANNOTATION for each sentence there mention # Expecting format like this: ****Ayy, Mustard on the beat, ho (Genius Annotation Los Angeles-based producer Mustard\u2019s signature producer tag is an excerpt of frequent collaborator and Compton artist YG that originated from YG\u2019s 2011 track \u201cI\u2019m Good.\u201dThis is notable because Drake aligned himself with YG in an attempt to discredit Kendrick\u2019s street cred in his then-previous diss track, \u201cFamily Matters\u201d:You know who really bang a set? My nigga YGMustard tweeted the following shortly after the song dropped:I\u2019ll never turn my back on my city \u2026. and I\u2019m fully loadedWhile Mustard shot down the rumor of him sampling Nas' \u201cEther\u201d for the track, the production does feature a sped-up sample from the 1968 track \u201cI Believe To My Soul\u201d by Monk Higgins:) # Deebo any rap nigga, he a free throw ( Genius Annotation Deebo, portrayed by Compton actor Tommy Lister Jr., is a fictional character from the iconic 1995 film Friday. He is depicted as a sociopathic bully that no one in the community is willing to stand up to. This parallels Kendrick\u2019s depiction of Drake in this song and all the previous installments of his Drake diss tracks. However, here, Kendrick is knowingly taking on the persona of a bully. He may also be making a callback to his verse on \u201cLike That,\u201d where he said, \u201cI\u2019m snatchin' chains,\u201d as, in Friday, Deebo snatches a character\u2019s chain.Deebo is also the nickname of NBA player DeMar DeRozan, who played for The Chicago Bulls when this track dropped. The significance of this is Kendrick\u2019s parents came from Chicago. Although DeRozan is from Compton, he has a connection to Drake\u2019s hometown, as he previously played for Toronto Raptors, for whom Drake is an ambassador. Kendrick mentions DeRozan later in the song:I\u2019m glad DeRoz' came home, y'all didn\u2019t deserve him neitherDeRozan is a proficient free throw shooter, with his 84.1% career average only 6.9% short of the record. Therefore, Kendrick is implying that beefing with other rappers is as effortless for him as free throws are for DeRozan.DeRozan went on to cameo in the \u201cNot Like Us\u201d visuals.) # Thank you :) ps. i am non-coder so reply jargon-free"}
{"id": "mb10yoe", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/mb10yoe/", "author": "Any_Pirate_7025", "created_utc": 1738718588, "score": 1, "content": "u/Sea_Cardiologist_212 Intent to download EC2 instances in AWS and working well because, when a failed request, simply eliminate the instance and assign me a new IP. Probe in Hetzner, but you still have to assign the IP misma to create this other instance, and in DigitalOcean you have the misma limitation with the other band. Contracted proxy services, but the majority gives me errors. Did you have a similar experience or recommend a better strategy?"}
{"id": "lo7ef98", "type": "comment", "parent_id": "t3_1flgwup", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo7ef98/", "author": "H4SK1", "created_utc": 1726925642, "score": 0, "content": "13. Can you give a few example of sites you recommend to learn from? 2. In which way do you think Selenium is worse than other browser scraping library, beside asycn?"}
{"id": "lo7rnh0", "type": "comment", "parent_id": "t1_lo5gtpy", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo7rnh0/", "author": "dedikado", "created_utc": 1726930327, "score": 5, "content": "any tips on finding apis in websites?"}
{"id": "lo9kvpf", "type": "comment", "parent_id": "t1_lo5gtpy", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo9kvpf/", "author": "None", "created_utc": 1726952080, "score": 2, "content": "or learn to root android phone and sniff the api used by the website\u201ds apps to scrape directly through api without getting blocked."}
{"id": "lo6riu4", "type": "comment", "parent_id": "t1_lo5gtpy", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6riu4/", "author": "Sea_Cardiologist_212", "created_utc": 1726914254, "score": 1, "content": "Great advice!"}
{"id": "loa2ziq", "type": "comment", "parent_id": "t1_lo5gtpy", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loa2ziq/", "author": "Additional-Target874", "created_utc": 1726958646, "score": 1, "content": "On a site to know drug data and prices, you search for the name or letter, and the search result appears. Does anyone have an idea on how to do web scraping? In order to know how to access the entire drug database or how to create an interface in any language, I write the name of the drug and it searches on this site and the result appears in the interface that I created. Thank you."}
{"id": "lo6rkt2", "type": "comment", "parent_id": "t1_lo4cbli", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6rkt2/", "author": "Sea_Cardiologist_212", "created_utc": 1726914292, "score": 3, "content": "Good advice, and noted! I think a combination of both is good. I'd suggest having a fallback method so if a request fails, to use residential and finally mobile to keep costs/usage down."}
{"id": "lo6y5t0", "type": "comment", "parent_id": "t1_lo6vz5l", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6y5t0/", "author": "Sea_Cardiologist_212", "created_utc": 1726918350, "score": 1, "content": "That's a good idea! API is of course the best way to interact with the data, but also if you're scraping just the one site and they don't have any mechanisms to prevent you doing this way (like a CSRF/signed token or something) then that's perfect!"}
{"id": "lo42int", "type": "comment", "parent_id": "t1_lo3y3zy", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo42int/", "author": "Sea_Cardiologist_212", "created_utc": 1726866354, "score": 24, "content": "I built a tool that you could ask any questions about a company, or many companies at once. It would then go to the website and find the relevant data, and provide a response. You could also give it multiple companies/sites and ask it to find specific information, or filter companies by criteria. Essentially it was to help us find and qualify leads. It used gpt-4o-mini. I'm going to release as open source once I've tested and refined some more!"}
{"id": "lo4e142", "type": "comment", "parent_id": "t1_lo44tm1", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo4e142/", "author": "Single_Advice1111", "created_utc": 1726870320, "score": 11, "content": "A raspberry pi. Hook it up with a free account for rabbitmq or lavinmq and send it back to your server. You\u2019ll need: A raspberry pi with Docker A working api in the cloud to receive data from your workers A place to host your message queue - this can easily be found by using google and there are lots of free options. A docker container or script in your raspberry pi that consumes jobs and sends the results to your api. Quite simple tbh"}
{"id": "locfmbc", "type": "comment", "parent_id": "t1_lo8jda8", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locfmbc/", "author": "Sea_Cardiologist_212", "created_utc": 1727003046, "score": 1, "content": "I have such a love/hate relationship with it!"}
{"id": "locmawp", "type": "comment", "parent_id": "t1_lo8jda8", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locmawp/", "author": "RacoonInThePool", "created_utc": 1727006749, "score": 1, "content": "Never use lambda before, why lambda is the biggest mistake to you"}
{"id": "lo6si9h", "type": "comment", "parent_id": "t1_lo548xw", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6si9h/", "author": "Sea_Cardiologist_212", "created_utc": 1726914920, "score": 5, "content": "If you look up \"what are my browser request headers\" in Google, loads of sites offer this. Here is the code I use to initiate Playwright: const browser = await chromium.launch({ args: [\"--no-sandbox\", \"--disable-setuid-sandbox\"], ignoreHTTPSErrors: true, }); browser = await getBrowser(browserIndex); if (!browser) { console.error(\"Browser instance is null or undefined\"); throw new Error(\"Failed to get a browser instance\"); } context = await browser.newContext({ userAgent }); // userAgent is my agent set earlier page = await context.newPage(); await page.setViewportSize({ width: 1900, height: 728 }); await page.setExtraHTTPHeaders({ webdriver: \"false\", \"sec-ch-ua\": '\"Chromium\";v=\"128\", \"Not;A=Brand\";v=\"24\", \"Google Chrome\";v=\"128\"', // Change these based on your headers \"sec-ch-ua-form-factors\": '\"Desktop\"', \"Accept-Language\": \"en-US,;q=0.9,en;q=0.8\", // Change these based on your headers }); Change the variables of course, based on your headers"}
{"id": "loql5xq", "type": "comment", "parent_id": "t1_locpiba", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loql5xq/", "author": "ABQFlyer", "created_utc": 1727205545, "score": 3, "content": "Scrapping? Like throwing away? Or scraping?"}
{"id": "lo7hqo2", "type": "comment", "parent_id": "t1_lo63wok", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo7hqo2/", "author": "sudodoyou", "created_utc": 1726926899, "score": 4, "content": "I can provide insight: 13. OP is saying that if you learn how to detect scraping then you\u2019ll be better at avoiding detection. If you merely try to learn to learn avoid detection, you\u2019ll likely miss other techniques. 14. If you\u2019re mimicking normal requests, you will not get a user request exactly every 5 seconds, it will appear to be more randomly distributed. So when you request to pull data from a website, grab the data at random intervals."}
{"id": "lo6rh89", "type": "comment", "parent_id": "t1_lo67voa", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6rh89/", "author": "Sea_Cardiologist_212", "created_utc": 1726914224, "score": 1, "content": "It's whatever you are most comfortable with, tbh... I know both and say NodeJS has a more mature community and generally more modern approaches. Because Python is easy and accessible, plus Selenium with python has been around forever, a lot of the online guidance is quite dated. Playwright/Puppeteer is more \"recent\" so I preferred to go with it."}
{"id": "lo6isqs", "type": "comment", "parent_id": "t1_lo6gt19", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6isqs/", "author": "Sensi1093", "created_utc": 1726908093, "score": 1, "content": "Scraping is not a good fit for lambda because you\u2019ll probably have something to scrape all the time and the load is usually pretty constant. Lambda shines when you either have spiky load or need/want to scale to zero. Neither of those things apply for the usual Webscraping load."}
{"id": "locfy75", "type": "comment", "parent_id": "t1_lo6gt19", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locfy75/", "author": "Sea_Cardiologist_212", "created_utc": 1727003245, "score": 1, "content": "It doesn't behave too well with the web drivers that Chromium needs to run. Generally complex operations should stay away from Lambda and in something like Fargate."}
{"id": "lo6skjr", "type": "comment", "parent_id": "t1_lo6nbu3", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6skjr/", "author": "Sea_Cardiologist_212", "created_utc": 1726914962, "score": 1, "content": "Yes, agreed. Suggest data center, then residential, then mobile fallback on failures"}
{"id": "lo6xzd3", "type": "comment", "parent_id": "t1_lo6vm43", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6xzd3/", "author": "Sea_Cardiologist_212", "created_utc": 1726918250, "score": 1, "content": "I don't suggest to have speed as an objective or you'll trigger defence mechanisms on sites you try to scrape too quickly. My preferred route is to have concurrent/parallel requests to multiple sites at one time."}
{"id": "locgcx3", "type": "comment", "parent_id": "t1_loay2l5", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locgcx3/", "author": "Sea_Cardiologist_212", "created_utc": 1727003487, "score": 1, "content": "Bard is now Gemini, and together with GPT they have cut-off dates, plus they don't always have all the information or relevant information that you require. If I'm taking the data from a company website, it's probably going to be accurate, whereas if it is scraped from some random person on YouTube (which trained a lot of GPT data), it may not be so accurate. Plus look up AI Hallucination, it's quite common (at least, for now). We published a whole whitepaper on it!"}
{"id": "locgj84", "type": "comment", "parent_id": "t1_lob0hkh", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locgj84/", "author": "Sea_Cardiologist_212", "created_utc": 1727003589, "score": 1, "content": "I would generally say yes, use SST\u2014we use it for some of our NextJS projects. Vercel is hosted on Lambda too, I discovered! SST is awesome, but I'd say it is more for full-stack applications."}
{"id": "locnrwj", "type": "comment", "parent_id": "t1_loclr19", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locnrwj/", "author": "Sea_Cardiologist_212", "created_utc": 1727007482, "score": 2, "content": "12 I have discussed a fair amount in other comments, but generally speaking Lambda is a limited virtual environment with strict hardware restrictions that Chromium generally needs to run. 15: async function initializeBrowserPool(setConcurrent) { MAX_CONCURRENT = setConcurrent; for (let i = 0; i < MAX_CONCURRENT; i++) { const browser = await chromium.launch({ args: [\"--no-sandbox\", \"--disable-setuid-sandbox\"], ignoreHTTPSErrors: true, }); browserPool.push(browser); } } async function getBrowser(index) { if (browserPool.length === 0) { await initializeBrowserPool(); } return browserPool[index % MAX_CONCURRENT]; } const queue = []; for (let i = 0; i < rows.length; i++) { const row = rows[i]; // If we've reached the maximum number of concurrent tasks, wait for one to finish if (queue.length >= MAX_CONCURRENT) { await Promise.race(queue); // Remove the completed task from the queue const index = queue.findIndex((p) => p.status !== \"pending\"); if (index !== -1) { queue.splice(index, 1); } } // Start a new task and add it to the queue const browserIndex = queue.length; // Use the current queue length as the browser index const task = processRow(sheet, row, i, browserIndex); queue.push(task); // Immediately add error handling so we don't lose track of the promise task.catch((error) => console.error(`Unhandled error in task for row ${i}:`, error), ); // Sleep, and on to the next! await new Promise(resolve => setTimeout(resolve, 300));"}
{"id": "lohpdbb", "type": "comment", "parent_id": "t1_lofko4m", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lohpdbb/", "author": "Sea_Cardiologist_212", "created_utc": 1727075754, "score": 1, "content": "I just did this with Node.js using promises but it's also possible in Python. You can concurrent request anything."}
{"id": "lohpgkl", "type": "comment", "parent_id": "t1_lohhm4t", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lohpgkl/", "author": "Sea_Cardiologist_212", "created_utc": 1727075816, "score": 1, "content": "I responded to this in another comment with some example code, take a look and let me know if you get stuck"}
{"id": "lomiu79", "type": "comment", "parent_id": "t1_lol2sth", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lomiu79/", "author": "webscraping-ModTeam", "created_utc": 1727142339, "score": 1, "content": "Thank you for contributing to r/webscraping! Referencing paid products or services is generally discouraged, as such your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "locgvke", "type": "comment", "parent_id": "t1_lo7ef98", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locgvke/", "author": "Sea_Cardiologist_212", "created_utc": 1727003792, "score": 4, "content": "13. I used Medium (paid subscription) and also google search a lot, I filtered results only in 2024 as I wanted recent knowledge only. In all honesty, the advice generally isn't very good out there... it's all quite dated. Some say use the stealth plugin, etc which doesn't even work. I would suggest trial and error. It's your best friend in this! 2. \"Worse\" is perhaps not the right word, but it's quite dated, and I feel Playwright/Puppeteer/hrequests have been built with a more modern approach. It's my opinion, not necessarily fact. The main reason is that a lot of traditional scraping techniques were based on Selenium, so finding modern/accurate/reliable tutorials/guidance is VERY hard. You could be digging out an article generated by AI that was based on data from 10-15 years ago that is now mostly-redundant. I followed so many tutorials that took me to dead-ends!"}
{"id": "lo9nmd8", "type": "comment", "parent_id": "t1_lo7rnh0", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo9nmd8/", "author": "None", "created_utc": 1726953013, "score": 9, "content": "Network tab of dev tools!"}
{"id": "locf7eq", "type": "comment", "parent_id": "t1_lo9kvpf", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locf7eq/", "author": "Sea_Cardiologist_212", "created_utc": 1727002794, "score": 2, "content": "Great idea and advice! That's next level reverse engineering and very forward-thinking, I love it!"}
{"id": "loavn2k", "type": "comment", "parent_id": "t1_lo9kvpf", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loavn2k/", "author": "Glittering_Push8905", "created_utc": 1726970264, "score": 1, "content": "How to sniff the api"}
{"id": "lobsnjv", "type": "comment", "parent_id": "t1_lo9kvpf", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lobsnjv/", "author": "ivanoski-007", "created_utc": 1726987459, "score": 1, "content": "What does rooting android have anything to do with it?"}
{"id": "lobsqzm", "type": "comment", "parent_id": "t1_loa2ziq", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lobsqzm/", "author": "ivanoski-007", "created_utc": 1726987521, "score": 1, "content": "You either use the search function, have a list of url, find a hidden api or make a web crawler"}
{"id": "lo4llpf", "type": "comment", "parent_id": "t1_lo42int", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo4llpf/", "author": "lopnax", "created_utc": 1726873228, "score": 3, "content": "Is it worldwide? What type of information gives? Structured data?"}
{"id": "lo9agp6", "type": "comment", "parent_id": "t1_lo42int", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo9agp6/", "author": "rclabo", "created_utc": 1726948584, "score": 2, "content": "Any way to get in a list to be notified when you release it open source?"}
{"id": "lo6f5vs", "type": "comment", "parent_id": "t1_lo42int", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6f5vs/", "author": "deadcoder0904", "created_utc": 1726905527, "score": 1, "content": "Wow, how much did this cost? Also, what were the gtp-4o cost? I assume the latter is cheap af."}
{"id": "lo6s3y1", "type": "comment", "parent_id": "t1_lo4e142", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6s3y1/", "author": "Sea_Cardiologist_212", "created_utc": 1726914652, "score": 2, "content": "Lambda is just not so good dealing with web drivers, etc and many people have tried hacking it around to get it on lambda. We did get selenium on it after a lot of hassle but even then it's old Chromium and discovered most sites can detect this now, it's just not worth it. I use an EC2 instance I spin up just for the exercise, ran from a docker container, and shut it down again after I've finished using it. I tried the small EC2 on AWS free tier but it kept crashing it unless 1 site at a time, so I span up a lightweight small server (can't remember exact specs!) You can use EKS/Fargate/etc to spin these servers up at scale, I believe it will work ok in this if you want the same kind of results as lambda. Costs a bit more but ultimately still scaleable/\"serverless\". I haven't tried this route yet though. I have a guy on my team that seems to think it's quite easily possible, however!"}
{"id": "lor7qmb", "type": "comment", "parent_id": "t1_loql5xq", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lor7qmb/", "author": "None", "created_utc": 1727212631, "score": 1, "content": "Sorry. Web scraping."}
{"id": "locfuue", "type": "comment", "parent_id": "t1_lo7hqo2", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locfuue/", "author": "Sea_Cardiologist_212", "created_utc": 1727003189, "score": 1, "content": "Yes, thanks u/sudodoyou - spot on! A delay will mimic the typical user that will load a site, look for a link/content and then click on it. I always try to imagine how a real person would behave on these sites."}
{"id": "lo6jaij", "type": "comment", "parent_id": "t1_lo6isqs", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6jaij/", "author": "ReceptionRadiant6425", "created_utc": 1726908448, "score": 1, "content": "For instance my scraper runs only 4 times a day for an average of 10 minutes, still lambda is a bad option for that or not. I do not need to scrape the data 24x7 for the business logic I am currently working on."}
{"id": "loawewx", "type": "comment", "parent_id": "t1_lo6skjr", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loawewx/", "author": "Glittering_Push8905", "created_utc": 1726970591, "score": 1, "content": "But afaik proxy works on monthly subscription not bandwidth used ?"}
{"id": "lochac6", "type": "comment", "parent_id": "t1_locgcx3", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lochac6/", "author": "Purple-Control8336", "created_utc": 1727004033, "score": 1, "content": "Thanks makes sense for now. Google is Father of all Data\u2026"}
{"id": "loauvet", "type": "comment", "parent_id": "t1_lo9nmd8", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loauvet/", "author": "Glittering_Push8905", "created_utc": 1726969938, "score": 3, "content": "I want to exactly learn this systematically but I could never find resources"}
{"id": "loj6ryg", "type": "comment", "parent_id": "t1_locf7eq", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loj6ryg/", "author": "None", "created_utc": 1727102845, "score": 1, "content": "Yeah, i use it already to create bot or track some websites. ;). crawling by using API will not get you blocked and you can bypass captchas ."}
{"id": "lobj646", "type": "comment", "parent_id": "t1_loavn2k", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lobj646/", "author": "None", "created_utc": 1726981570, "score": 2, "content": "Use Chales proxy and install Charles root ca on rooted android phone. You can see all traffic in plain text in Charles."}
{"id": "lobw4s0", "type": "comment", "parent_id": "t1_lobsnjv", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lobw4s0/", "author": "None", "created_utc": 1726989750, "score": 1, "content": "because it is not possible to use custom CA with Apps traffic without rooting the phone and use this magisk addon:"}
{"id": "lo6rvab", "type": "comment", "parent_id": "t1_lo4llpf", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6rvab/", "author": "Sea_Cardiologist_212", "created_utc": 1726914491, "score": 3, "content": "Worldwide, and you can ask gpt4o mini to produce structured json, it's good at it too! I sometimes use it to parse output from other models that sometimes fail."}
{"id": "locfejb", "type": "comment", "parent_id": "t1_lo9agp6", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locfejb/", "author": "Sea_Cardiologist_212", "created_utc": 1727002915, "score": 2, "content": "I'll probs announce on X when I do, but don't have a list at the moment sorry."}
{"id": "lo6ru41", "type": "comment", "parent_id": "t1_lo6f5vs", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6ru41/", "author": "Sea_Cardiologist_212", "created_utc": 1726914469, "score": 4, "content": "I use gpt4o-mini which is super cheap. You can also use BeautifulSoap or Cheerio to parse HTML first and remove a lot of noise/attributes from tags/etc that you won't likely need. I also put in code to strip out scripts, css, RSS, other assets, etc as I didn't want to send it to gpt4o-mini. GPT4o mini is good to use because it doesn't have a huge LLM with lots of parameters to sift through (hence it is cheap too) - it's the NLP you're after really, which is great for such a lightweight model - essentially you're only requesting basic reasoning and parsing data you give it, so it's good for this purpose. I think it cost about $1 to parse around 600 sites, including sub pages. I put logic in to only get to the pages that hold the information I need so it wasn't parsing EVERYTHING!"}
{"id": "loce8wt", "type": "comment", "parent_id": "t1_lo6s3y1", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loce8wt/", "author": "youdig_surf", "created_utc": 1727002190, "score": 1, "content": "Lambda is crap to setup if you want to add library, i spend too much Time packing a docker container with aws linux distro for this mess and setting it up just to get a random error at this end."}
{"id": "lovdv5x", "type": "comment", "parent_id": "t1_lor7qmb", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lovdv5x/", "author": "OkuboTV", "created_utc": 1727279258, "score": 1, "content": "Plenty of people would pay to have qualified leads. Doctors, Dentists, Lawyers. That's just a specific niche. Businesses would pay to get data on competitors. You can get a good amount of data from public sites through web scraping. Scraping is just accumulating data. Data is data. There's an inifinite amount of use cases with good data."}
{"id": "lo6jq36", "type": "comment", "parent_id": "t1_lo6jaij", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6jq36/", "author": "Sensi1093", "created_utc": 1726908760, "score": 1, "content": "Sure lambda is fine for that. Just be aware of the 15m execution limitation and 10GB memory limitation, both are hard limits for lambda. If you plan to go beyond that, maybe look into other solutions which allow short lived / task based execution like ECS"}
{"id": "locg3ua", "type": "comment", "parent_id": "t1_loawewx", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locg3ua/", "author": "Sea_Cardiologist_212", "created_utc": 1727003339, "score": 2, "content": "Yes, but you pay for the IPs/slots, so say you were scraping 15 sites at once and you had 5 IP addresses to rotate for mobile proxy, you would want to keep them as free as possible for those that need them, or you'd end up with quite a big queue or paying for a lot of IP addresses. Of course; that's only if you're scraping multiple sites. Some charge on bandwidth, some charge on monthly."}
{"id": "lochrjc", "type": "comment", "parent_id": "t1_lochac6", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lochrjc/", "author": "Sea_Cardiologist_212", "created_utc": 1727004308, "score": 1, "content": "They've been at it a long time, and store up to 30 versions of a site's page - it's wild, how much data they must have! Then on top the data they collect through their DNS servers, Google Phone, Chrome, Maps, etc - I can't imagine any company that has more data than they do!"}
{"id": "lodkriz", "type": "comment", "parent_id": "t1_loauvet", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lodkriz/", "author": "None", "created_utc": 1727019965, "score": 7, "content": "selective fall run workable familiar chief innocent provide repeat wild *This post was mass deleted and anonymized with [Redact]("}
{"id": "lobsl6r", "type": "comment", "parent_id": "t1_loauvet", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lobsl6r/", "author": "ivanoski-007", "created_utc": 1726987416, "score": 1, "content": "Not all sites have them, some do and some don't"}
{"id": "locf5qm", "type": "comment", "parent_id": "t1_loauvet", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locf5qm/", "author": "Sea_Cardiologist_212", "created_utc": 1727002766, "score": 1, "content": "Load developer tools in Chrome and find the network tab, look at the things being loaded in there and see if any return structured data like JSON. Then you can copy that URL and reverse engineer by looking at pagination (page=3, start=2024-03-01, etc, etc...)"}
{"id": "loim8ar", "type": "comment", "parent_id": "t1_lobj646", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loim8ar/", "author": "balanciagas", "created_utc": 1727095386, "score": 1, "content": "can setup & use mitmproxy on your mac/linux to achieve the same result for iphone"}
{"id": "locfc2t", "type": "comment", "parent_id": "t1_lobw4s0", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locfc2t/", "author": "Sea_Cardiologist_212", "created_utc": 1727002873, "score": 1, "content": "I feel technically you could set up a laptop as a hotspot/proxy with a log of traffic and capture the requests this way also..."}
{"id": "locfjgy", "type": "comment", "parent_id": "t1_loce8wt", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/locfjgy/", "author": "Sea_Cardiologist_212", "created_utc": 1727003000, "score": 2, "content": "Yes, Lambda is a very limited \"virtualized\" environment, so complex things like drivers, etc. don't work so well. Especially when it comes to rendering or things that would typically use the graphics card or other complex hardware features. It is possible, but not worth the effort because by the time you've packaged the driver and got it working, a new version is out and required to do the job effectively."}
{"id": "lo6kfvy", "type": "comment", "parent_id": "t1_lo6jq36", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lo6kfvy/", "author": "ReceptionRadiant6425", "created_utc": 1726909273, "score": 1, "content": "Sure will do that. Thanks"}
{"id": "lomavus", "type": "comment", "parent_id": "t1_lodkriz", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lomavus/", "author": "Glittering_Push8905", "created_utc": 1727139354, "score": 1, "content": "Wow thank you so much"}
{"id": "lzq83ep", "type": "comment", "parent_id": "t1_lodkriz", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lzq83ep/", "author": "None", "created_utc": 1732982838, "score": 1, "content": "What did you suggest here?"}
{"id": "md0lksv", "type": "comment", "parent_id": "t1_lodkriz", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/md0lksv/", "author": "YourKoolPal", "created_utc": 1739676620, "score": 1, "content": "u/-267- What did you suggest here? This post was mass deleted and anonymized with Redact"}
{"id": "loj7y0t", "type": "comment", "parent_id": "t1_lobsl6r", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loj7y0t/", "author": "None", "created_utc": 1727103226, "score": 1, "content": "Yes, you are right. I had same problem. i get blocked with captchas and so on. Most websites have mobile Apps and you can install the apps on rooted android phones/emulators and use proxy like http toolkit/Chales to sniff the apis. Plus points: - the server can\u2019t distinguish between apps and bot and you dont have to deal with captchas . - structured data"}
{"id": "loiozbr", "type": "comment", "parent_id": "t1_loim8ar", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loiozbr/", "author": "None", "created_utc": 1727096493, "score": 1, "content": "I dont know, if it can be done with iphone. Android dont use user certificates by default."}
{"id": "loj6gcy", "type": "comment", "parent_id": "t1_locfc2t", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/loj6gcy/", "author": "None", "created_utc": 1727102738, "score": 1, "content": "or use android emulator with vanilla Android and http toolkit . works flawlessly :)"}
{"id": "lwblowu", "type": "comment", "parent_id": "t1_lomavus", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lwblowu/", "author": "SaoolDaLegend", "created_utc": 1731192243, "score": 1, "content": "Hey, what was the service he suggested? The original comment got deleted."}
{"id": "lon7f1t", "type": "comment", "parent_id": "t1_loiozbr", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lon7f1t/", "author": "BeginningWaltz9766", "created_utc": 1727152902, "score": 1, "content": "Toy can install certs on iPhone too."}
{"id": "londn1v", "type": "comment", "parent_id": "t1_lon7f1t", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/londn1v/", "author": "None", "created_utc": 1727156421, "score": 1, "content": "Installing certificates will not help if ios prevent apps to use those certificates. Are you share Apps use them ?"}
{"id": "lot9z0f", "type": "comment", "parent_id": "t1_londn1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1flgwup/after_2_months_learning_scraping_im_sharing_what/lot9z0f/", "author": "BeginningWaltz9766", "created_utc": 1727242488, "score": 1, "content": "Yes..checkout the app named surge on app store. Everything works out of the box."}
{"id": "1j0x8wy", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/", "author": "convicted_redditor", "created_utc": 1740827616, "score": 330, "title": "I published my 3rd python lib for stealth web scraping", "content": "Hey everyone, I published my 3rd pypi lib and it's open source. It's called **stealthkit** \\- requests on steroids. Good for those who want to send http requests to websites that might not allow it through programming - like amazon, yahoo finance, stock exchanges, etc. **What My Project Does** * **User-Agent Rotation**: Automatically rotates user agents from Chrome, Edge, and Safari across different OS platforms (Windows, MacOS, Linux). * **Random Referer Selection**: Simulates real browsing behavior by sending requests with randomized referers from search engines. * **Cookie Handling**: Fetches and stores cookies from specified URLs to maintain session persistence. * **Proxy Support**: Allows requests to be routed through a provided proxy. * **Retry Logic**: Retries failed requests up to three times before giving up. * **RESTful Requests**: Supports GET, POST, PUT, and DELETE methods with automatic proxy integration. **Why did I create it?** In 2020, I created a yahoo finance lib and it required me to tweak python's requests module heavily - like session, cookies, headers, etc. In 2022, I worked on my django project which required it to fetch amazon product data; again I needed requests workaround. This year, I created second pypi - amzpy. And I soon understood that all of my projects evolve around web scraping and data processing. So I created a separate lib which can be used in multiple projects. And I am working on another stock exchange python api wrapper which uses this module at its core. It's open source, and anyone can fork and add features and use the code as s/he likes. If you're into it, please let me know if you liked it. Pypi: [ Github: [ **Target Audience** Developers who scrape websites blocked by anti-bot mechanisms. **Comparison** So far I don't know of any pypi packages that does it better and with such simplicity."}
{"id": "mffdenf", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffdenf/", "author": "None", "created_utc": 1740835283, "score": 26, "content": "What about curl_cffi ?"}
{"id": "mfficxg", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfficxg/", "author": "SurenGuide", "created_utc": 1740837246, "score": 9, "content": "I see only using fakeuseragent and some referer from search engine. What's make it stealth?"}
{"id": "mff024j", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mff024j/", "author": "archieyang", "created_utc": 1740828850, "score": 5, "content": "Can this library work as an HTTP proxy alternative for web scraping?"}
{"id": "mffh9ub", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffh9ub/", "author": "maty2200", "created_utc": 1740836826, "score": 7, "content": "What about [Crawlee]( How does it compare?"}
{"id": "mff2ql5", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mff2ql5/", "author": "_okayash_", "created_utc": 1740830326, "score": 2, "content": "Interesting! What advantages does it offer over cloudscraper?"}
{"id": "mffbgbi", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffbgbi/", "author": "Koninhooz", "created_utc": 1740834469, "score": 2, "content": "Fantastic!"}
{"id": "mffua1p", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffua1p/", "author": "Violin-dude", "created_utc": 1740841377, "score": 2, "content": "Stupid question: what web scraping libraries are best for websites not protected against scrapers? Library should be as user friendly as possible"}
{"id": "mffymza", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffymza/", "author": "hrdcorbassfishin", "created_utc": 1740842774, "score": 2, "content": "I've been trying to get windsurf and cursor to convert this simple Reddit scraper js file to a python equivalent and it's been a nightmare. To be fair I'm not doing any coding, just trying to articulate my way to working code which clearly isn't working.. but I'm going to check this out and I am hopeful it'll get me what I need from Reddit :) thanks"}
{"id": "mfje0re", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfje0re/", "author": "None", "created_utc": 1740881854, "score": 2, "content": "[deleted]"}
{"id": "mfkcnmv", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfkcnmv/", "author": "Jungypoo", "created_utc": 1740896033, "score": 2, "content": "Nice work, will give this a try soon :)"}
{"id": "mflnhx4", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mflnhx4/", "author": "Project_Nile", "created_utc": 1740922049, "score": 2, "content": "Can it crawl LinkedIn Public Profiles?"}
{"id": "mfm206f", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfm206f/", "author": "_Khairos_", "created_utc": 1740927301, "score": 2, "content": "Thanks for making this! Does it also work for dynamic JS sites?"}
{"id": "mff0ha4", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mff0ha4/", "author": "trankhaihoang", "created_utc": 1740829091, "score": 2, "content": "Interested"}
{"id": "mfhep7a", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfhep7a/", "author": "LoadingALIAS", "created_utc": 1740858392, "score": 1, "content": "How does this differ from stealth requests? That\u2019s using curl_cffi and is async?"}
{"id": "mfl3i3r", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfl3i3r/", "author": "DENSELY_ANON", "created_utc": 1740911946, "score": 1, "content": "This sounds great tbh. I've been in this space a while now. Excited to test it. With the random selector concept, does this help avoid cloudflare etc that relies on capturing robot like behaviour? I'm really interested in the selector stuff. Thank you"}
{"id": "mfunjrj", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfunjrj/", "author": "Peter1Pan2233", "created_utc": 1741037600, "score": 1, "content": "Can you tool help me understand how many orders an online shop gets? Example: I would like yup know the number of orders for a specified time period for a specified url. Would that be possible?"}
{"id": "mg0hx7j", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mg0hx7j/", "author": "Acceptable_Set_4392", "created_utc": 1741116401, "score": 1, "content": "Does it work if I need to login to a site (say flipkart) as a user, then I want to extract the cookies and other headers for that access, and then make a request to a site API (say flipkart/cart/api) later? If so, how?"}
{"id": "mgstoc3", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mgstoc3/", "author": "cnydox", "created_utc": 1741493436, "score": 1, "content": "Can this crawl LinkedIn pfp? I'm in need to crawl a lot of them"}
{"id": "mffkpld", "type": "comment", "parent_id": "t3_1j0x8wy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffkpld/", "author": "tradegreek", "created_utc": 1740838110, "score": 1, "content": "Currently traveling in Mexico but will definitely give this a look when I\u2019m back"}
{"id": "mfh2kuy", "type": "comment", "parent_id": "t1_mffdenf", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfh2kuy/", "author": "convicted_redditor", "created_utc": 1740854736, "score": 8, "content": "That's a great point! curl\\_cffi focuses on low-level TLS fingerprinting, which is crucial for bypassing advanced anti-bot measures that analyze network traffic. StealthKit, on the other hand, operates at the application level, managing headers, cookies, and user-agent rotation for general stealth."}
{"id": "mfh2vkj", "type": "comment", "parent_id": "t1_mfficxg", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfh2vkj/", "author": "convicted_redditor", "created_utc": 1740854823, "score": 3, "content": "Apart from fakeuseragent, `StealthKit` also handles cookie management for session persistence, implements retry logic to mimic natural browsing, and provides a framework to easily add custom headers. These elements collectively make requests less obviously automated. While not foolproof against advanced detection, it's designed to raise the bar against basic bot detection methods."}
{"id": "mfh0xy3", "type": "comment", "parent_id": "t1_mff024j", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfh0xy3/", "author": "convicted_redditor", "created_utc": 1740854249, "score": 2, "content": "`StealthKit` is designed to make your web scraping sessions appear more like real user activity, which helps avoid detection based on request headers and session behavior. It does this by rotating user agents, managing cookies, and randomizing referers. `StealthKit` can actually work *with* proxies. You can provide your proxy details to `StealthKit`, and it will route your requests through those proxies, combining the benefits of both approaches."}
{"id": "mfqlqiv", "type": "comment", "parent_id": "t1_mffh9ub", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfqlqiv/", "author": "convicted_redditor", "created_utc": 1740980999, "score": 1, "content": "Crawlee compares with Beautifulsoup, mine is a requests wrapper used before scrapping."}
{"id": "mffd7sj", "type": "comment", "parent_id": "t1_mff2ql5", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mffd7sj/", "author": "Typical-Armadillo340", "created_utc": 1740835204, "score": 12, "content": "These projects serve two entirely different use cases. Cloudscraper is designed specifically for bypassing Cloudflare\u2019s protections, but it has been abandoned. In contrast, this project aims to reduce detectability during scraping, even though its current methods are fairly basic. While simply rotating user-agents and setting referrer headers won't fool sophisticated anti-bot systems, consider this: sending 100,000 GET requests with the same headers and IP address will be quickly detected by a site owner. By using this project, you can send those 100,000 requests with varied headers(user agent, referrer) and different IP addresses, making them appear as if they originate from distinct clients."}
{"id": "mff8fmg", "type": "comment", "parent_id": "t1_mff2ql5", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mff8fmg/", "author": "Runthescript", "created_utc": 1740833141, "score": 1, "content": "Clone it and find out"}
{"id": "mfgg5tf", "type": "comment", "parent_id": "t1_mffua1p", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfgg5tf/", "author": "kemijo", "created_utc": 1740848064, "score": 2, "content": "If using Python, BeautifulSoup will let you scrape from the html of a page after it\u2019s been loaded. Selenium will let you automate a web browser, letting you scrape from whatever is displayed. Another one similar to that is Playwright, which I\u2019ve heard is good. I\u2019d probably start with Playwright if it were me."}
{"id": "mfgdhel", "type": "comment", "parent_id": "t1_mffymza", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfgdhel/", "author": "kemijo", "created_utc": 1740847277, "score": 2, "content": "What are you trying to scrape? Newbie here as well but if you want to scrape Reddit with python check out the praw package, pretty easy to use. If using an LLM ask it to build a python Reddit scraper using praw, that should get you the post data, and then you can filter the data how you want."}
{"id": "mfk4u54", "type": "comment", "parent_id": "t1_mfje0re", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfk4u54/", "author": "convicted_redditor", "created_utc": 1740892260, "score": 1, "content": "Stealthkey handles user agent rotation through fake\\_useragent(another pypi lib) and I have pre-selected them to be Chrome, Edge, or Safari and OS is also pre-selected. It also supports multiple proxies which you can insert into as a list."}
{"id": "mfql46z", "type": "comment", "parent_id": "t1_mflnhx4", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfql46z/", "author": "convicted_redditor", "created_utc": 1740980683, "score": 2, "content": "Yes, I just tried it. It works without even cookies or proxies."}
{"id": "mfqjvs8", "type": "comment", "parent_id": "t1_mfl3i3r", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfqjvs8/", "author": "convicted_redditor", "created_utc": 1740980061, "score": 1, "content": "I haven't tested on cloudflare wall but I don't think it'll work on it. Maybe someone can contribute this feature. :)"}
{"id": "mfh5hjg", "type": "comment", "parent_id": "t1_mfh2kuy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfh5hjg/", "author": "None", "created_utc": 1740855590, "score": 1, "content": "So can you crawl Reddit, expedia.co.uk and tripadvisor.co.uk ? Tough nuts to crack."}
{"id": "mfrhrsg", "type": "comment", "parent_id": "t1_mfh2kuy", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfrhrsg/", "author": "scrapeway", "created_utc": 1741000348, "score": 1, "content": "Maybe you can integrate it with curl\\_cffi? That would be very useful!"}
{"id": "mfh3er3", "type": "comment", "parent_id": "t1_mfgdhel", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfh3er3/", "author": "convicted_redditor", "created_utc": 1740854979, "score": 3, "content": "or you can just add .json at the end of any reddit post or subreddit."}
{"id": "mfgjx2g", "type": "comment", "parent_id": "t1_mfgdhel", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfgjx2g/", "author": "hrdcorbassfishin", "created_utc": 1740849175, "score": 2, "content": "Basically search subreddits for keywords and for each post get all the comments. I'll check out praw"}
{"id": "mfnq0di", "type": "comment", "parent_id": "t1_mfk4u54", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfnq0di/", "author": "DefiantScarcity3133", "created_utc": 1740945108, "score": 1, "content": "let say I want to scrape google search result page, I have noticed using different agent causes different html leading to my scrapping failed. How to tackle this?"}
{"id": "mgbatmn", "type": "comment", "parent_id": "t1_mfql46z", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mgbatmn/", "author": "Project_Nile", "created_utc": 1741261238, "score": 1, "content": "Can you pls provide example code?"}
{"id": "mfi0uz9", "type": "comment", "parent_id": "t1_mfh5hjg", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfi0uz9/", "author": "mouad_war", "created_utc": 1740865324, "score": 3, "content": "use rnet"}
{"id": "mfh6mfj", "type": "comment", "parent_id": "t1_mfh5hjg", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfh6mfj/", "author": "convicted_redditor", "created_utc": 1740855931, "score": 2, "content": "I have crawled reddit without any anti-bot setup with PRAW. And I have used stealthkit to crawl stock exchanges and amazon."}
{"id": "mfzjorx", "type": "comment", "parent_id": "t1_mfh5hjg", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfzjorx/", "author": "DEMORALIZ3D", "created_utc": 1741106920, "score": 1, "content": "And currys.co.uk... effing cloudflare"}
{"id": "mfqlx7p", "type": "comment", "parent_id": "t1_mfnq0di", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfqlx7p/", "author": "convicted_redditor", "created_utc": 1740981095, "score": 1, "content": "That's why I have preselected random UA between OS (PC,Mac) and Browsers (Edge, Chrome, etc) and they alone have many combinations. For more frequent requests, please use proxies."}
{"id": "mfi1ezo", "type": "comment", "parent_id": "t1_mfi0uz9", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfi1ezo/", "author": "None", "created_utc": 1740865499, "score": 1, "content": "What\u2019s that? Got a sample script with output for those sites?"}
{"id": "mfha484", "type": "comment", "parent_id": "t1_mfh6mfj", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfha484/", "author": "None", "created_utc": 1740856997, "score": 3, "content": "Well of course PRAW is their standard API \u2014 that\u2019s not scraping IIRC. If you can scrape those three with your stealth kit, colour me impressed \u2026"}
{"id": "mflpeqk", "type": "comment", "parent_id": "t1_mfi1ezo", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mflpeqk/", "author": "mouad_war", "created_utc": 1740922799, "score": 5, "content": "["}
{"id": "mfql6qe", "type": "comment", "parent_id": "t1_mfha484", "permalink": "https://www.reddit.com/r/webscraping/comments/1j0x8wy/i_published_my_3rd_python_lib_for_stealth_web/mfql6qe/", "author": "convicted_redditor", "created_utc": 1740980718, "score": 1, "content": "I tried expedia and it didn't work. Does curl\\_cffi work here?"}
{"id": "1n5t3p2", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/", "author": "0xReaper", "created_utc": 1756743528, "score": 282, "title": "Scrapling v0.3 - Solve Cloudflare automatically and a lot more!", "content": "Excited to announce Scrapling v0.3 - The most significant update yet! After months of development, we've completely rebuilt Scrapling from the ground up with revolutionary features that change how we approach web scraping: **AI-Powered Web Scraping:** Built-in MCP Server integrates directly with Claude, ChatGPT, and other AI chatbots. Now you can scrape websites conversationally with smart CSS selector targeting and automatic content extraction. \ufe0f **Advanced Anti-Bot Capabilities:** - Automatic Cloudflare Turnstile solver - Real browser fingerprint impersonation with TLS matching - Enhanced stealth mode for protected sites \ufe0f **Session-Based Architecture:** Persistent browser sessions, concurrent tab management, and async browser automation that keep contexts alive across requests. \u26a1 **Massive Performance Gains:** - 60% faster dynamic content scraping - 50% speed boost in core selection methods - and more... **Terminal commands for scraping without programming** **Interactive Web Scraping shell:** - Interactive IPython shell with smart shortcuts - Direct curl-to-request conversion from DevTools And this is just the tip of the iceberg; there are many changes in this release This update represents 4 months of intensive development and community feedback. We've maintained backward compatibility while delivering these game-changing improvements. Ideal for data engineers, researchers, automation specialists, and anyone working with large-scale web data. **Full release notes:** **Get started:**"}
{"id": "nbvmb5r", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbvmb5r/", "author": "c0njur", "created_utc": 1756750244, "score": 8, "content": "Thanks for the work on this!"}
{"id": "nbw9k34", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbw9k34/", "author": "SoumyadipNayak", "created_utc": 1756757177, "score": 3, "content": "Great work man! Keep it up!"}
{"id": "nbwmtjc", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwmtjc/", "author": "usert313", "created_utc": 1756761193, "score": 3, "content": "Looks promising will give it a shot."}
{"id": "nbwv427", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwv427/", "author": "stratz_ken", "created_utc": 1756763858, "score": 2, "content": "Does it work with CDP, to read incoming packets? Is there any known memory leaks that would stop long run agents?"}
{"id": "nby3pzm", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nby3pzm/", "author": "Relevant-Flounder633", "created_utc": 1756780063, "score": 2, "content": "This is exactly what i was looking for!"}
{"id": "nbzt2jk", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbzt2jk/", "author": "randomharmeat", "created_utc": 1756811417, "score": 2, "content": "What about hcaptcha?"}
{"id": "nbw11io", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbw11io/", "author": "iridescent_herb", "created_utc": 1756754607, "score": 2, "content": "Legit. Will try at my current project."}
{"id": "nbx47e1", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbx47e1/", "author": "Rich-Independent1202", "created_utc": 1756766987, "score": 1, "content": "I building an e-commerce scrapping and anytime I deploy to cloud I get block by 403 error will this help fix it?"}
{"id": "nbxcgtq", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbxcgtq/", "author": "Kind-Radio-4990", "created_utc": 1756769967, "score": 1, "content": "Can it scrape linkedin?"}
{"id": "nby957b", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nby957b/", "author": "Embarrassed_Age6990", "created_utc": 1756782148, "score": 1, "content": "Does it can pass Akamai anti bot manager?"}
{"id": "nc0pcea", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc0pcea/", "author": "Goldman7911", "created_utc": 1756823104, "score": 1, "content": "Does it works with Shopee?"}
{"id": "nc1c9vb", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc1c9vb/", "author": "AnnualLevel4807", "created_utc": 1756829917, "score": 1, "content": "This seems promising. I've tested it on a site featuring challenge-based CAPTCHA, and it performed flawlessly. That said, I haven't discovered a method to bypass the Turnstile CAPTCHA that pops up after browsing 2 or 3 pages."}
{"id": "nc1s2xa", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc1s2xa/", "author": "rodeslab", "created_utc": 1756834584, "score": 1, "content": "I'll check this out"}
{"id": "nc3lljp", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3lljp/", "author": "0xReaper", "created_utc": 1756854684, "score": 1, "content": "Checkout this new demo:"}
{"id": "nc3v9r2", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3v9r2/", "author": "basedguytbh", "created_utc": 1756858065, "score": 1, "content": "Good fucking shit man, needed something like this. Playwright was giving me a headache."}
{"id": "nc67fva", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc67fva/", "author": "DryAssumption224", "created_utc": 1756897363, "score": 1, "content": "Seen this it looks awesome"}
{"id": "nc6814h", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc6814h/", "author": "gaupoit", "created_utc": 1756897628, "score": 1, "content": "Legit. Thanks for your work"}
{"id": "nc6oy0b", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc6oy0b/", "author": "Thunder_Cls", "created_utc": 1756904227, "score": 1, "content": "This is fire my guy, thanks for sharing!"}
{"id": "nc7eh0w", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc7eh0w/", "author": "None", "created_utc": 1756912141, "score": 1, "content": "[removed]"}
{"id": "nc9vtj8", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc9vtj8/", "author": "corelabjoe", "created_utc": 1756938457, "score": 1, "content": "This looks incredible really, any chance it could be dockerized in the future?"}
{"id": "ncoewf9", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/ncoewf9/", "author": "Murky-End-1134", "created_utc": 1757130906, "score": 1, "content": "Great work"}
{"id": "nddqwjr", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nddqwjr/", "author": "MasterFricker", "created_utc": 1757472582, "score": 1, "content": "I'll have to test it was hoping to run this in github actions, will keep tracking this"}
{"id": "ndp6o19", "type": "comment", "parent_id": "t3_1n5t3p2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/ndp6o19/", "author": "caroteno-beta", "created_utc": 1757622759, "score": 1, "content": "What kind of cloudflare turnstile solves? Only the implicit ones? What about the tokens generated in the backend?"}
{"id": "nbvpuk6", "type": "comment", "parent_id": "t1_nbvmb5r", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbvpuk6/", "author": "0xReaper", "created_utc": 1756751272, "score": 2, "content": "Thanks, mate. Glad you liked it!"}
{"id": "nbwpswn", "type": "comment", "parent_id": "t1_nbw9k34", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwpswn/", "author": "0xReaper", "created_utc": 1756762130, "score": 1, "content": "Thanks, mate. I'm looking forward to your feedback!"}
{"id": "nbwpugg", "type": "comment", "parent_id": "t1_nbwmtjc", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwpugg/", "author": "0xReaper", "created_utc": 1756762144, "score": 1, "content": "Thanks, mate. I'm looking forward to your feedback!"}
{"id": "nbwx4ni", "type": "comment", "parent_id": "t1_nbwv427", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwx4ni/", "author": "0xReaper", "created_utc": 1756764539, "score": 1, "content": "1. Yes, it works with CDP, but to use the browser for scraping, not reading the network. 2. No, there are no known memory leaks right now, but if you experienced any, report them and I will fix it"}
{"id": "nc3e618", "type": "comment", "parent_id": "t1_nby3pzm", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3e618/", "author": "0xReaper", "created_utc": 1756852144, "score": 1, "content": "Glad you liked it, don't forget the feedback!"}
{"id": "nbw5wcp", "type": "comment", "parent_id": "t1_nbw11io", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbw5wcp/", "author": "0xReaper", "created_utc": 1756756062, "score": 1, "content": "Nice, don't forget the feedback :)"}
{"id": "nbx5c48", "type": "comment", "parent_id": "t1_nbx47e1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbx5c48/", "author": "0xReaper", "created_utc": 1756767393, "score": 1, "content": "Yes, sure, just try the available stealth options"}
{"id": "nc3e454", "type": "comment", "parent_id": "t1_nbxcgtq", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3e454/", "author": "0xReaper", "created_utc": 1756852126, "score": 1, "content": "With proper logic and residential/mobile proxies, it can"}
{"id": "nbyydk6", "type": "comment", "parent_id": "t1_nby957b", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbyydk6/", "author": "c0njur", "created_utc": 1756794194, "score": 2, "content": "I\u2019ve used this on Akamai sites, the long answer is yes but doesn\u2019t mean every request will be successful. They appear to use ML to determine patterns. So you need to use rotating resi proxies and multistage retries to get a high level of success"}
{"id": "nc3l2ww", "type": "comment", "parent_id": "t1_nc0pcea", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3l2ww/", "author": "0xReaper", "created_utc": 1756854508, "score": 1, "content": "yes sure"}
{"id": "nc3lctv", "type": "comment", "parent_id": "t1_nc1c9vb", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3lctv/", "author": "0xReaper", "created_utc": 1756854602, "score": 2, "content": "Haha, then maybe use the `solve_cloudflare` argument with `StealthyFetcher` so the library solves it automatically for you :D"}
{"id": "nc3lenk", "type": "comment", "parent_id": "t1_nc1s2xa", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3lenk/", "author": "0xReaper", "created_utc": 1756854619, "score": 1, "content": "Don't forget the feedback :)"}
{"id": "nc6gtek", "type": "comment", "parent_id": "t1_nc3v9r2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc6gtek/", "author": "0xReaper", "created_utc": 1756901295, "score": 1, "content": "haha glad you liked it"}
{"id": "nc6gui5", "type": "comment", "parent_id": "t1_nc67fva", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc6gui5/", "author": "0xReaper", "created_utc": 1756901306, "score": 2, "content": "thanks mate!"}
{"id": "nc6gvpb", "type": "comment", "parent_id": "t1_nc6814h", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc6gvpb/", "author": "0xReaper", "created_utc": 1756901320, "score": 1, "content": "Glad you liked it :)"}
{"id": "nc9vr4g", "type": "comment", "parent_id": "t1_nc6oy0b", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc9vr4g/", "author": "0xReaper", "created_utc": 1756938435, "score": 1, "content": "Thanks a lot mate, glad you liked it!"}
{"id": "nca0d21", "type": "comment", "parent_id": "t1_nc7eh0w", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nca0d21/", "author": "webscraping-ModTeam", "created_utc": 1756940003, "score": 2, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "nca5b2k", "type": "comment", "parent_id": "t1_nc9vtj8", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nca5b2k/", "author": "0xReaper", "created_utc": 1756941676, "score": 2, "content": "yes sure I will"}
{"id": "ncqsutq", "type": "comment", "parent_id": "t1_ncoewf9", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/ncqsutq/", "author": "0xReaper", "created_utc": 1757171902, "score": 1, "content": "Thanks mate :)"}
{"id": "nbwxcm4", "type": "comment", "parent_id": "t1_nbwx4ni", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwxcm4/", "author": "stratz_ken", "created_utc": 1756764614, "score": 2, "content": "Is there any feature that allows for sniffing the network traffic? I dont want the HTML, I want the HTTP Request POST/GET data from certain urls. (And no, I cannot just send the HTTP requests, due to Cookie/Required json logic from the site)."}
{"id": "nbx5erk", "type": "comment", "parent_id": "t1_nbx5c48", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbx5erk/", "author": "Rich-Independent1202", "created_utc": 1756767419, "score": 2, "content": "Thanks \u263a\ufe0f"}
{"id": "nbxz31l", "type": "comment", "parent_id": "t1_nbx5c48", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbxz31l/", "author": "Rich-Independent1202", "created_utc": 1756778330, "score": 2, "content": "Unfortunately it did not work."}
{"id": "ncia83h", "type": "comment", "parent_id": "t1_nc3e454", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/ncia83h/", "author": "Azurrrrr", "created_utc": 1757049916, "score": 1, "content": "Is there any guide on this? I\u2019m new on this."}
{"id": "nc4tlhc", "type": "comment", "parent_id": "t1_nc3lctv", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc4tlhc/", "author": "AnnualLevel4807", "created_utc": 1756870670, "score": 1, "content": "Yeah, i've tried it. But it does not work either. I guess the package does not automatically solve captcha if it appears after navigating through 2 or 3 web pages."}
{"id": "nbwxq14", "type": "comment", "parent_id": "t1_nbwxcm4", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbwxq14/", "author": "0xReaper", "created_utc": 1756764742, "score": 1, "content": "No, there are not."}
{"id": "nc3e24z", "type": "comment", "parent_id": "t1_nbxz31l", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3e24z/", "author": "0xReaper", "created_utc": 1756852107, "score": 2, "content": "With proper logic and residential/mobile proxies, it penetrates through almost anything. I have been using it in my Web Scraping job for a year now."}
{"id": "nc9voub", "type": "comment", "parent_id": "t1_nc4tlhc", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc9voub/", "author": "0xReaper", "created_utc": 1756938414, "score": 1, "content": "Keep the option enabled for all requests to this website and with every request the library will check if it has the captcha or not before continuing"}
{"id": "nbx0cg2", "type": "comment", "parent_id": "t1_nbwxq14", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbx0cg2/", "author": "stratz_ken", "created_utc": 1756765637, "score": 0, "content": "How much to implemented a feature? Need it ASAP. All the browsers I test have a memory leak"}
{"id": "nbx51yn", "type": "comment", "parent_id": "t1_nbx0cg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbx51yn/", "author": "0xReaper", "created_utc": 1756767291, "score": 1, "content": "The documentation website is above bro"}
{"id": "nbzxdrd", "type": "comment", "parent_id": "t1_nbx0cg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nbzxdrd/", "author": "Atomic1221", "created_utc": 1756813292, "score": 1, "content": "One browser window, one tab. Opening multiple tabs is memory leak prone even in chrome proper."}
{"id": "nc3coz1", "type": "comment", "parent_id": "t1_nbzxdrd", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3coz1/", "author": "0xReaper", "created_utc": 1756851641, "score": 1, "content": "Have you experienced it here? We are using a custom version of a modified Firefox browser called Camoufox with a custom Browser tabs pool manager"}
{"id": "nc3cufb", "type": "comment", "parent_id": "t1_nc3coz1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n5t3p2/scrapling_v03_solve_cloudflare_automatically_and/nc3cufb/", "author": "Atomic1221", "created_utc": 1756851692, "score": 2, "content": "No I was replying to the comment that all browsers have memory leaks, not about yours specifically. I use selenium and seleniumbase and yes at scale browsers do have memory leaks juggling tabs especially in dockers."}
{"id": "1jbd7ph", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/", "author": "Pigik83", "created_utc": 1741983409, "score": 258, "title": "I've collected 350+ proxy pricing plans and this is the result", "content": "As the title says, I've spent the past few days creating a free proxy pricing comparison tool. You all know how hard it can be to compare prices from different providers, so I tried my best and this is the result: [ I hope you don't flag it as spam or self-promotion, I just wanted to share something useful. EDIT: it's still an alpha version, so any feedback is welcome. I'm filling it with more companies in these days."}
{"id": "mhtef3s", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtef3s/", "author": "True_Masterpiece224", "created_utc": 1741986733, "score": 11, "content": "I was recently searching for something like this. Really great site thank you for the effort !"}
{"id": "mhtq03z", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtq03z/", "author": "Orni66", "created_utc": 1741990337, "score": 5, "content": "That's awesome! great job! thanks for sharing."}
{"id": "mhtu8ka", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtu8ka/", "author": "BBMolotov", "created_utc": 1741991731, "score": 3, "content": "What a great work, thank you very much."}
{"id": "mi0jc5y", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi0jc5y/", "author": "otiuk", "created_utc": 1742086783, "score": 4, "content": "Useful for sure.. I\u2019d make it a bit better for mobile \u2014 lots of padding = very thin content view"}
{"id": "mhtywjf", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtywjf/", "author": "35202129078", "created_utc": 1741993281, "score": 2, "content": "Any reason scraperapi.com and scrapingbee.com aren't in the list? They're what I use"}
{"id": "mhul42v", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhul42v/", "author": "Sea-Remote-2040", "created_utc": 1742000842, "score": 2, "content": "This is super helpful! Comparing proxy prices is such a pain, so having everything in one place is a great idea. Have you noticed any trends in pricing\u2014like which providers offer the best value for large-scale scraping?"}
{"id": "mhuzj2l", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhuzj2l/", "author": "None", "created_utc": 1742006067, "score": 2, "content": "[deleted]"}
{"id": "mhw1769", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhw1769/", "author": "reizals", "created_utc": 1742025335, "score": 2, "content": "To shame that there's to information about payments. I buy proxies only by crypto. But! Good work. Btw. I did something similar but more detailed. I bought couple of proxies and I tested its speed. If you add this 2 changes your listing it be the best!"}
{"id": "mhwd21d", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwd21d/", "author": "NotDeffect", "created_utc": 1742033142, "score": 2, "content": "Insane, thank you!"}
{"id": "mhx35aj", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhx35aj/", "author": "poedy78", "created_utc": 1742045951, "score": 2, "content": "Bim! This is super helpful! Thx."}
{"id": "mhyi06k", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhyi06k/", "author": "jcavalcantt", "created_utc": 1742062307, "score": 2, "content": "Extremely useful. Thanks for sharing."}
{"id": "mi2gutc", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi2gutc/", "author": "SerhatOzy", "created_utc": 1742121883, "score": 2, "content": "Thanks"}
{"id": "mi2jhi9", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi2jhi9/", "author": "Dismal-Hunter-3484", "created_utc": 1742123436, "score": 2, "content": "Guay!"}
{"id": "mi2qlhz", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi2qlhz/", "author": "javad94", "created_utc": 1742127169, "score": 2, "content": "Great, thanks bro"}
{"id": "mi9pvgk", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi9pvgk/", "author": "RobSm", "created_utc": 1742223899, "score": 2, "content": "Missing bandwidth limitations when choosing pay-per-ip option. Some have very low limits and then you need to pay extra which increases total cost by 10x"}
{"id": "mia7ynx", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mia7ynx/", "author": "AutomaticPiglet3047", "created_utc": 1742229222, "score": 2, "content": "Good stuff, can you also include Torchlabs into it, would be really helpful"}
{"id": "miaxdkd", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miaxdkd/", "author": "Element1501", "created_utc": 1742236488, "score": 2, "content": "Great idea, however under Mobile Proxies you have only one provider? Also you should be 100% transparent that you use affiliate links in your comparison, so many providers are missed out. Nevertheless good idea and might become useful tool"}
{"id": "mibvasw", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mibvasw/", "author": "None", "created_utc": 1742246233, "score": 2, "content": "[removed]"}
{"id": "mjyy6l3", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mjyy6l3/", "author": "VierFaeuste", "created_utc": 1743056528, "score": 2, "content": "Thank you, great site"}
{"id": "mhwdiep", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwdiep/", "author": "Ok_Map_2755", "created_utc": 1742033424, "score": 1, "content": "Some inaccuracies, like you should quote BrightData prices for the pay-as-you-go package, not monthly commitment package. BUT, this is fucking GOATED, thank you, I love it! Could you have a .CSV or .JSON download of the data available?"}
{"id": "moij72m", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/moij72m/", "author": "opecadorembuscadaren", "created_utc": 1745362495, "score": 1, "content": "do you have any ratings? i really need some premium proxies to buy"}
{"id": "mzf2bok", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mzf2bok/", "author": "houseswappa", "created_utc": 1750720410, "score": 1, "content": "Hi I need to move low volume of data from youtube to a google firebase server. I am getting blocked due to copyright issues. What type of proxy would work for maybe 500mb a month in this case ?"}
{"id": "n2ur8f3", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n2ur8f3/", "author": "CryptoToTheMoon317", "created_utc": 1752385958, "score": 1, "content": "omg this is exactly what I needed, thank you!!! Would be cool to see a way to include performance stats or reliability metrics, but this is amazing anyways!"}
{"id": "n31rj1w", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n31rj1w/", "author": "None", "created_utc": 1752486411, "score": 1, "content": "[removed]"}
{"id": "n5v12f3", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n5v12f3/", "author": "Appropriate-Slide824", "created_utc": 1753822045, "score": 1, "content": "crypto payment options?"}
{"id": "n6vdnpf", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n6vdnpf/", "author": "None", "created_utc": 1754315667, "score": 1, "content": "[removed]"}
{"id": "n8pg0zb", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n8pg0zb/", "author": "hansentenseigan", "created_utc": 1755200141, "score": 1, "content": "what table do you use on this website? it looks clean and super fast"}
{"id": "nb3deiu", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/nb3deiu/", "author": "nextbrowser_t", "created_utc": 1756366501, "score": 1, "content": "sharing your research!"}
{"id": "nb9mung", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/nb9mung/", "author": "webmanpt", "created_utc": 1756445966, "score": 1, "content": "Nice one"}
{"id": "mhtz7n8", "type": "comment", "parent_id": "t3_1jbd7ph", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtz7n8/", "author": "Infamous_Tomatillo53", "created_utc": 1741993384, "score": -1, "content": "You have time doing this but no time to maintain nodriver? Hahahaha"}
{"id": "mhtg165", "type": "comment", "parent_id": "t1_mhtef3s", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtg165/", "author": "Pigik83", "created_utc": 1741987166, "score": 3, "content": "thank you so much."}
{"id": "ndev83e", "type": "comment", "parent_id": "t1_mhtef3s", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/ndev83e/", "author": "nyctophilliat", "created_utc": 1757492808, "score": 1, "content": "Amazing tool, thanks so much. It is missing some key providers no? If it helps I completed your tool [**with some of the top ones I use in this list.**]("}
{"id": "mhtq3it", "type": "comment", "parent_id": "t1_mhtq03z", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhtq3it/", "author": "Pigik83", "created_utc": 1741990370, "score": 3, "content": "Thanks!"}
{"id": "mhv7ljj", "type": "comment", "parent_id": "t1_mhtu8ka", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhv7ljj/", "author": "Pigik83", "created_utc": 1742009266, "score": 1, "content": "Thank you"}
{"id": "miaypgd", "type": "comment", "parent_id": "t1_mi0jc5y", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miaypgd/", "author": "Pigik83", "created_utc": 1742236873, "score": 1, "content": "Sorry about that, not a frontend dev here"}
{"id": "mhwfxje", "type": "comment", "parent_id": "t1_mhtywjf", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwfxje/", "author": "Pigik83", "created_utc": 1742034883, "score": 3, "content": "Their pricing plan is based on credits and they\u2019re difficult to compare with other providers. Same applies with Zyte api and Zenrows"}
{"id": "mi8144q", "type": "comment", "parent_id": "t1_mhtywjf", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi8144q/", "author": "None", "created_utc": 1742195373, "score": 1, "content": "[removed]"}
{"id": "mhv69in", "type": "comment", "parent_id": "t1_mhul42v", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhv69in/", "author": "Pigik83", "created_utc": 1742008716, "score": 1, "content": "Thanks! Difficult to say, one trend is for sure that prices are generally lowering and more providers are starting to offer more complex solutions like web unblockers"}
{"id": "mhv6htz", "type": "comment", "parent_id": "t1_mhuzj2l", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhv6htz/", "author": "Pigik83", "created_utc": 1742008811, "score": 1, "content": "Not my main area of competence but I would go with mobile ones. Prices are not high and using APIs (i assume) the bandwidth used won\u2019t be that much"}
{"id": "mhw1m37", "type": "comment", "parent_id": "t1_mhw1769", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhw1m37/", "author": "Pigik83", "created_utc": 1742025605, "score": 2, "content": "Thanks for the feedback, can you share your work if available?"}
{"id": "mhwdebt", "type": "comment", "parent_id": "t1_mhwd21d", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwdebt/", "author": "Pigik83", "created_utc": 1742033353, "score": 0, "content": "Thanks!"}
{"id": "mhxod9n", "type": "comment", "parent_id": "t1_mhx35aj", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhxod9n/", "author": "Pigik83", "created_utc": 1742053141, "score": 1, "content": "Thanks!"}
{"id": "miayl8t", "type": "comment", "parent_id": "t1_mhyi06k", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miayl8t/", "author": "Pigik83", "created_utc": 1742236839, "score": 1, "content": "Thanks"}
{"id": "miayutn", "type": "comment", "parent_id": "t1_mi9pvgk", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miayutn/", "author": "Pigik83", "created_utc": 1742236915, "score": 1, "content": "Good point, should be added in the pay per IP plans"}
{"id": "miayw2z", "type": "comment", "parent_id": "t1_mia7ynx", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miayw2z/", "author": "Pigik83", "created_utc": 1742236926, "score": 1, "content": "Noted, thanks!"}
{"id": "micnzi6", "type": "comment", "parent_id": "t1_mia7ynx", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/micnzi6/", "author": "matty_fu", "created_utc": 1742255364, "score": 1, "content": "Thanks u/AutomaticPiglet3047 let me know if you have any difficulty with getting your preferred vendor listed"}
{"id": "miay5qr", "type": "comment", "parent_id": "t1_miaxdkd", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miay5qr/", "author": "Pigik83", "created_utc": 1742236715, "score": 2, "content": "Probably you selected Pay per IP plans, where there\u2019s only one provider. If you have a look at the pay per request/GB tab you find more. Thanks dor the feedback"}
{"id": "micmkaj", "type": "comment", "parent_id": "t1_miaxdkd", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/micmkaj/", "author": "matty_fu", "created_utc": 1742254890, "score": 1, "content": "hey u/Element1501 \\- can you name a few proxy vendors that have not been included? u/Pigik83 \\- if you're only including affiliated links and not providing a true market / comprehensive comparison, I'll have to take this down. How do you respond?"}
{"id": "mieh05y", "type": "comment", "parent_id": "t1_mibvasw", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mieh05y/", "author": "Pigik83", "created_utc": 1742283712, "score": 1, "content": "Included only Residential proxies, since daily plans are not currently supported in the model"}
{"id": "mhwfqne", "type": "comment", "parent_id": "t1_mhwdiep", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwfqne/", "author": "Pigik83", "created_utc": 1742034770, "score": 1, "content": "There is also the pay as you go package for BD. Good point, i could think about an export"}
{"id": "n35uov4", "type": "comment", "parent_id": "t1_n31rj1w", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n35uov4/", "author": "webscraping-ModTeam", "created_utc": 1752533243, "score": 1, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "n8pw57l", "type": "comment", "parent_id": "t1_n8pg0zb", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n8pw57l/", "author": "Pigik83", "created_utc": 1755204786, "score": 1, "content": "Supabase with Lovable"}
{"id": "mhu75gr", "type": "comment", "parent_id": "t1_mhtz7n8", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhu75gr/", "author": "Lafftar", "created_utc": 1741996049, "score": 3, "content": "This is probably far easier than maintaining a browser library"}
{"id": "mhv56vd", "type": "comment", "parent_id": "t1_mhtz7n8", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhv56vd/", "author": "Pigik83", "created_utc": 1742008282, "score": 2, "content": "I\u2019m not the creator of nodriver"}
{"id": "mi868wi", "type": "comment", "parent_id": "t1_mi8144q", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi868wi/", "author": "webscraping-ModTeam", "created_utc": 1742198767, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mi86r21", "type": "comment", "parent_id": "t1_mi8144q", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mi86r21/", "author": "matty_fu", "created_utc": 1742199103, "score": 1, "content": "Web scraper API pricing models are not comparable in the same way that raw usage-based data proxies are. It's also noted that your comparison tool is rather favorable towards the product that you recommend in your comment history."}
{"id": "mhwcaht", "type": "comment", "parent_id": "t1_mhw1m37", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwcaht/", "author": "reizals", "created_utc": 1742032664, "score": 1, "content": "Sorry its not prepared for publishing :("}
{"id": "micnukb", "type": "comment", "parent_id": "t1_miayw2z", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/micnukb/", "author": "matty_fu", "created_utc": 1742255319, "score": 2, "content": "What is the lead time for adding new proxies? eg. when can we expect to see Torchlabs listed?"}
{"id": "mief8e5", "type": "comment", "parent_id": "t1_micmkaj", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mief8e5/", "author": "Pigik83", "created_utc": 1742282523, "score": 1, "content": "I\u2019m not comparing only proxies with affiliate links. Most of them they have it, others don\u2019t. The proxy vendors excluded at the moment are because 1) don\u2019t have public pricing plans on websites or they are not compatible with the model or 2) simply never heard them and were not in the first pages of Google of my researches. But happy to integrate them, with or without affiliate links"}
{"id": "miefkao", "type": "comment", "parent_id": "t1_micmkaj", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miefkao/", "author": "Pigik83", "created_utc": 1742282745, "score": 1, "content": "NetNut, Ping Proxies, and Plain Proxies, just to name three vendors without affiliate links. NetNut has tracking just to say that the traffic is coming from my site but doesn't have any affiliation program."}
{"id": "mhwhkjq", "type": "comment", "parent_id": "t1_mhwfqne", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhwhkjq/", "author": "Ok_Map_2755", "created_utc": 1742035853, "score": 1, "content": "Yes please, I need this data. Can you also make one on WAF/Captcha solvers? Cloudflare, ReCaptcha v2, PerimeterX, FunCaptcha, etc?"}
{"id": "n8s1d7u", "type": "comment", "parent_id": "t1_n8pw57l", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/n8s1d7u/", "author": "hansentenseigan", "created_utc": 1755232484, "score": 1, "content": "wow i was suprised that it is using postgresql, notion did this and it still runs veryyyyy slow"}
{"id": "mhxp1on", "type": "comment", "parent_id": "t1_mhv56vd", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhxp1on/", "author": "Ok-Code6623", "created_utc": 1742053352, "score": 3, "content": "Not with that attitude! /s Please check your css though. On mobile, half of the horizontal space is taken up by padding"}
{"id": "mieflm7", "type": "comment", "parent_id": "t1_micnukb", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mieflm7/", "author": "Pigik83", "created_utc": 1742282770, "score": 1, "content": "Doing now, wait a few minutes to see it"}
{"id": "miegi7m", "type": "comment", "parent_id": "t1_micnukb", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miegi7m/", "author": "Pigik83", "created_utc": 1742283374, "score": 1, "content": "Available on the website"}
{"id": "mhxtjw5", "type": "comment", "parent_id": "t1_mhxp1on", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mhxtjw5/", "author": "Pigik83", "created_utc": 1742054763, "score": 2, "content": ""}
{"id": "mij1onc", "type": "comment", "parent_id": "t1_miegi7m", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/mij1onc/", "author": "matty_fu", "created_utc": 1742340712, "score": 1, "content": "Still not able to see it on both per IP and per Request results"}
{"id": "miki1ne", "type": "comment", "parent_id": "t1_mij1onc", "permalink": "https://www.reddit.com/r/webscraping/comments/1jbd7ph/ive_collected_350_proxy_pricing_plans_and_this_is/miki1ne/", "author": "Pigik83", "created_utc": 1742360342, "score": 1, "content": "They are all pay as you go plan, so you should select pay per request/gb and one off purchases to see them"}
{"id": "1jy62vg", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744545758, "score": 221, "title": "I created a solution to bypass Cloudflare", "content": "Cloudflare blocks are a common headache when scraping. I created a small Node.js API called **Unflare** that uses `puppeteer-real-browser` to solve Cloudflare challenges in a real browser session. It returns valid session cookies and headers so you can make direct requests afterward. It supports: * GET/POST (form data) * Proxy configuration * Automatic screenshots on block * Using it through Docker Here\u2019s the GitHub repo if you want to try it out or contribute: ["}
{"id": "mn280kh", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn280kh/", "author": "ThatHappenedOneTime", "created_utc": 1744638765, "score": 6, "content": ""}
{"id": "mmvznah", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmvznah/", "author": "Still_Steve1978", "created_utc": 1744548871, "score": 3, "content": "great work, thank you for sharing :)"}
{"id": "mndopsj", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mndopsj/", "author": "Low_Promotion_2574", "created_utc": 1744794811, "score": 3, "content": "I have also worked with the bypasses. The main thing CF uses is cf\\_clearance cookie. If you send that cookie which has passed the cloudflare challenge from a browser, the CF will pass your request to origin. But you should know that the cf\\_clearance is bound to the User-Agent and IP address, so if you use rotating proxies they should be sticky. Also User-Agent should be the same as the one which you passed the challenge with."}
{"id": "mmwdqmd", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmwdqmd/", "author": "RandomPantsAppear", "created_utc": 1744554027, "score": 3, "content": "Could you go a little into how you did it for us python folks?"}
{"id": "mn6sa02", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn6sa02/", "author": "Key-Contact-6524", "created_utc": 1744694650, "score": 2, "content": "Crazy stuff a"}
{"id": "mn71y47", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn71y47/", "author": "Jumpy-Desk4215", "created_utc": 1744700437, "score": 2, "content": "Thank you"}
{"id": "mnm7kdt", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mnm7kdt/", "author": "None", "created_utc": 1744909972, "score": 2, "content": "[deleted]"}
{"id": "mrf58wh", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mrf58wh/", "author": "Gold_Attention_7650", "created_utc": 1746799055, "score": 2, "content": "Excelente work! Thank you for sharing."}
{"id": "mmwd93h", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmwd93h/", "author": "Historical-City-7708", "created_utc": 1744553865, "score": 1, "content": "great Is the puppeteer real browser is actively updated?"}
{"id": "mmworzr", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmworzr/", "author": "None", "created_utc": 1744557559, "score": 1, "content": "[deleted]"}
{"id": "mmy7tsz", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmy7tsz/", "author": "Infamous_Tomatillo53", "created_utc": 1744574828, "score": 1, "content": "Could you explain how this works under the hood? In your starter code (js) it fetches localhost. But what happens under the hood? What website does it ping? How is Cloundflare is triggered and how do you know if the headers and cookies is acceptable?"}
{"id": "mmzi9di", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmzi9di/", "author": "Suspicious_Cap532", "created_utc": 1744590992, "score": 1, "content": "aw man not playwright?"}
{"id": "mn0ubt3", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn0ubt3/", "author": "External_Skirt9918", "created_utc": 1744613307, "score": 1, "content": "Lol simply connect tailscale and use your home internet via VPS 24/7. If IP blocked by Cloudflare simply turn off and on the router you will get new ip"}
{"id": "mn5m8um", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn5m8um/", "author": "Prince_of_Caspian", "created_utc": 1744677184, "score": 1, "content": "thx for the tools, I tried but doesn\u2019t work. Can\u2019t continue with the cookies and session, it says blocked"}
{"id": "mqatjr8", "type": "comment", "parent_id": "t3_1jy62vg", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mqatjr8/", "author": "Useless_Devs", "created_utc": 1746235331, "score": 1, "content": "i try to use it and even with proxy i face that issue \"\\[01:03:28 UTC\\] ERROR: Timeout Error endpoint: \"scrapeClearance\"\" // i use a clean datacenter proxy"}
{"id": "mn3nzj5", "type": "comment", "parent_id": "t1_mn280kh", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn3nzj5/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744654466, "score": 4, "content": "FlareSolverr doesn't work anymore, at least it didn't work for me and a couple of other people"}
{"id": "mng5l7n", "type": "comment", "parent_id": "t1_mndopsj", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mng5l7n/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744826580, "score": 1, "content": "Yes, that's correct"}
{"id": "mmy88na", "type": "comment", "parent_id": "t1_mmwdqmd", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmy88na/", "author": "None", "created_utc": 1744574960, "score": 3, "content": "[deleted]"}
{"id": "mmxlv7o", "type": "comment", "parent_id": "t1_mmwdqmd", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmxlv7o/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744567803, "score": 5, "content": "Hello, I haven't used python before, but here's how ChatGPT translated the JavaScript request to Python, feel free to add corrections: import requests url = \" payload = { \"url\": \" \"timeout\": 60000, \"proxy\": { \"host\": \"proxy.example.com\", \"port\": 8080, \"username\": \"user\", \"password\": \"pass\" } } headers = { \"Content-Type\": \"application/json\" } response = requests.post(url, json=payload, headers=headers) if response.status_code == 200: data = response.json() cookies = data.get(\"cookies\", []) headers = data.get(\"headers\", {}) print(\"Cookies:\", cookies) print(\"Headers:\", headers) else: print(\"Error:\", response.status_code, response.text)"}
{"id": "mpar6ct", "type": "comment", "parent_id": "t1_mnm7kdt", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mpar6ct/", "author": "Mean-Cantaloupe-6383", "created_utc": 1745754449, "score": 1, "content": "Hi! The main idea behind Unflare is that it handles the Cloudflare protection page \u2014 the one you see when visiting a site protected by Cloudflare \u2014 and returns a valid `cf_clearance` token. This token proves the challenge was solved and allows your own scripts or browser to access the page without going through the challenge again."}
{"id": "mmxetvg", "type": "comment", "parent_id": "t1_mmwd93h", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmxetvg/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744565645, "score": 2, "content": "The author of Puppeteer Real Browser stopped working on it, but I'm pretty sure the current implementation will keep working for a while. And if Cloudflare manages to block it, I'm confident the community will fork it and find a workaround again."}
{"id": "mmxd3tz", "type": "comment", "parent_id": "t1_mmworzr", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmxd3tz/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744565124, "score": 0, "content": "Using a custom user agent isn\u2019t supported and isn\u2019t really recommended, because if you change only the user agent, you also have to handle a lot of small details to avoid getting detected by Cloudflare."}
{"id": "mn1fjhq", "type": "comment", "parent_id": "t1_mmy7tsz", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn1fjhq/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744626743, "score": 3, "content": "When you provide the target website URL, Unflare navigates to that website using puppeteer-real-browser. Once the page loads, it faces a Cloudflare challenge page\u2014this is the page that normally blocks bots. Thanks to Puppeteer\u2019s real browser environment, it behaves just like a human: it waits for the challenge to appear and then interacts with it, including clicking the CAPTCHA button if needed. Once the challenge is passed and the real page is shown, Unflare captures the response headers and cookies from that session. These cookies (especially the \\_\\_cf\\_clearance token) and headers are essential. You need to copy them into your own automation browser or script. Cloudflare is very sensitive to headers\u2014changing even one can trigger another challenge. That\u2019s why it\u2019s best to reuse the exact headers and cookies provided by Unflare in your automation logic. Once you\u2019ve done that, your browser will have full access to the page, as if a human had passed the challenge."}
{"id": "mn6k8iv", "type": "comment", "parent_id": "t1_mn0ubt3", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn6k8iv/", "author": "kmonlinesolutions", "created_utc": 1744690526, "score": 1, "content": "i tried this, i can log in to my vps. but i couldnt access my docker services via my subdomains."}
{"id": "mnajq54", "type": "comment", "parent_id": "t1_mn5m8um", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mnajq54/", "author": "Mean-Cantaloupe-6383", "created_utc": 1744748126, "score": 1, "content": "If it says that you're blocked, it means that the target website blocks you by IP. Use proxy in this scenario, Unflare supports that"}
{"id": "mqaujs9", "type": "comment", "parent_id": "t1_mqatjr8", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mqaujs9/", "author": "Useless_Devs", "created_utc": 1746235706, "score": 1, "content": "My ip is not blocked. I tested it directly on cloudlflare ip=xxxxxx tls=TLSv1.3 uag=Mozilla/5.0 (Windows NT 10.0; Win64; x64) loc=DE fl=471f84 colo=FRA warp=off gateway=off"}
{"id": "mn3o7z6", "type": "comment", "parent_id": "t1_mn3nzj5", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn3o7z6/", "author": "ThatHappenedOneTime", "created_utc": 1744654536, "score": 3, "content": "It works for me"}
{"id": "mmya91c", "type": "comment", "parent_id": "t1_mmy88na", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mmya91c/", "author": "RandomPantsAppear", "created_utc": 1744575596, "score": 2, "content": "Yeah I\u2019m just mostly interested in how the bypass itself works."}
{"id": "mn6rzfj", "type": "comment", "parent_id": "t1_mn6k8iv", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mn6rzfj/", "author": "External_Skirt9918", "created_utc": 1744694487, "score": 1, "content": "Use seperate server vps for scraping and loading data to your main server."}
{"id": "mqd6x19", "type": "comment", "parent_id": "t1_mqaujs9", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mqd6x19/", "author": "Mean-Cantaloupe-6383", "created_utc": 1746278104, "score": 1, "content": "Check the /screenshots folder inside the container, please share the image that you see."}
{"id": "mnaabvm", "type": "comment", "parent_id": "t1_mn3o7z6", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mnaabvm/", "author": "No-Drummer4059", "created_utc": 1744745328, "score": 1, "content": "it depends on CF config, try with [ it should fail edit: my mistake, I was referring to Cloudscrapper which doesn't uses a real browser, although it is working now."}
{"id": "mnagvi3", "type": "comment", "parent_id": "t1_mnaabvm", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mnagvi3/", "author": "ThatHappenedOneTime", "created_utc": 1744747277, "score": 1, "content": "My residential server worked on the first try, the datacenter server worked on the second try. I tried it a few more times, here are the results: Residential: 4/4 Datacenter: 1/4 I can always use a VPN to my residential server."}
{"id": "mnaht1w", "type": "comment", "parent_id": "t1_mnagvi3", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mnaht1w/", "author": "No-Drummer4059", "created_utc": 1744747549, "score": 1, "content": "can u post a video or tell me what country are you using (considering one can setup CF to block some countries) ? here is not even working with Chilean residential IP which is the same as the original URL. edit: it is working right now, but it wasn't when i posted a couple of minutes ago. edit 2: my mistake, I was referring to Cloudscrapper which doesn't uses a real browser, although it is working now."}
{"id": "mnajcld", "type": "comment", "parent_id": "t1_mnaht1w", "permalink": "https://www.reddit.com/r/webscraping/comments/1jy62vg/i_created_a_solution_to_bypass_cloudflare/mnajcld/", "author": "ThatHappenedOneTime", "created_utc": 1744748014, "score": 1, "content": "Edit: Removed country mention as the issue is resolved; this detail could be identifying."}
{"id": "1bapx0j", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/", "author": "FromAtoZen", "created_utc": 1710012558, "score": 175, "title": "How did OpenAI scrap the entire Internet for training Chat GPT?", "content": "Out of curiosity, how did OpenAI *scrape the entire Internet for training ChatGPT?"}
{"id": "ku4dbjz", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4dbjz/", "author": "nananawatman", "created_utc": 1710015590, "score": 70, "content": "According to [wikipedia]( 60% of the data is from [Common crawl]( >Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of [Common Crawl]( consisting of 410 billion [byte-pair-encoded]( tokens.^(: 9) Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.^(: 9) GPT-3 was trained on hundreds of billions of words and is also capable of coding in [CSS]( [JSX]( and [Python]( among others."}
{"id": "ku45qru", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku45qru/", "author": "None", "created_utc": 1710013002, "score": 26, "content": "[removed]"}
{"id": "ku4nnyr", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4nnyr/", "author": "salestoolsss", "created_utc": 1710019191, "score": 17, "content": "they used Common Crawl dataset wiki and many other data"}
{"id": "ku4dgom", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4dgom/", "author": "divided_capture_bro", "created_utc": 1710015637, "score": 5, "content": "Thet have a webcrawler called GPTbot. They also licensed a lot of data."}
{"id": "ku5pmql", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku5pmql/", "author": "Various-Inside-4064", "created_utc": 1710034072, "score": 5, "content": "As other commenter mentioned that common crawl was pretty common to train large LLMs but after ChatGPT release the ai community become secretive of what data they used to train model. So we can take a guess that it is from common crawl and also from their own crawler. You pointed out people do not want Openai to scrap their data if that is so then they can block openai bot from their website see [Now you can block OpenAI\u2019s web crawler - The Verge]( Even Google in gemini paper did not reveal the source of their training data. They just said it is been trained on different data from web filtered heavily for safety reason. So in short chatgpt or any other LLm are not trained on entire internet but rather filtered or small portion of data (about 4 trillions token)"}
{"id": "kugdui4", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugdui4/", "author": "Street-Reindeer4020", "created_utc": 1710205006, "score": 5, "content": "So does this mean web scraping is legal? Or are the big companies of Google and OpenAI allowed to, whilst the individual can\u2019t scrape a website of interest?"}
{"id": "ku4bnsi", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4bnsi/", "author": "Ok-Dingo-9988", "created_utc": 1710015018, "score": 13, "content": "google how google scrapes the net"}
{"id": "kui9l64", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kui9l64/", "author": "None", "created_utc": 1710243504, "score": 2, "content": "I have wanted to know too. Thank you for asking"}
{"id": "kuizi48", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuizi48/", "author": "Thanosmiss234", "created_utc": 1710254583, "score": 2, "content": "I believe I have a better question: Will they be able to use the old results from the first \\*scrape indefinitely? I think this is important because in the future 1) scraping websites will cost money or be blocked 2) there is/will be more AI generated material that will dirty results. 3) They can limit the websites needed to scrape because they just need the diff. Hence, they have the last AI free Internet as a baseline data set to generated material from!"}
{"id": "ku47j1s", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku47j1s/", "author": "jhkoenig", "created_utc": 1710013607, "score": 8, "content": "1) It didn't. That isn't possible in a reasonable amount of time 2) It is \"scrape\" not \"scrap.\""}
{"id": "ku5nrio", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku5nrio/", "author": "Classic-Dependent517", "created_utc": 1710033278, "score": 1, "content": "I mean websites that dont heavily use anti bots tech are so easy that anyone even with 1 week of bootcamp can do."}
{"id": "ku5uilk", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku5uilk/", "author": "None", "created_utc": 1710036156, "score": 1, "content": "Have you tried asking chatGPT?"}
{"id": "ku8nm4f", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku8nm4f/", "author": "PeteGoua", "created_utc": 1710089416, "score": 1, "content": "Sooo \u2026 when they scrape the sites - they copy all of that data and store it on different storage devices ? That would be huge as the data is all of the internet and all of the published journals and books and \u2026 well everything in a library!"}
{"id": "kugbztn", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugbztn/", "author": "WishIWasOnACatamaran", "created_utc": 1710204314, "score": 1, "content": "First by developing alrorighms that are capable of scraping all known public data. Now them and everybody else are raising capital to buy and scrap as much non-public data as possible."}
{"id": "kugp4l9", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugp4l9/", "author": "HardPress", "created_utc": 1710209236, "score": 1, "content": "The AI training datasets GPT3 was trained on are: CommonCrawl WebText2 Books1 Books2 Wikipedia"}
{"id": "kuh7uqg", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuh7uqg/", "author": "None", "created_utc": 1710217755, "score": 1, "content": "Is this the first time people have heard of Common Crawl?"}
{"id": "kuixk7n", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuixk7n/", "author": "None", "created_utc": 1710253889, "score": 1, "content": "How does it ensure it doesn\u2019t ingest mostly crap false data?"}
{"id": "kul0iw6", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kul0iw6/", "author": "Skepticmindlogic", "created_utc": 1710279433, "score": 1, "content": "They also, in addition to the top voted comment, scrapped Youtube illegally"}
{"id": "kun9vfs", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kun9vfs/", "author": "Agreeable-Ad-0111", "created_utc": 1710315635, "score": 1, "content": "More importantly, did they, or did they not, scrape r/shittyaskscience and similar. I really hope so"}
{"id": "kurfv6n", "type": "comment", "parent_id": "t3_1bapx0j", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kurfv6n/", "author": "Level-Anxiety-2986", "created_utc": 1710375366, "score": 1, "content": "Initially they used common crawl. Later they didn\u2019t have to. They partnered with Microsoft who already scrapes the internet for Bing"}
{"id": "kuh0769", "type": "comment", "parent_id": "t1_ku4dbjz", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuh0769/", "author": "Syrupwizard", "created_utc": 1710213967, "score": 4, "content": "It\u2019s funny to think how I\u2019m hearing the same things about chat gpt from teachers now that I heard about Wikipedia back in the day. That being said, Wikipedia is much more reliable imo."}
{"id": "kuk1aor", "type": "comment", "parent_id": "t1_ku4dbjz", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuk1aor/", "author": "djamp42", "created_utc": 1710267256, "score": 1, "content": "The pre-processing the data to me is one of the more amazing things in all of this. That is such a crazy task I can't even comprehend it."}
{"id": "ku49bkw", "type": "comment", "parent_id": "t1_ku45qru", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku49bkw/", "author": "FromAtoZen", "created_utc": 1710014217, "score": 5, "content": "Specifically."}
{"id": "ku4np1b", "type": "comment", "parent_id": "t1_ku4nnyr", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4np1b/", "author": "salestoolsss", "created_utc": 1710019201, "score": 5, "content": "+ bing data"}
{"id": "ku6mmjr", "type": "comment", "parent_id": "t1_ku4nnyr", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku6mmjr/", "author": "TonyGTO", "created_utc": 1710049503, "score": 5, "content": "Common Crawl, Reddit, and lots of piracy (mainly books and papers) processed by a shit load of Africans."}
{"id": "kuhav79", "type": "comment", "parent_id": "t1_ku4dgom", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuhav79/", "author": "Unhappy-Squirrel-731", "created_utc": 1710219420, "score": 1, "content": "Yea found it pretty wild they launched the bot too"}
{"id": "ku4c5wp", "type": "comment", "parent_id": "t1_ku4bnsi", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4c5wp/", "author": "FromAtoZen", "created_utc": 1710015191, "score": 24, "content": "Websites want Google to crawl them. OpenAI, not so much."}
{"id": "ku482ai", "type": "comment", "parent_id": "t1_ku47j1s", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku482ai/", "author": "MulhollandDr1ve", "created_utc": 1710013792, "score": 7, "content": "Right but they\u2019re asking how did they automatically get so much data. Including stuff behind paywalls"}
{"id": "ku4qlwn", "type": "comment", "parent_id": "t1_ku47j1s", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4qlwn/", "author": "shuz", "created_utc": 1710020248, "score": 6, "content": "This is r/webscrapping. Every post typos \u201cscrap\u201d at this point."}
{"id": "ku49az0", "type": "comment", "parent_id": "t1_ku47j1s", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku49az0/", "author": "FromAtoZen", "created_utc": 1710014212, "score": 2, "content": "1. I wasn\u2019t being literal with \u201centire\u201d \u2014 but they do have a massive subset of data for training their models. How was this achieved? 2. Thanks for typo notice"}
{"id": "ku48koc", "type": "comment", "parent_id": "t1_ku47j1s", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku48koc/", "author": "None", "created_utc": 1710013964, "score": 0, "content": "[deleted]"}
{"id": "kuguipy", "type": "comment", "parent_id": "t1_ku8nm4f", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuguipy/", "author": "akilter_", "created_utc": 1710211470, "score": 1, "content": "Assuming it's just text it's a lot less data than images, audio and video files. Plus, hard drives are cheap."}
{"id": "kuh7ci0", "type": "comment", "parent_id": "t1_ku8nm4f", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuh7ci0/", "author": "None", "created_utc": 1710217484, "score": 1, "content": "things like commoncrawl are already in s3 buckets in the cloud: [ the march 2024 crawl is about 110 TiB"}
{"id": "kukosku", "type": "comment", "parent_id": "t1_kuh0769", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kukosku/", "author": "Effective-Ear4823", "created_utc": 1710275406, "score": 2, "content": "Wikipedia has always been an excellent place to *start*, because it links to sources of its info. Both Wikipedia and ChatGPT are not a *primary* source though. ChatGPT is only useful for informational reasons in the case that it tells you where to go to find that info (and you actually go read the primary source to be sure it is real and actually says what ChatGPT says it says). There are other cool uses for ChatGPT, it's just not a reliable witness!"}
{"id": "ku5opyb", "type": "comment", "parent_id": "t1_ku49bkw", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku5opyb/", "author": "Mescallan", "created_utc": 1710033683, "score": 4, "content": "A.com Aa.com Aaa.com"}
{"id": "kurua7y", "type": "comment", "parent_id": "t1_ku49bkw", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kurua7y/", "author": "EarthquakeBass", "created_utc": 1710380732, "score": 2, "content": "They wrote code to GET data, probably with something like scrapy, parsed the HTML into readable content and then indexed it in some data store (Postgres, S3, mongodb who knows). Your question is answerable in specific without having worked at OpenAI but if you read up on how someone like Google indexes the internet it\u2019s probably similar."}
{"id": "kugcvi6", "type": "comment", "parent_id": "t1_ku49bkw", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugcvi6/", "author": "Jumper775-2", "created_utc": 1710204645, "score": 1, "content": "\ufe0f\ufe0f"}
{"id": "kuhax7n", "type": "comment", "parent_id": "t1_kuhav79", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuhax7n/", "author": "Unhappy-Squirrel-731", "created_utc": 1710219452, "score": 1, "content": "Anyone used it? I wonder how it compares to scrapy"}
{"id": "ku4e259", "type": "comment", "parent_id": "t1_ku4c5wp", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4e259/", "author": "Ok-Dingo-9988", "created_utc": 1710015847, "score": 9, "content": "as a website owner they only edit sitemap and the Robots.txt nothing special, you can even mimic your crawler that it looks like the Googlebot.. (these was a old trick to read forums without an account \\^\\^ ) i meant the technics it use about link handling and saving data ..."}
{"id": "ku4lpmd", "type": "comment", "parent_id": "t1_ku4c5wp", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4lpmd/", "author": "2AMMetro", "created_utc": 1710018492, "score": 3, "content": "So what\u2019s stopping them? There\u2019s nothing illegal about sending a GET request to some website."}
{"id": "ku49crg", "type": "comment", "parent_id": "t1_ku482ai", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku49crg/", "author": "jhkoenig", "created_utc": 1710014229, "score": -4, "content": "A lot of paywalls are very easy to beat. A lot of training data can be scraped from a few thousand high profile web sites. With some venture funding, buying capacity on AWS would make that very achievable within a short time. I'm sure that they continue to add to their training data."}
{"id": "ku4f0ta", "type": "comment", "parent_id": "t1_ku482ai", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4f0ta/", "author": "RobSm", "created_utc": 1710016183, "score": -4, "content": "They are owned by Microsoft and these guys own Bing. All data is already there."}
{"id": "ku4qypm", "type": "comment", "parent_id": "t1_ku4qlwn", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4qypm/", "author": "jhkoenig", "created_utc": 1710020375, "score": 3, "content": "haha. Makes me sad thinking about people's early education, though."}
{"id": "ku4cnua", "type": "comment", "parent_id": "t1_ku48koc", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku4cnua/", "author": "FromAtoZen", "created_utc": 1710015361, "score": 1, "content": "French people like to scrap too \u2014 especially on their !"}
{"id": "kumy6dz", "type": "comment", "parent_id": "t1_kukosku", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kumy6dz/", "author": "ImSoCul", "created_utc": 1710307548, "score": 2, "content": "For a lack of better word, GPT is basically just \"vibes\". This token (partial word) is likely good, I'm feeling this token next, rinse repeat."}
{"id": "kutnnee", "type": "comment", "parent_id": "t1_kukosku", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kutnnee/", "author": "identicalBadger", "created_utc": 1710418197, "score": 2, "content": "ChatGPT happily makes stuff up, including fake references. Hard to rely on it for much apart from drafting emails and maybe the occasional poweshell script."}
{"id": "kuljw27", "type": "comment", "parent_id": "t1_kukosku", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuljw27/", "author": "Syrupwizard", "created_utc": 1710286118, "score": 1, "content": "Very true!"}
{"id": "kuu7iu4", "type": "comment", "parent_id": "t1_kukosku", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuu7iu4/", "author": "Banksie123", "created_utc": 1710426344, "score": 1, "content": "I agree with your points, but one interesting note about primary sources on Wikipedia is that they are actually seldom allowed as a reference in a Wikipedia article without a reliable secondary source which supports the interpretation you seek to publish in said Wikipedia article. This is to avoid erroneous misinterpretation of complex primary sources as Wikipedia knows most people don't actually dig into the source material."}
{"id": "ku9mmfe", "type": "comment", "parent_id": "t1_ku5opyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku9mmfe/", "author": "External_Shirt6086", "created_utc": 1710101671, "score": 2, "content": "ANumber1Imports!.com"}
{"id": "kugn2by", "type": "comment", "parent_id": "t1_kugcvi6", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugn2by/", "author": "CloudFaithTTV", "created_utc": 1710208426, "score": 1, "content": "Lgtm"}
{"id": "kv2d4u2", "type": "comment", "parent_id": "t1_kugcvi6", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kv2d4u2/", "author": "Parking_Knowledge891", "created_utc": 1710544500, "score": 1, "content": "\u2049\ufe0f more or less"}
{"id": "kur5813", "type": "comment", "parent_id": "t1_kuhax7n", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kur5813/", "author": "LookAtThisFnGuy", "created_utc": 1710371495, "score": 2, "content": "Not sure you can use it, but you can disallow it via robots.txt"}
{"id": "kuiqn3d", "type": "comment", "parent_id": "t1_ku4e259", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuiqn3d/", "author": "None", "created_utc": 1710251291, "score": 1, "content": "A good WAF or bot profiler would put that to sleep no?"}
{"id": "ku7nzgt", "type": "comment", "parent_id": "t1_ku4lpmd", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku7nzgt/", "author": "mcmaster-99", "created_utc": 1710074675, "score": 2, "content": "It\u2019s a complicated topic but sites will usually block IPs (mostly temporary) that are sending too many requests in a short amount of time."}
{"id": "ku959y6", "type": "comment", "parent_id": "t1_ku4lpmd", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku959y6/", "author": "StreetStripe", "created_utc": 1710095487, "score": 2, "content": "I setup a web server and asked GPT to crawl it. A GET came through with the GPT user-agent Then I asked it to crawl an Amazon url. It tried, and declined because Amazon has the GPT user-agent disallowed in their ROBOTS.txt So, OpenAI is respecting the robots file. But, I acknowledge that they could very well be handling scraping for training differently from scraping for user requests."}
{"id": "ku6srh0", "type": "comment", "parent_id": "t1_ku4cnua", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku6srh0/", "author": "Xxando", "created_utc": 1710053197, "score": 2, "content": "It\u2019s already so buttery!"}
{"id": "kunlt5o", "type": "comment", "parent_id": "t1_kumy6dz", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kunlt5o/", "author": "nedal8", "created_utc": 1710324435, "score": 1, "content": "im not totally seeing the difference between that, and how we do it. lol"}
{"id": "kulfds2", "type": "comment", "parent_id": "t1_kuiqn3d", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kulfds2/", "author": "Ok-Dingo-9988", "created_utc": 1710284515, "score": 1, "content": "Yeah reverse IP lookup could identify you but I think not many sites are doing these, it's more likely that cloud flare kicks you if you hammer to much, but like I said it's more about the Methodes they are using"}
{"id": "ku96fcz", "type": "comment", "parent_id": "t1_ku7nzgt", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku96fcz/", "author": "2AMMetro", "created_utc": 1710095888, "score": 2, "content": "There\u2019s many ways around that though, like setting up a constantly rotating proxy pool and using a fresh ip every time. I used to scrape Amazon a few million times per day at my previous job."}
{"id": "ku9798u", "type": "comment", "parent_id": "t1_ku959y6", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku9798u/", "author": "2AMMetro", "created_utc": 1710096178, "score": 0, "content": "Just because their end product won\u2019t scrape websites for a user doesn\u2019t mean their company follows the same rules internally. Scraping websites with GPT also doesn\u2019t make much sense compared to writing a bunch of scripts. It would be highly inefficient in terms of processing power, especially considering the volume of data they need to scrape."}
{"id": "kuo67qe", "type": "comment", "parent_id": "t1_kunlt5o", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuo67qe/", "author": "Axis3673", "created_utc": 1710335260, "score": 1, "content": "Gpt uses probability, explicitly lol"}
{"id": "kuhqx4t", "type": "comment", "parent_id": "t1_ku96fcz", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuhqx4t/", "author": "tomrangerusa", "created_utc": 1710230493, "score": 2, "content": "Why?"}
{"id": "kuiqt5w", "type": "comment", "parent_id": "t1_ku96fcz", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuiqt5w/", "author": "None", "created_utc": 1710251356, "score": 2, "content": "There ways around that too. With a WAF or smart bot defense."}
{"id": "ku98c1r", "type": "comment", "parent_id": "t1_ku9798u", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/ku98c1r/", "author": "StreetStripe", "created_utc": 1710096558, "score": 1, "content": "Reread my last sentence."}
{"id": "kum07m2", "type": "comment", "parent_id": "t1_kuhqx4t", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kum07m2/", "author": "beauzero", "created_utc": 1710292267, "score": 1, "content": "For pricing information and product descriptions."}
{"id": "kuk1fit", "type": "comment", "parent_id": "t1_kuiqt5w", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuk1fit/", "author": "2AMMetro", "created_utc": 1710267299, "score": 1, "content": "Totally. It's a pretty constant back and forth battle."}
{"id": "kubolvl", "type": "comment", "parent_id": "t1_ku98c1r", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kubolvl/", "author": "anxman", "created_utc": 1710130766, "score": 1, "content": "You are missing the point. The frontend crawler is probably different than the training data crawler."}
{"id": "kuby7ga", "type": "comment", "parent_id": "t1_kubolvl", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kuby7ga/", "author": "StreetStripe", "created_utc": 1710136476, "score": 2, "content": "Am I being trolled, or can people in this thread not read? > But, I acknowledge that they could very well be handling scraping for training differently from scraping for user requests. What does this sentence mean to you? Because it's saying literally the same thing that you've just insightfully chimed in with."}
{"id": "kugonn4", "type": "comment", "parent_id": "t1_kuby7ga", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugonn4/", "author": "truthputer", "created_utc": 1710209050, "score": 1, "content": "Not the poster you\u2019ve been interacting with here, but yeah - you\u2019re good, but this other person is literally arguing against you with points you\u2019ve already made. Internet comment threads are weird sometimes."}
{"id": "kugssh2", "type": "comment", "parent_id": "t1_kugonn4", "permalink": "https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/kugssh2/", "author": "None", "created_utc": 1710210739, "score": 1, "content": "[removed]"}
{"id": "1lrqdhr", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/", "author": "None", "created_utc": 1751655048, "score": 176, "title": "i mean... yeah okay, you asked nicely", "content": ""}
{"id": "n1cr5xy", "type": "comment", "parent_id": "t3_1lrqdhr", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1cr5xy/", "author": "No_River_8171", "created_utc": 1751655997, "score": 23, "content": "Ahahahahahhahahaahha Thats some Shit i would do Your Lucky you didnt get no jump scare"}
{"id": "n1dgfh8", "type": "comment", "parent_id": "t3_1lrqdhr", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1dgfh8/", "author": "friday305", "created_utc": 1751664298, "score": 22, "content": "This is hilarious. While reversing a sight I once got the response of something like \u201cif you\u2019ve gotten this far apply here. With the sites career page link lol"}
{"id": "n1hsdgn", "type": "comment", "parent_id": "t3_1lrqdhr", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1hsdgn/", "author": "None", "created_utc": 1751733641, "score": 1, "content": "[removed]"}
{"id": "n1tgae9", "type": "comment", "parent_id": "t3_1lrqdhr", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1tgae9/", "author": "Independent-Air-1151", "created_utc": 1751900389, "score": 1, "content": "Nice going"}
{"id": "n581ix0", "type": "comment", "parent_id": "t3_1lrqdhr", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n581ix0/", "author": "DanielCrytpo", "created_utc": 1753510700, "score": 1, "content": "Haha, that's what I do occasionally..."}
{"id": "n5fvppj", "type": "comment", "parent_id": "t3_1lrqdhr", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n5fvppj/", "author": "LifeUnable4168", "created_utc": 1753627685, "score": 1, "content": "LoL =)) if you ask nicely, okay then..."}
{"id": "n1cri64", "type": "comment", "parent_id": "t1_n1cr5xy", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1cri64/", "author": "None", "created_utc": 1751656107, "score": 14, "content": "It's really funny, i was reverse engineering another website and it showed \"nice try\" as the error message"}
{"id": "n1emg37", "type": "comment", "parent_id": "t1_n1cr5xy", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1emg37/", "author": "No_Influence_4968", "created_utc": 1751680798, "score": 6, "content": "I would never, your telling people your safe guards, keep it standard, no hints given. Now this guy can keep at it with faux headers IP etc until he gets it right"}
{"id": "n1dm0ur", "type": "comment", "parent_id": "t1_n1dgfh8", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1dm0ur/", "author": "gutem", "created_utc": 1751666255, "score": 2, "content": "Bandcamp has (at least, had before) a disclaimer like this"}
{"id": "n1fjljf", "type": "comment", "parent_id": "t1_n1dgfh8", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1fjljf/", "author": "No_River_8171", "created_utc": 1751697083, "score": 2, "content": "Now that is amazing !! Im Not joking"}
{"id": "n1ifue7", "type": "comment", "parent_id": "t1_n1hsdgn", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1ifue7/", "author": "webscraping-ModTeam", "created_utc": 1751740963, "score": 1, "content": "Please review the sub rules"}
{"id": "n1dog6r", "type": "comment", "parent_id": "t1_n1cri64", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1dog6r/", "author": "indicava", "created_utc": 1751667135, "score": 17, "content": "\u201cNice try\u201d? That sounds familiar\u2026. Looks in codebase."}
{"id": "n1croqf", "type": "comment", "parent_id": "t1_n1cri64", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1croqf/", "author": "No_River_8171", "created_utc": 1751656166, "score": 3, "content": "QhshahahahaH this is funny what where the Websites i only get Blocked when i try"}
{"id": "n1csegd", "type": "comment", "parent_id": "t1_n1croqf", "permalink": "https://www.reddit.com/r/webscraping/comments/1lrqdhr/i_mean_yeah_okay_you_asked_nicely/n1csegd/", "author": "None", "created_utc": 1751656402, "score": 5, "content": "I really don't remember it was a long time ago, but it was a niche website, i occasionally reverse engineer solo developers when I got nothing better to do, i saw stuff like that now i do the same in my backend, i hope it cause a couple giggles lol, i even leave messages and ask them to send me an email and I'll give them premium because why the hell not, i haven't received any email so far lmao"}
{"id": "1kjvv68", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/", "author": "aaronn2", "created_utc": 1746950559, "score": 160, "title": "The real costs of web scraping", "content": "After reading this sub for a while, it looks like there's plenty of people who are scraping millions of pages every month with minimal costs - meaning dozens of $ per month (excluding servers, database, etc). I am still new to this, but I get confused by that figure. If I want to reliably (meaning with relatively high success rate) scrape websites, I probably should residential proxies. These are not cheap - the prices are going from roughly $0.50/1GB of bandwidth to almost $10 in some cases. There are web scraping API services on the web that handle headless browsers, proxies, CAPTCHAs etc, which costs starts from around \\~$150/month for 1M requests (no bandwidth limits). At glance, it looks like the residential proxies are way cheaper than the API solutions, but because of bandwidth, the price starts to quickly add up and it can actually get more expensive than the API solutions. Back to my first paragraph, to the people who scrape data very cheaply - how do they do it? Are they scraping without proxies (but that would likely mean they would get banned soon)? Or am I missing anything obvious here?"}
{"id": "mrpxlv4", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpxlv4/", "author": "Haningauror", "created_utc": 1746952471, "score": 69, "content": "What I do is continue scraping using a proxy, but I block all unnecessary network requests to save bandwidth. For example, when logging in, there's no need to load all the images on the login page, you probably only need the form and the submit button. Additionally, some scraping tasks are performed via hidden APIs instead of real browser requests, which is highly bandwidth-efficient."}
{"id": "mrpx9yq", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpx9yq/", "author": "albert_in_vine", "created_utc": 1746952258, "score": 19, "content": "I recently made around 2 million requests using ISP proxies that cost me about $3 per week with a 250GB bandwidth cap. The API I was calling only used about 5GB, so bandwidth really depends on the website. Just my two cents, ISP proxies are pretty reliable, but datacenter proxies are the worst; they get detected almost instantly."}
{"id": "mrpw314", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpw314/", "author": "None", "created_utc": 1746951485, "score": 16, "content": "[removed]"}
{"id": "mrpwxzo", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpwxzo/", "author": "None", "created_utc": 1746952040, "score": 12, "content": "You just cannot scrape at large scale without proxies."}
{"id": "mrqckoh", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqckoh/", "author": "Pigik83", "created_utc": 1746961801, "score": 10, "content": "We scrape at our company 1 billion of product prices per month, more or less. Our proxy bill never went above 1k per month. The truth is that by rotating IPs by using cloud providers\u2019 VMs, you can scrape 60/70 % of the e-commerces out there."}
{"id": "mrqzyjk", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqzyjk/", "author": "PriceScraper", "created_utc": 1746971930, "score": 9, "content": "I own my own bare metal and built my own proxy network. Other than electricity and ISP fees it\u2019s all a sunk costs paid off many years ago."}
{"id": "mrw8tu6", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrw8tu6/", "author": "surfskyofficial", "created_utc": 1747048852, "score": 9, "content": "In our infrastructure, scrape over 10M pages daily. It's not always cost-effective to use residential proxies for server requests and assets. With some outdated or easy-level antibot systems, you can extract cookies and use cheaper server proxies until they expire. You can also use a hybrid approach where xhr / fetch requests are executed using less expensive proxies. Server proxies can be purchased for less than $0.05 per unit each with unmetered 100+ Gbps (over 10x savings). As mentioned above, it's good practice to block unnecessary resources. If using Chrome / Chromium, you can pass the --proxy-bypass-list flag without the need for filtering in your framework like Playwright / Puppeteer. If you still need to load assets, you can add a shared cache that can be reused between browser instances. If you frequently work with the same website and use a headless browser, reuse the session and store cache, cookies, local storage, and sometimes service workers. This above save up to 90-95% of traffic costs. For complex websites, at 1M requests, you can save around $950 on proxies alone, and at $0.5/GB, about $30-40. The RTT between your scraping infra and the upstream API / proxy servers is also important. Every interaction with the page, including seemingly simple ones, may trigger multiple CDP, which increases the RTT. You can typically achieve at least 2x latency reduction by placing servers in the right geographic locations and data centers, sometimes even achieving 5x improvement. There are more ways to decrease costs at scale, e.g. using anti-detect browsers, pipelines, warmed-up browsers, but that's another story."}
{"id": "mrqxibk", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqxibk/", "author": "Oblivian69", "created_utc": 1746971032, "score": 5, "content": "I had to bump up aws resources because of web scraping. 1 day and $250 later I implemented fail2ban. If they would have been polite and not hammer the servers they could still be scrapping my stuff"}
{"id": "mrpvoto", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpvoto/", "author": "iamzamek", "created_utc": 1746951233, "score": 3, "content": "Remindme! 48 hours"}
{"id": "mrrb7rq", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrrb7rq/", "author": "None", "created_utc": 1746975691, "score": 3, "content": "[deleted]"}
{"id": "mrvz9dk", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrvz9dk/", "author": "shantud", "created_utc": 1747043757, "score": 3, "content": "I make my own chrome extensions using cursor for every website I want to scrape. Automate Injecting js code to do all work and save json data locally. Instead of proxies. I use android apps (their ips) connected to my wifi to keep changing ips to not get the privilege of getting blacklisted. Ik it is very slow to do this, to manually load pages, manually change proxies after every 70-100 pages, scroll like a human user, then inject code to get json data locally. But I don't like the target website getting loaded with requests after which they'll definitely work on their anti scraping measures. I like to replicate real users, somehow it feels ethical to me."}
{"id": "mrpuqwj", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpuqwj/", "author": "moiz9900", "created_utc": 1746950644, "score": 2, "content": "Remind me 24 hours!"}
{"id": "mrpv9gn", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpv9gn/", "author": "jlg30730", "created_utc": 1746950966, "score": 2, "content": "Remind me 24 hours"}
{"id": "mrqn49u", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqn49u/", "author": "foeffa", "created_utc": 1746966861, "score": 1, "content": "Remindme! 24 hours"}
{"id": "mrqy5la", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqy5la/", "author": "cgoldberg", "created_utc": 1746971270, "score": 1, "content": "If you are scraping at scale, you are paying for infrastructure."}
{"id": "mrr5vm6", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr5vm6/", "author": "None", "created_utc": 1746973959, "score": 1, "content": "[removed]"}
{"id": "mrs2d37", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrs2d37/", "author": "wannabe_kinkg", "created_utc": 1746984322, "score": 1, "content": "what are you guys doing with it? I know how to scrap too but not working anywhere, is there anything I could do if I do it myself?"}
{"id": "mrupbwy", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrupbwy/", "author": "External_Skirt9918", "created_utc": 1747017967, "score": 1, "content": "If you are from india and i would suggest to use tailscale and connect your broadband router to the VPS. If IP is blocked just turn it off and on the router to get new ip and im scrapping here like a hell with that. They are providing me 3TB of bandwidth per month and paying 7$ for broadband and VPS per month 50$ with spec of 4 core and 12GB obviously its from lowendtalk openvz from TNAHOSTING"}
{"id": "mrz0sin", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrz0sin/", "author": "askolein", "created_utc": 1747080840, "score": 1, "content": "In reality scraping at a moderate scale immediately costs 1-5k/month and large scale real time scraping can cost easily 10-50k/month in larger orgs, without data pipeline and engineering considerations. I am conservative here. Senior data engineer."}
{"id": "msdbsok", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/msdbsok/", "author": "None", "created_utc": 1747270114, "score": 1, "content": "[removed]"}
{"id": "msoff94", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/msoff94/", "author": "None", "created_utc": 1747423882, "score": 1, "content": "[removed]"}
{"id": "my2awmp", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/my2awmp/", "author": "None", "created_utc": 1750073793, "score": 1, "content": "[removed]"}
{"id": "n1yqgnh", "type": "comment", "parent_id": "t3_1kjvv68", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/n1yqgnh/", "author": "GoolyK", "created_utc": 1751972161, "score": 1, "content": "Great question. Your confusion about the low cost figures makes sense because they often leave out the most important part of the strategy. The secret is that nobody doing serious volume affordably is paying the per gigabyte fees for those big residential proxy networks. The real strategy is to use dedicated datacenter or ISP proxies. For many sites fast proxies from a reputable datacenter are perfectly fine. You can get these for a flat monthly fee with unlimited bandwidth which gives you a predictable low operational cost. For tougher targets you can build a fallback system. You use the cheap datacenter proxies for almost all requests. If one fails your system automatically retries with a higher trust mobile proxy. It is worth researching how mobile IPs work because anti bot systems are very reluctant to block them. They are highly effective. The problem then is not bandwidth cost but management. The challenge is rotating thousands of your own proxies and handling complex fallback logic without it becoming a nightmare. So the formula is cheap dedicated IPs plus a smart management system. That is how you get to millions of pages without spending a fortune. Hope that helps."}
{"id": "mrqhhmm", "type": "comment", "parent_id": "t1_mrpxlv4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqhhmm/", "author": "OkTry9715", "created_utc": 1746964293, "score": 15, "content": "Some websites (especially sport bookmakers) have ability to detect that you are using API instead of browser and instantly ban you."}
{"id": "mrq3akz", "type": "comment", "parent_id": "t1_mrpxlv4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrq3akz/", "author": "4bhii", "created_utc": 1746956156, "score": 3, "content": "how do you find those hidden apis? like php apis what doesn't even show in network tab"}
{"id": "mrvip61", "type": "comment", "parent_id": "t1_mrpxlv4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrvip61/", "author": "deadcoder0904", "created_utc": 1747033114, "score": 1, "content": "> there's no need to load all the images on the login page, you probably only need the form and the submit button. how do you know the image isn't captcha? just through manual flow? i've never heard about this before but damn its pretty dang good insight."}
{"id": "mrpysbl", "type": "comment", "parent_id": "t1_mrpx9yq", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpysbl/", "author": "aaronn2", "created_utc": 1746953236, "score": 6, "content": "\"Just my two cents, ISP proxies are pretty reliable, but datacenter proxies are the worst; they get detected almost instantly.\" I'm not very very experiences in this field, but for that price of $3/week for an ISP - isn't ISP provide 1 or 2 proxies? So effectively, you are still using that 1 or 2 proxies to scrape 2M requests? I thought that this would be a red flag for the administrators of that website and they would ban that IP."}
{"id": "mrvc4el", "type": "comment", "parent_id": "t1_mrpx9yq", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrvc4el/", "author": "None", "created_utc": 1747029213, "score": 1, "content": "[removed]"}
{"id": "mrpyzgz", "type": "comment", "parent_id": "t1_mrpw314", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpyzgz/", "author": "aaronn2", "created_utc": 1746953366, "score": 2, "content": "Unmetered proxy plan = ISP? And an ISP package contains typically 1-5 (maybe up to 10) IPs? So basically, that 1M pages per day serve those 1-10 IPs?"}
{"id": "mrurl0k", "type": "comment", "parent_id": "t1_mrpw314", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrurl0k/", "author": "ruzigcode", "created_utc": 1747018931, "score": 2, "content": "The cheapest services offer at scale is about 2-4 USD per 1000 requests. For 1M pages, it should be around 2000 - 4000 USD. You can not find any cheaper prices at scale. If you buy the proxies, buy captcha resolver services, hire devs to build scrapers... it will be cheaper but unreliable for sure."}
{"id": "mrurok5", "type": "comment", "parent_id": "t1_mrpwxzo", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrurok5/", "author": "ruzigcode", "created_utc": 1747018973, "score": 2, "content": "Yes, proxies is a must-have component in web scraping."}
{"id": "mswtidl", "type": "comment", "parent_id": "t1_mrpwxzo", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mswtidl/", "author": "hanktertelbaum", "created_utc": 1747546419, "score": 2, "content": "Can you explain large scale? What/where do the constraints come into play?"}
{"id": "mrr3chg", "type": "comment", "parent_id": "t1_mrqckoh", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr3chg/", "author": "aaronn2", "created_utc": 1746973113, "score": 2, "content": "I assume \"1 billion of product prices\" != 1 billion requests, right? Shall I ask you what do you mean by \"rotating IPs by using cloud providers\u2019 VMs\"? Specifically cloud providers' VMs?"}
{"id": "mrr3ajd", "type": "comment", "parent_id": "t1_mrqckoh", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr3ajd/", "author": "RobSm", "created_utc": 1746973094, "score": 1, "content": "How do you rotate VMs at scale?"}
{"id": "mrtmcl1", "type": "comment", "parent_id": "t1_mrqckoh", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrtmcl1/", "author": "None", "created_utc": 1747002934, "score": 1, "content": "[removed]"}
{"id": "mrz133w", "type": "comment", "parent_id": "t1_mrqckoh", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrz133w/", "author": "askolein", "created_utc": 1747080926, "score": 1, "content": "Why just mention the proxy? Seems like the sites you scrape are not that defended. How about the rest (VM and DBs)?"}
{"id": "mrr47ju", "type": "comment", "parent_id": "t1_mrqzyjk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr47ju/", "author": "aaronn2", "created_utc": 1746973401, "score": 5, "content": "I am very interested to learn about the proxy network. How and/or where do you source it? How much do you pay for it on a monthly basis? Isn't it that you need to regularly check if the proxies are still working, so you removed the invalid ones from your pool?"}
{"id": "mrtk5s2", "type": "comment", "parent_id": "t1_mrqzyjk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrtk5s2/", "author": "JitStill", "created_utc": 1747002125, "score": 1, "content": "Same. This seems interesting."}
{"id": "ms14s7p", "type": "comment", "parent_id": "t1_mrqxibk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms14s7p/", "author": "Not_your_guy_buddy42", "created_utc": 1747106440, "score": 2, "content": "i had to scroll SO far down to find the first view from the victim side of scraping , but to anyone paying bandwidth cost scrapers are basically the plague lol and this thread is a bit of a \"Are we ze Baddies, Hans\" xD"}
{"id": "mrr9i50", "type": "comment", "parent_id": "t1_mrqxibk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr9i50/", "author": "thefirstfedora", "created_utc": 1746975147, "score": 2, "content": "That's interesting, I had a website ban my ip after 4 failed login attempts (sometimes less) but they failed for unknown reasons because the login credentials were correct. So you could be accidentally banning actual users lol"}
{"id": "mrpvsfc", "type": "comment", "parent_id": "t1_mrpvoto", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpvsfc/", "author": "RemindMeBot", "created_utc": 1746951296, "score": 1, "content": "I will be messaging you in 2 days on [**2025-05-13 08:13:53 UTC**]( to remind you of [**this link**]( [**CLICK THIS LINK**]( to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)]( ***** |[^(Info)]( Reminders)]( |-|-|-|-|"}
{"id": "mrpxk3y", "type": "comment", "parent_id": "t1_mrpvoto", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrpxk3y/", "author": "ConsiderationHot8106", "created_utc": 1746952440, "score": 0, "content": "Why?"}
{"id": "mrsc0iw", "type": "comment", "parent_id": "t1_mrrb7rq", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrsc0iw/", "author": "No-Drummer4059", "created_utc": 1746987376, "score": 4, "content": "where do you sell the data?"}
{"id": "mrw9wcv", "type": "comment", "parent_id": "t1_mrvz9dk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrw9wcv/", "author": "surfskyofficial", "created_utc": 1747049354, "score": 3, "content": "it's important to consider that methods that allow injecting and executing custom js like playwright's addInitScript may be detected by the website in some cases."}
{"id": "ms4kf2c", "type": "comment", "parent_id": "t1_mrvz9dk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms4kf2c/", "author": "Axelblase", "created_utc": 1747157761, "score": 2, "content": "I don\u2019t understand when you say you use android apps. You mean you use multiple phones to access a webpage through your WiFi network?"}
{"id": "ms17192", "type": "comment", "parent_id": "t1_mrvz9dk", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms17192/", "author": "didanet", "created_utc": 1747107369, "score": 1, "content": "Hey, u/shantud! Great idea. Could you shed some light on how you made it? I'm working on a project that needs to scrap 40-50 websites"}
{"id": "mrr4jza", "type": "comment", "parent_id": "t1_mrqy5la", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr4jza/", "author": "aaronn2", "created_utc": 1746973517, "score": 1, "content": "I understand that it costs money. When reading through this sub-reddit, I somehow got an impression that the professional individuals pay basically close to zero in costs, while when I look at prices of some API solutions or residential proxies, the costs are quite significant, especially when making 10M+ requests per month."}
{"id": "mrri9bg", "type": "comment", "parent_id": "t1_mrr5vm6", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrri9bg/", "author": "webscraping-ModTeam", "created_utc": 1746977930, "score": 1, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "msqntlv", "type": "comment", "parent_id": "t1_mrupbwy", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/msqntlv/", "author": "apple1064", "created_utc": 1747453318, "score": 1, "content": ""}
{"id": "n1cv0il", "type": "comment", "parent_id": "t1_mrupbwy", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/n1cv0il/", "author": "sdjnd", "created_utc": 1751657268, "score": 1, "content": "But won't your personal ip get blocked even when using tailscale?"}
{"id": "ms1pa3m", "type": "comment", "parent_id": "t1_mrz0sin", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms1pa3m/", "author": "aaronn2", "created_utc": 1747116026, "score": 1, "content": "Hello, and thank you. What number of requests do you consider \"moderate scale\" per month? 1M, or 5M, or 10M? And large scale? By data pipeline - do you mean by that extracting details from the scraped information and cleaning it up before saving it to the database?"}
{"id": "msdklg4", "type": "comment", "parent_id": "t1_msdbsok", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/msdklg4/", "author": "webscraping-ModTeam", "created_utc": 1747273147, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "msp3jv9", "type": "comment", "parent_id": "t1_msoff94", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/msp3jv9/", "author": "webscraping-ModTeam", "created_utc": 1747431330, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "my2s5q0", "type": "comment", "parent_id": "t1_my2awmp", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/my2s5q0/", "author": "webscraping-ModTeam", "created_utc": 1750080464, "score": 1, "content": "Please review the sub rules"}
{"id": "mrqjit3", "type": "comment", "parent_id": "t1_mrqhhmm", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqjit3/", "author": "Haningauror", "created_utc": 1746965249, "score": 23, "content": "Yeah, it's basic 101, when developers build an API, they have to protect it. But isn't that like... 80% of the scraping job? Getting around detection? That's what I did with the Shopee API."}
{"id": "mrwfpm2", "type": "comment", "parent_id": "t1_mrqhhmm", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrwfpm2/", "author": "LinuxTux01", "created_utc": 1747051916, "score": 2, "content": "Then found a way around it lol. An http request is still an http request whether done by a browser or a script"}
{"id": "mrq8oj6", "type": "comment", "parent_id": "t1_mrq3akz", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrq8oj6/", "author": "vinilios", "created_utc": 1746959557, "score": 18, "content": "if you monitor a browsing session on a website you may find out that most of the information is coming through some kind of api rest calls, if you analyse these calls you can reproduce the communication and extract needed information via these calls with no browser overhead"}
{"id": "mrwch6j", "type": "comment", "parent_id": "t1_mrq3akz", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrwch6j/", "author": "fftommi", "created_utc": 1747050522, "score": 5, "content": "John Watson Rooney on YouTube has some really great vids explain stuff like this"}
{"id": "mrqjoc6", "type": "comment", "parent_id": "t1_mrq3akz", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqjoc6/", "author": "Haningauror", "created_utc": 1746965319, "score": 2, "content": "Well, if it's MVC, there's no way around it. But most websites, especially complex ones, call their APIs for data instead of serving it through PHP."}
{"id": "mrvpnwv", "type": "comment", "parent_id": "t1_mrvip61", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrvpnwv/", "author": "Haningauror", "created_utc": 1747037535, "score": 4, "content": "If it's a CAPTCHA, it will have a CDN path, class, or ID that indicates it's a CAPTCHA. If I detect it, I just skip the blocking part. Funnily enough, on a poorly designed website, I once blocked the CAPTCHA's JS request and it bypassed it, lol. Not going to work on well-equipped websites, though."}
{"id": "mrq08g4", "type": "comment", "parent_id": "t1_mrpysbl", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrq08g4/", "author": "albert_in_vine", "created_utc": 1746954161, "score": 6, "content": "You can choose the number of proxies based on the pricing. I used around 20 proxies and since you can refresh them 3 times, that gave me about 60 in total. I also set up a browser fingerprint, and so far, I haven\u2019t been banned."}
{"id": "mrutkps", "type": "comment", "parent_id": "t1_mrurl0k", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrutkps/", "author": "None", "created_utc": 1747019807, "score": 4, "content": "[removed]"}
{"id": "ms35qde", "type": "comment", "parent_id": "t1_mrurl0k", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms35qde/", "author": "ish099", "created_utc": 1747142657, "score": 1, "content": "This is wrong! If you figure out all the possible ways you are being fingerprinted by websites, you can build unique signatures directly into your bots."}
{"id": "mrrextw", "type": "comment", "parent_id": "t1_mrr3chg", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrrextw/", "author": "Pigik83", "created_utc": 1746976871, "score": 7, "content": "Correct, but we\u2019re still talking about several million requests per day. You basically have two ways: - create an automation that deploys your scrapers to a newly created VM and executes it. At the end of the execution, VM is killed - use a proxy manager that spawns the VMs for you and configures them as a proxy, rotating them."}
{"id": "mrrfcie", "type": "comment", "parent_id": "t1_mrr3ajd", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrrfcie/", "author": "Pigik83", "created_utc": 1746977001, "score": 7, "content": "As mentioned in another comment, you simply create and kill VMs where you upload the code and run it. Or you can use a proxy manager that spawns them for you and rotate them. Consider you can use different could providers at the same time"}
{"id": "mrto0h4", "type": "comment", "parent_id": "t1_mrtmcl1", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrto0h4/", "author": "webscraping-ModTeam", "created_utc": 1747003557, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "ms1l4ts", "type": "comment", "parent_id": "t1_mrz133w", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms1l4ts/", "author": "Pigik83", "created_utc": 1747113825, "score": 1, "content": "Of course in the remaining 20% of the websites you have antibots and then you have to choose from site to site if it\u2019s better to use unblockers or a custom solution. Our cloud bill ranges between 5-7k per month, split in different providers. This is because all the executions of the scrapers are on the cloud, as the DB"}
{"id": "mrqdx0m", "type": "comment", "parent_id": "t1_mrpxk3y", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqdx0m/", "author": "Furrynote", "created_utc": 1746962524, "score": 6, "content": "So he can read the responses after some time and soak up some knowledge"}
{"id": "mrsgaxd", "type": "comment", "parent_id": "t1_mrsc0iw", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrsgaxd/", "author": "Infamous_Pickle2975", "created_utc": 1746988781, "score": 3, "content": "That is a great question and I would be interested to know as well"}
{"id": "mxgj6x0", "type": "comment", "parent_id": "t1_ms4kf2c", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mxgj6x0/", "author": "Local-Hornet-3057", "created_utc": 1749765624, "score": 1, "content": "If you got an answer to this part I'd like to know if it's not a problem."}
{"id": "mshguc3", "type": "comment", "parent_id": "t1_ms17192", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mshguc3/", "author": "shantud", "created_utc": 1747330830, "score": 1, "content": "Just use any ai to code the chrome extension. Start with \"code me an extension for <site> for this these data.\" As you move forward provide 2-3 whole pages source code from the products/pages of the target website to the ai so that it can distinguish between the elements to find the proper selectors to get the data. Make sure you give the ai prompts like 'separate window for the chrome extension when invoked' also for opening the target website links so that instead of the extension being on the same page it could work as a separate tool even when the page it was invoked on is closed. Keep taking backups of the source code as you're building. \\+Many other things."}
{"id": "mrr55hr", "type": "comment", "parent_id": "t1_mrr4jza", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrr55hr/", "author": "cgoldberg", "created_utc": 1746973718, "score": 4, "content": "You got the wrong impression. Nobody is doing data collection at scale and paying zero for infrastructure."}
{"id": "n1e7xgx", "type": "comment", "parent_id": "t1_n1cv0il", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/n1e7xgx/", "author": "Odd_Insect_9759", "created_utc": 1751674581, "score": 1, "content": "It will be blocked. I will turn off and on the router. It will give you new ip"}
{"id": "ms29shg", "type": "comment", "parent_id": "t1_ms1pa3m", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms29shg/", "author": "askolein", "created_utc": 1747128479, "score": 3, "content": "moderate scale is 1M per day I would say. large scale are in billions generally, per month. depends on how you define datapoints but it's generally like that. Data pipeline: yes, all the ETL process, the databases, the s3 buckets, the various monitoring systems, the VMs to run it all and any orchestration on top of it (k8s, k3s, if any.)"}
{"id": "mrs00an", "type": "comment", "parent_id": "t1_mrqjit3", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrs00an/", "author": "Brlala", "created_utc": 1746983588, "score": 2, "content": "Shopee now throws error in the page when you open the network tab, what\u2019s the way you got around this to capture network request?"}
{"id": "mrq72ve", "type": "comment", "parent_id": "t1_mrq08g4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrq72ve/", "author": "seateq64", "created_utc": 1746958566, "score": 2, "content": "2m requests from 60 proxies sounds quite risky. The website must be having quite low level protection Usually websites have limit on requests from a single IP per minute. If u reach that number - IP gets blocked"}
{"id": "ms07rvx", "type": "comment", "parent_id": "t1_mrutkps", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms07rvx/", "author": "ruzigcode", "created_utc": 1747094661, "score": 1, "content": "If you scrape unpopular websites, it will be very easy. But if you scrape like Google pages, it is very challenging. Unreliable I mean services like Google have many ways to block bots. You also need to maintain your scrapers, there are many different pages, different selectors"}
{"id": "ms07wry", "type": "comment", "parent_id": "t1_mrutkps", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms07wry/", "author": "ruzigcode", "created_utc": 1747094708, "score": 1, "content": "Also, Scraping at scale, you face many errors, weird errors. Services already handle them for you."}
{"id": "my0sb14", "type": "comment", "parent_id": "t1_ms35qde", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/my0sb14/", "author": "ruzigcode", "created_utc": 1750044524, "score": 1, "content": "Could you show more insights? Any sources, refs or examples? I would love to know cause I built and use many scrapers but I may some blind spots"}
{"id": "mrrs3o2", "type": "comment", "parent_id": "t1_mrrfcie", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrrs3o2/", "author": "RobSm", "created_utc": 1746981085, "score": 2, "content": "Sure, I am more interested in exact tools you use to manage VM spawning and termination. Feel free to DM if you don't want to mention brands. Thanks."}
{"id": "ms3hma5", "type": "comment", "parent_id": "t1_mrrfcie", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms3hma5/", "author": "ish099", "created_utc": 1747146443, "score": 1, "content": "VMs are very hardware expensive and difficult to scale, why don't you consider using containerization instead"}
{"id": "ms2a43g", "type": "comment", "parent_id": "t1_ms1l4ts", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/ms2a43g/", "author": "askolein", "created_utc": 1747128679, "score": 2, "content": "Sounds similar to my company"}
{"id": "mrtr4q6", "type": "comment", "parent_id": "t1_mrs00an", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrtr4q6/", "author": "Haningauror", "created_utc": 1747004719, "score": 6, "content": "Yes, Shopee now detects CDP, I can only say it's possible to get around it with other network capturer tools."}
{"id": "mse887w", "type": "comment", "parent_id": "t1_mrs00an", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mse887w/", "author": "theSharkkk", "created_utc": 1747282389, "score": 2, "content": "You can use HTTP Toolkit"}
{"id": "mry3ccp", "type": "comment", "parent_id": "t1_mrs00an", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mry3ccp/", "author": "Lafftar", "created_utc": 1747070940, "score": 1, "content": "Use burp suite, or Charles proxy or fiddler."}
{"id": "mrqh5rd", "type": "comment", "parent_id": "t1_mrq72ve", "permalink": "https://www.reddit.com/r/webscraping/comments/1kjvv68/the_real_costs_of_web_scraping/mrqh5rd/", "author": "uxgb", "created_utc": 1746964134, "score": 2, "content": "If you are crawling many different sites (not just hundreds of thousands of of pages in a single site) you can add some logic to spread out your requests over time when they hit the same site or hosting provider. That way you don't really hit the \"x request per minute\". Basically do one page for each site first, then 2nd page of each site, etc. It can become more tricky if you need sticky sessions but the basic principle still applies."}
{"id": "1iapbff", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/", "author": "sachinsankar", "created_utc": 1737923189, "score": 157, "title": "I Made My Python Proxy Library 15x Faster \u2013 Perfect for Web Scraping!", "content": "Hey r/webscraping! If you\u2019re tired of getting IP-banned or waiting ages for proxy validation, I\u2019ve got news for you: I just released **v2.0.0** of my Python library, **[swiftshadow]( and it\u2019s now **15x faster** thanks to async magic! ### **What\u2019s New?** \u26a1 **15x Speed Boost**: Rewrote proxy validation with `aio \u2013 dropped from **~160s** to **~10s** for 100 proxies. **8 New Providers**: Added sources like *KangProxy*, *GoodProxy*, and *Anonym0usWork1221* for more reliable IPs. **Proxy Class**: Use `Proxy.as_requests_dict()` to plug directly into `requests` or ` \ufe0f **Faster Caching**: Switched to `pickle` \u2013 no more JSON slowdowns. ### **Why It Matters for Scraping** - **Avoid Bans**: Rotate proxies seamlessly during large-scale scraping. - **Speed**: Validate hundreds of proxies in seconds, not minutes. - **Flexibility**: Filter by country/protocol (HTTP/HTTPS) to match your target site. ### **Get Started** ```bash pip install swiftshadow ``` Basic usage: ```python from swiftshadow import ProxyInterface # Fetch and auto-rotate proxies proxy_manager = ProxyInterface(autoRotate=True) proxy = proxy_manager.get() # Use with requests import requests response = requests.get(\" proxies=proxy.as_requests_dict()) ``` ### **Benchmark Comparison** | Task | v1.2.1 (Sync) | v2.0.0 (Async) | |---------------------|---------------|----------------| | Validate 100 Proxies | ~160s | **~10s** | ### **Why Use This Over Alternatives?** Most free proxy tools are slow, unreliable, or lack async support. **swiftshadow** focuses on: - **Speed**: Async-first design for large-scale scraping. - **Simplicity**: No complex setup \u2013 just import and go. - **Transparency**: Open-source with type hints for easy debugging. ### **Try It & Feedback Welcome!** GitHub: [github.com/sachin-sankar/swiftshadow]( Let me know how it works for your projects! If you hit issues or have ideas, open a GitHub ticket. Stars \u2b50 are appreciated too! --- **TL;DR:** Async proxy validation = 15x faster scraping. Avoid bans, save time, and scrape smarter. \ufe0f"}
{"id": "m9c2uog", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9c2uog/", "author": "Lopsided_Speaker_553", "created_utc": 1737926306, "score": 3, "content": "Hey, this sounds really cool. I\u2019m not a Python programmer (do have a little knowledge), so what do you think, would it be possible for me to create a local http endpoint from your library that returns the proxy to, say, a nodejs client? That way other languages could benefit from your code."}
{"id": "m9qm9a2", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9qm9a2/", "author": "chrislbrown84", "created_utc": 1738111325, "score": 2, "content": "This looks handy, thanks"}
{"id": "m9dral5", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9dral5/", "author": "alexp9000", "created_utc": 1737943908, "score": 1, "content": "Any chance this could work for curl\\_cffi? New to scraping and working with a tricky site, think curl\\_cffi would be the best for my use case. This looks amazing, will definitely give it a try."}
{"id": "m9esi33", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9esi33/", "author": "Djkid4lyfe", "created_utc": 1737957961, "score": 1, "content": "Built in cloudflare bypass? Can u implement a waf session or something?"}
{"id": "m9hij2j", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9hij2j/", "author": "AwareSeaworthiness52", "created_utc": 1737999508, "score": 1, "content": "Does this work on all websites/what are the limitations? And what's the reliability rate? We had to switch our proxy provider because our previous one blocks all government websites. Our new provider is expensive :("}
{"id": "m9pabmj", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9pabmj/", "author": "None", "created_utc": 1738097174, "score": 1, "content": "[removed]"}
{"id": "mazngmd", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/mazngmd/", "author": "damian_konin", "created_utc": 1738703040, "score": 1, "content": "Hi, this import from swiftshadow.classes import ProxyInterface as well as from swiftshadow import ProxyInterface give me ImportError, any idea why? Am I doing something wrong? I just pip installed it, and tried to run some examples from here and from github. And this one works for me: from swiftshadow import QuickProxy"}
{"id": "n25ia2l", "type": "comment", "parent_id": "t3_1iapbff", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/n25ia2l/", "author": "PrestigiousZombie531", "created_utc": 1752058371, "score": 1, "content": "I ll definitely take a deep hard look into this one but curious, what are some other libraries that are capable of doing what swiftshadow does"}
{"id": "m9c41o7", "type": "comment", "parent_id": "t1_m9c2uog", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9c41o7/", "author": "sachinsankar", "created_utc": 1737926619, "score": 8, "content": "```py from fastapi import FastAPI from swiftshadow import ProxyInterface from pydantic import BaseModel # Initialize FastAPI app app = FastAPI() # Initialize swiftshadow ProxyInterface proxy_manager = ProxyInterface(autoRotate=True) # Pydantic model for JSON response class ProxyResponse(BaseModel): ip: str port: int protocol: str proxy_string: str @app.get(\"/random-proxy\", response_model=ProxyResponse) async def get_random_proxy(): \"\"\" Fetch a random proxy from swiftshadow and return it as JSON. \"\"\" # Get a random proxy proxy = proxy_manager.get() # Prepare the response response = ProxyResponse( ip=proxy.ip, port=proxy.port, protocol=proxy.protocol, proxy_string=proxy.as_string(), ) return response # Run the server if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"0.0.0.0\", port=8000) ```"}
{"id": "m9c30i4", "type": "comment", "parent_id": "t1_m9c2uog", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9c30i4/", "author": "sachinsankar", "created_utc": 1737926350, "score": 2, "content": "you sure can"}
{"id": "m9ehieg", "type": "comment", "parent_id": "t1_m9dral5", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9ehieg/", "author": "sachinsankar", "created_utc": 1737952857, "score": 1, "content": "```python from swiftshadow.classes import ProxyInterface from curl_cffi import requests swift = ProxyInterface() for proxy in swift.proxies: resp = requests.get(\" proxy=proxy.as_string()) print(resp.text) ```"}
{"id": "m9fqgip", "type": "comment", "parent_id": "t1_m9esi33", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9fqgip/", "author": "sachinsankar", "created_utc": 1737978005, "score": 3, "content": "Cloudflare bypassing is not in the scope of the project."}
{"id": "m9hlpnk", "type": "comment", "parent_id": "t1_m9hij2j", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9hlpnk/", "author": "LinuxTux01", "created_utc": 1738000381, "score": 1, "content": "They are free so they're probably flagged"}
{"id": "mad2ngq", "type": "comment", "parent_id": "t1_m9pabmj", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/mad2ngq/", "author": "sachinsankar", "created_utc": 1738410743, "score": 1, "content": "sure"}
{"id": "mce698v", "type": "comment", "parent_id": "t1_mazngmd", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/mce698v/", "author": "Careless_Jelly_3186", "created_utc": 1739379593, "score": 1, "content": "Checking the classes code within swiftshadow library and see if the class name was Proxy or ProxyInterface. If it's the later, then it's supposed to be: import ProxyInterface. If it's the former then change it to import Proxy."}
{"id": "m9cjmuz", "type": "comment", "parent_id": "t1_m9c41o7", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9cjmuz/", "author": "Lopsided_Speaker_553", "created_utc": 1737930952, "score": 3, "content": "Wow, I didn\u2019t expect that! Thanks for your reply. I\u2019ll try and get working in my setup this week"}
{"id": "m9ehoch", "type": "comment", "parent_id": "t1_m9ehieg", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9ehoch/", "author": "alexp9000", "created_utc": 1737952927, "score": 2, "content": "Thank you for the quick response \u2014appreciate you breaking it down for me (a noob)!"}
{"id": "m9efj7n", "type": "comment", "parent_id": "t1_m9cjmuz", "permalink": "https://www.reddit.com/r/webscraping/comments/1iapbff/i_made_my_python_proxy_library_15x_faster_perfect/m9efj7n/", "author": "sachinsankar", "created_utc": 1737952038, "score": 3, "content": "Have fun"}
{"id": "1jugawo", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/", "author": "0xReaper", "created_utc": 1744125993, "score": 159, "title": "Scrapling v0.2.99 website - Effortless Web Scraping with Python!", "content": "Scrapling is an Undetectable, high-performance, intelligent Web scraping library for Python 3 to make Web Scraping easy! Scrapling isn't only about making undetectable requests or fetching pages under the radar! It has its own parser that adapts to website changes and provides many element selection/querying options other than traditional selectors, powerful DOM traversal API, and many other features while significantly outperforming popular parsing alternatives. Scrapling is built from the ground up by Web scraping experts for beginners and experts. The goal is to provide powerful features while maintaining simplicity and minimal boilerplate code. After a long wait (and a battle with perfectionism), I\u2019m excited to finally launch the official documentation website for **Scrapling** Why this matters: * Scrapling has grown greatly, and the old README wasn\u2019t enough. * The new site includes detailed documentation with rich examples \u2014 especially for Fetchers \u2014 to help both beginners and advanced users. * It also features helpful articles like how to migrate from BeautifulSoup to Scrapling. * Plus, an auto-generated reference section from the library\u2019s source code makes exploring internal functions much easier. > This has been long overdue, but I wanted it to reflect the level of quality I\u2019m proud of. Now that it\u2019s live, I can fully focus on building v3, which will be a game-changer Link: Thanks for the support! \u2764\ufe0f"}
{"id": "mm1wtfh", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm1wtfh/", "author": "dimsumham", "created_utc": 1744127626, "score": 4, "content": "How does the stealthy fetching work for http calls? On mobile and very curious."}
{"id": "mmjilft", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmjilft/", "author": "Apprehensive-Mind212", "created_utc": 1744368661, "score": 3, "content": "Great lib, build one for my react-native app using webview and js. For iqloud protection I only check if there is then I await and present a modal for user to verify, from time to time. Dose your script work for react-native ? Otherwise greet script."}
{"id": "mm3a6s3", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm3a6s3/", "author": "LocalLeadsUSA", "created_utc": 1744142006, "score": 2, "content": "This is awesome! Definitely going to try it."}
{"id": "mm8w2hy", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm8w2hy/", "author": "Murky-End-1134", "created_utc": 1744220122, "score": 2, "content": "Wating for Using Scrapling instead of AI \u2764\ufe0f"}
{"id": "mm2bkwn", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2bkwn/", "author": "yousephx", "created_utc": 1744131994, "score": 1, "content": "How does this compare with Crawl4AI?"}
{"id": "mm2u27u", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2u27u/", "author": "None", "created_utc": 1744137238, "score": 1, "content": "[deleted]"}
{"id": "mm4j1l3", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm4j1l3/", "author": "None", "created_utc": 1744156053, "score": 1, "content": "How does it go on creepy fingerprinting?"}
{"id": "mm6fzjq", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm6fzjq/", "author": "Upbeat_Invite3782", "created_utc": 1744186759, "score": 1, "content": "I'm a bit new to scraping, but can this be used instead of being used for scraping, but instead be used to navigate through a site automatically? Like I would need it to log in, click certain buttons, and input things a bit?"}
{"id": "mm6hnnf", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm6hnnf/", "author": "ViperAMD", "created_utc": 1744187874, "score": 1, "content": "Any benefits over seleniumbase?"}
{"id": "mm73atu", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm73atu/", "author": "planetearth80", "created_utc": 1744199688, "score": 1, "content": "Does it support capturing network requests (fetch/xhr)?"}
{"id": "mm7l6mu", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm7l6mu/", "author": "SeamusCowden", "created_utc": 1744206225, "score": 1, "content": "Looks great. Will test this out. I am particularly interested in scraping/crawling content behind paywalls. How effective it this for it?"}
{"id": "mm7zfok", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm7zfok/", "author": "ciapsss", "created_utc": 1744210568, "score": 1, "content": "Looks cool, does it handlem cookies pop ups? E.g. some website have content gated behind cookie popup"}
{"id": "mm9w14z", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm9w14z/", "author": "SpiritualReply1889", "created_utc": 1744230498, "score": 1, "content": "Looks great, is there a way to detect which web pages generate dynamic content for scraping and need js enabled vs web pages whose text content can be fetched directly using fetcher so that we don\u2019t have to open a browser every time? Context: am looking for a scraper to scrape content and feed it to AI, and hence, it should handle scraping for almost any web page without specific rule based extraction."}
{"id": "mmci6ua", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmci6ua/", "author": "Mefisto4444", "created_utc": 1744266277, "score": 1, "content": "Do you plan on integrating http libraries that spoof TLS like curl-cffi or hrequests?"}
{"id": "mmd6wrs", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmd6wrs/", "author": "intentazera", "created_utc": 1744281687, "score": 1, "content": "Could this be used to develop an Instagram public post archiving system where the IG poster's pictures/videos are also downloaded locally, as well as comments + commentor names etc? I haven't come across one that can do this yet."}
{"id": "mmh9a5v", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmh9a5v/", "author": "Infamous_Tomatillo53", "created_utc": 1744328989, "score": 1, "content": "I haven't fully tested it out yet. But I pinged a Amazon search url with it and it appears returning the full source content - so I hope I can leverage it to overcome the issue I encountered here [ I have a few questions - 1. what underlying measures does your library take to stay \"undetected\"? 2. what's the difference or connection between scrapling, and other libraries such as nodriver, selenium, playwright, crawless, etc? Asking because I have tried many other libraries and they, overtime, have failed to scrape a lot of websites and run into anti-bot problems. 3. How can scrapling keep up with new anti-bot technologies and become a sustainable solution people can rely on? 4. Will there be support to scrape dynamic sites where javascript is needed? Or this is intended to scrape static sites? Thanks!"}
{"id": "mmlk9ox", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmlk9ox/", "author": "unnkeet", "created_utc": 1744393209, "score": 1, "content": "How does it work for dynamic content? There is a API call that gets the data I am interested in, but cookies are set based on user login, which is in turn based on solving an image based captcha. How can Scrapling help?"}
{"id": "mokxebx", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mokxebx/", "author": "dave-lon", "created_utc": 1745401067, "score": 1, "content": "can i create a scraper using vibe coding with scrapling?"}
{"id": "moxsghm", "type": "comment", "parent_id": "t3_1jugawo", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/moxsghm/", "author": "Ordinary_Floor_6628", "created_utc": 1745570560, "score": 1, "content": "Hey! I'm currently testing your Fetcher in a parallel loop. I am generally happy but after a few runs I get the following errors which breaks it: \"\\[Errno 24\\] Too many open files: '/.venv/lib/python3.8/site-packages/browserforge/headers/data/browser-helper-file.json'\" How can I solve this? Thanks!"}
{"id": "mm2724r", "type": "comment", "parent_id": "t1_mm1wtfh", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2724r/", "author": "0xReaper", "created_utc": 1744130689, "score": 7, "content": "It uses a modified Firefox browser and a bunch of tricks :) Here's the full page:"}
{"id": "mm3dv1m", "type": "comment", "parent_id": "t1_mm3a6s3", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm3dv1m/", "author": "0xReaper", "created_utc": 1744143050, "score": 2, "content": "Glad to hear that! Don't forget to give feedback :D"}
{"id": "mm8wnr0", "type": "comment", "parent_id": "t1_mm8w2hy", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm8wnr0/", "author": "0xReaper", "created_utc": 1744220284, "score": 2, "content": "The article should be finished soon :rocket:"}
{"id": "mmsjwgd", "type": "comment", "parent_id": "t1_mm8w2hy", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmsjwgd/", "author": "0xReaper", "created_utc": 1744490975, "score": 2, "content": "Here it is :)"}
{"id": "mm2efhy", "type": "comment", "parent_id": "t1_mm2bkwn", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2efhy/", "author": "0xReaper", "created_utc": 1744132813, "score": 8, "content": "Crawl4AI is simpler and has easier interfaces for linking directly to AI libraries for users without extensive programming experience. Scrapling has more features and can bypass protections that Crawl4AI can't, but it needs users' work to link it to AI libraries and isn't too easy for users without programming experience. The next version will solve that part as planned."}
{"id": "mm2z07m", "type": "comment", "parent_id": "t1_mm2u27u", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2z07m/", "author": "0xReaper", "created_utc": 1744138691, "score": 10, "content": "A lot of things like mcp server, analyzer mode, bypassing cloudflare automatically and more :)"}
{"id": "mm55h6d", "type": "comment", "parent_id": "t1_mm4j1l3", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm55h6d/", "author": "0xReaper", "created_utc": 1744163954, "score": 2, "content": "I can't upload a screenshot in the reply here, but on creepjs and Headless mode, I got a 60% trust score. I used the below code on my local machine: ```python from scrapling.fetchers import StealthyFetcher def take_screenshot(p): p.wait_for_timeout(10000) p.screenshot(path=\"screenshot.png\") return p StealthyFetcher.fetch(' page_action=take_screenshot, network_idle=True) ```"}
{"id": "mm6pcf3", "type": "comment", "parent_id": "t1_mm6fzjq", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm6pcf3/", "author": "0xReaper", "created_utc": 1744192770, "score": 1, "content": "Yes the automation part can be done through the \u2018page_action\u2019 argument"}
{"id": "mm6p8gw", "type": "comment", "parent_id": "t1_mm6hnnf", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm6p8gw/", "author": "0xReaper", "created_utc": 1744192704, "score": 1, "content": "Yes, it\u2019s better in nearly all aspects"}
{"id": "mm7q56k", "type": "comment", "parent_id": "t1_mm73atu", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm7q56k/", "author": "0xReaper", "created_utc": 1744207782, "score": 1, "content": "No, it focuses on web scraping, but it can be done through playwright API and the `page_action` argument. Through network events specifically like here"}
{"id": "mm7qqf2", "type": "comment", "parent_id": "t1_mm7l6mu", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm7qqf2/", "author": "0xReaper", "created_utc": 1744207963, "score": 1, "content": "Every paywall is a specific case, and bypassing it requires different strategies, so it's not possible for me or anyone to create a tool to bypass paywalls in general but one for each paywall if possible."}
{"id": "mm7zr7p", "type": "comment", "parent_id": "t1_mm7zfok", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm7zr7p/", "author": "0xReaper", "created_utc": 1744210661, "score": 1, "content": "Yes, it can handle it, but not automatically. You have to click the popup yourself through the `page_action` argument."}
{"id": "mmb32ip", "type": "comment", "parent_id": "t1_mm9w14z", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmb32ip/", "author": "0xReaper", "created_utc": 1744244693, "score": 1, "content": "In most cases, if you install an extension that blocks Javascript in your browser, like \"script block\", then open the website and it looks like it didn't load or look right, then it needs Javascript. This will work in most cases, but it needs an expert eye to decide."}
{"id": "mmd9bql", "type": "comment", "parent_id": "t1_mmci6ua", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmd9bql/", "author": "0xReaper", "created_utc": 1744282908, "score": 1, "content": "Yes, but I don't want to break the code for anyone already using `Fetcher`, so it is left for now till I find a way"}
{"id": "mmd9l8p", "type": "comment", "parent_id": "t1_mmd6wrs", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmd9l8p/", "author": "0xReaper", "created_utc": 1744283037, "score": 1, "content": "The library can handle Instagram so it's dependant on your web scraping skills but it can't download images, you will have to download the images with another library like"}
{"id": "mmhhcvm", "type": "comment", "parent_id": "t1_mmh9a5v", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmhhcvm/", "author": "0xReaper", "created_utc": 1744331906, "score": 3, "content": "I don't mean to be rude, but your questions show that you didn't read the documentation, which explains all of your questions."}
{"id": "molrqgt", "type": "comment", "parent_id": "t1_mokxebx", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/molrqgt/", "author": "0xReaper", "created_utc": 1745414632, "score": 1, "content": "yes you probably can"}
{"id": "mp2g9gi", "type": "comment", "parent_id": "t1_moxsghm", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mp2g9gi/", "author": "0xReaper", "created_utc": 1745628149, "score": 1, "content": "Hi, can you open a ticket for this with all the details so I can have a better look? Thanks!"}
{"id": "mm2drhs", "type": "comment", "parent_id": "t1_mm2724r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2drhs/", "author": "dimsumham", "created_utc": 1744132621, "score": 2, "content": "Thanks!"}
{"id": "mmk929r", "type": "comment", "parent_id": "t1_mm2724r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmk929r/", "author": "Bird_Idea", "created_utc": 1744379210, "score": 1, "content": "So are you saying that it's almost impossible for website to flag the scraper bot? If so, this is huge."}
{"id": "mn6lqso", "type": "comment", "parent_id": "t1_mmsjwgd", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mn6lqso/", "author": "Murky-End-1134", "created_utc": 1744691250, "score": 2, "content": "Great work \u2764\ufe0f"}
{"id": "mm2t6y6", "type": "comment", "parent_id": "t1_mm2efhy", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2t6y6/", "author": "yousephx", "created_utc": 1744136978, "score": 2, "content": "The AI point isn't that important at all actually , personally extracting data using Crawl4AI is enough for me , I do the AI work separately! Definitely I'm going to use Scrapling in the next few days!"}
{"id": "mm39nir", "type": "comment", "parent_id": "t1_mm2z07m", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm39nir/", "author": "bmrheijligers", "created_utc": 1744141857, "score": 2, "content": "Have a look at block/goose and have this as an extension. I talked to them and they are looking for a good scraping framework"}
{"id": "mm3k9vm", "type": "comment", "parent_id": "t1_mm2z07m", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm3k9vm/", "author": "None", "created_utc": 1744144852, "score": 2, "content": "[deleted]"}
{"id": "mm4qvkz", "type": "comment", "parent_id": "t1_mm2z07m", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm4qvkz/", "author": "fluffyduck420", "created_utc": 1744158800, "score": 2, "content": "DUDE YESS!!!!"}
{"id": "mmiy6uw", "type": "comment", "parent_id": "t1_mm55h6d", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmiy6uw/", "author": "None", "created_utc": 1744356105, "score": 1, "content": "Interesting, can you point me out where in the source you are defining which renderer, etc. it is going to set? Or can we customize this?"}
{"id": "mmi9jdw", "type": "comment", "parent_id": "t1_mmd9bql", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmi9jdw/", "author": "Beautiful_Art9244", "created_utc": 1744342884, "score": 2, "content": "\\+1 for this feature"}
{"id": "mmhlp6t", "type": "comment", "parent_id": "t1_mmhhcvm", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmhlp6t/", "author": "Infamous_Tomatillo53", "created_utc": 1744333505, "score": 0, "content": "huh"}
{"id": "mmkcdo9", "type": "comment", "parent_id": "t1_mmk929r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmkcdo9/", "author": "0xReaper", "created_utc": 1744380231, "score": 1, "content": "Yup with the right logic and the right proxies, it will be almost impossible to be detected."}
{"id": "mn8slfd", "type": "comment", "parent_id": "t1_mn6lqso", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mn8slfd/", "author": "0xReaper", "created_utc": 1744729335, "score": 1, "content": "Thanks \\^\\^"}
{"id": "mm2ysgy", "type": "comment", "parent_id": "t1_mm2t6y6", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm2ysgy/", "author": "0xReaper", "created_utc": 1744138628, "score": 2, "content": "Thanks mate! Don\u2019t forget to give feedback :)"}
{"id": "mm3dply", "type": "comment", "parent_id": "t1_mm39nir", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm3dply/", "author": "0xReaper", "created_utc": 1744143008, "score": 2, "content": "This is the first time I heard about that project! I will look into it. Thanks for the suggestion."}
{"id": "mm41fga", "type": "comment", "parent_id": "t1_mm3k9vm", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm41fga/", "author": "0xReaper", "created_utc": 1744150088, "score": 1, "content": "Thanks buddy \\^\\_\\^"}
{"id": "mm55we4", "type": "comment", "parent_id": "t1_mm4qvkz", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm55we4/", "author": "0xReaper", "created_utc": 1744164099, "score": 1, "content": "Just wait for it :rocket:"}
{"id": "mmjyk1v", "type": "comment", "parent_id": "t1_mmiy6uw", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmjyk1v/", "author": "0xReaper", "created_utc": 1744375557, "score": 1, "content": "The page for StealthyFetcher:"}
{"id": "mmkewsy", "type": "comment", "parent_id": "t1_mmkcdo9", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmkewsy/", "author": "Bird_Idea", "created_utc": 1744381005, "score": 1, "content": "Awesome. I'll give it a try. Do you think I could easily connect this with Telegram bot?"}
{"id": "mmksx4u", "type": "comment", "parent_id": "t1_mmkcdo9", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmksx4u/", "author": "Bird_Idea", "created_utc": 1744385169, "score": 1, "content": "One more question. I'm building a real estate tool that tracks new postings and the most important part is to be the first one to see it once it's posted. So basically I have to track each page for certain changes. Can I do this with your tool and will I also be able to bypass being flagged for botting?"}
{"id": "mm986vk", "type": "comment", "parent_id": "t1_mm3dply", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mm986vk/", "author": "bmrheijligers", "created_utc": 1744223553, "score": 1, "content": "My pleasure"}
{"id": "mmkte1d", "type": "comment", "parent_id": "t1_mmkewsy", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmkte1d/", "author": "0xReaper", "created_utc": 1744385306, "score": 1, "content": "Yeah, why not"}
{"id": "mmku6zo", "type": "comment", "parent_id": "t1_mmksx4u", "permalink": "https://www.reddit.com/r/webscraping/comments/1jugawo/scrapling_v0299_website_effortless_web_scraping/mmku6zo/", "author": "0xReaper", "created_utc": 1744385541, "score": 2, "content": "You might need more automation than what the library provides to make the bot browse the website like a normal human, so maybe use raw Camoufox/Playwright instead if the website protection is a bit advanced and watches users' behavior. Otherwise, you can keep requesting the page every 5 minutes or so, check the current results, compare them, etc."}
{"id": "1n7ovr1", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/", "author": "_do_you_think", "created_utc": 1756928762, "score": 156, "title": "Browser fingerprinting\u2026", "content": "Calling anybody with a large and complex scraping setup\u2026 We have scrapers, ordinary ones, browser automation\u2026 we use proxies for location based blocking, residential proxies for data centre blockers, we rotate the user agent, we have some third party unblockers too. But often, we still get captchas, and CloudFlare can get in the way too. I heard about browser fingerprinting - a system where machine learning can identify your browsing behaviour and profile as robotic, and then block your IP. Has anybody got any advice about what else we can do to avoid being \u2018identified\u2019 while scraping? Also, I heard about something called phone farms (see image), as a means of scraping\u2026 anybody using that?"}
{"id": "nc9954e", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nc9954e/", "author": "Quentin_Quarantineo", "created_utc": 1756931511, "score": 44, "content": "For my scraping targets, device fingerprinting is key. Residential proxies, user agent headers(one small component of device fingerprint), are not enough. It really depends on which sites you are targeting. Different high value targets have different sophisticated anti scraping measures in place that need to be handled accordingly. The objectives you need to achieve once on site are important as well. Do you need to reverse engineer cookies to show data that otherwise won\u2019t be revealed? If you are running a complex set of browser actions, are you interacting with browser components using JavaScript, or are you doing so with some other method? Maybe headless isn\u2019t feasible and you need to use real system level keyboard and mouse inputs that mimic real human input patterns, ie random delays, dwell, jitter, curved mouse paths, etc. If you\u2019re in that deep, using a mobile device or devices may be the best option as it is less complex to implement complex user interactions, not to mention much less UI to deal with. If you are using AI to guide your user interactions through a vision API, screenshots will be much cheaper as well. I\u2019ve never used a mobile device bot farm before, but presumably they allow you to use your own proxy and whatnot. I would be somewhat weary of using devices that have been fingerprinted and used heavily for scraping everything under the sun and moon already, but presumably, these services would offer custom device fingerprinting solutions."}
{"id": "nc9emyh", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nc9emyh/", "author": "UsefulIce9600", "created_utc": 1756933074, "score": 11, "content": "u/pixel-counter-bot"}
{"id": "ncrrdnb", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncrrdnb/", "author": "404mesh", "created_utc": 1757182382, "score": 3, "content": "Something else you want to take into consideration is TLS cipher suites and other network level identifiers. Every packet has fingerprinting vectors, for your TCP/IP stack these are headers like TTL, Hop Limit, ToS (type of service), MSS (max segment size), and Window Size. These things all contribute to your fingerprint because OSs have prebaked values for these headers (TTL on Linux = 64 on Windows = 128). If the headers don\u2019t match with this, a server can identify your traffic. If you\u2019re editing HTTPS headers and not packet headers, you\u2019re being fingerprinted. For your TLS, if you\u2019re using a proxy you want to make sure you\u2019re doing either ephemeral key exchange or a secure (preferably on 127.0.0.1) MITM on your machine. TLS Cipher Suites and other identifiers during the SYN-ACK handshake allow for a server to identify you at the get go. You also want to make sure you\u2019re dealing with JS fingerprinting tools that web pages load, directly asking your browser for identifiers. These will run at load and, on some websites, at intervals as you remain on the page."}
{"id": "ncawljg", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncawljg/", "author": "Kooky-Principle5021", "created_utc": 1756951272, "score": 5, "content": "You need to be fake a lot of things, including installed fonts."}
{"id": "ncarhh4", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncarhh4/", "author": "martianwombat", "created_utc": 1756949428, "score": 2, "content": "Bro You're cooked"}
{"id": "nc962mo", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nc962mo/", "author": "Pigik83", "created_utc": 1756930642, "score": 2, "content": "For browser fingerprinting, just use an antidetect browser (camoufox or commercial ones)"}
{"id": "ncap7sr", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncap7sr/", "author": "HermaeusMora0", "created_utc": 1756948633, "score": 2, "content": "If you want to go \"complex and huge\" browser automation is definitely not the go to. Every website can be reverse engineered. If you have the money, you can get any bot protection \"bypassed\" for less than 5 figures. You CAN generate your own fingerprints, but that's unheard of, and rarely anyone does so. The \"industry-standard\" is creating a website and getting visitors' fingerprints this way. There's not really an industry on CAPTCHA solving or anti-bot bypassing, If you want to scale, learn reverse engineering. Learn JS obfuscation methods, WASM, JavaScript Virtual Machines (Kasada's VM is heavily documented on GitHub), sandboxing, etc. As per the phone farms, they're probably the stupidest thing you can do. It's definitely cheaper to hire a reverse engineer than to buy a dozen phones."}
{"id": "ncd42t7", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncd42t7/", "author": "WadieXkiller", "created_utc": 1756989728, "score": 1, "content": "Scary ass resident evil picture"}
{"id": "ncey913", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncey913/", "author": "None", "created_utc": 1757009057, "score": 1, "content": "[removed]"}
{"id": "ncgpfma", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncgpfma/", "author": "Valuable-Map6573", "created_utc": 1757028607, "score": 1, "content": "There are so called Anti-Detect-Browsers which suite this specific purpose. There are so many ways to fingerprint a device and having a browser with spoofed profiles is one of the safest way to get around them. Only downside is that it requires more resources to scrape using let's say a headless browser compared to direct http requests. More proxy bandwith and hardware power. That being said there are some clever ways to get around most antibot protections without having to use browsers. TLS fingerprinting for example but there is no one fit all solution."}
{"id": "ncgqjpw", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncgqjpw/", "author": "Valuable-Map6573", "created_utc": 1757028995, "score": 1, "content": "There are tools specifically designed to mitigate fingerprinting for real mobile hardware. Android has many \"Cloning\" apps which work quite similar to antidetect browsers. Creating multiple profiles with unique IDs and even proxies. In general most websites and services give mobile devices higher trust ratings than desktop devices."}
{"id": "nc951vd", "type": "comment", "parent_id": "t3_1n7ovr1", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nc951vd/", "author": "SnooSprouts3872", "created_utc": 1756930350, "score": 0, "content": "Nice setup"}
{"id": "ncas09f", "type": "comment", "parent_id": "t1_nc9954e", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncas09f/", "author": "cheezpnts", "created_utc": 1756949614, "score": 25, "content": "^^ this dude browses\u2026hard"}
{"id": "ncb0h1h", "type": "comment", "parent_id": "t1_nc9954e", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncb0h1h/", "author": "No_Statistician7685", "created_utc": 1756952685, "score": 4, "content": "When you talk about the vision API, is that to instead OCR the page instead of parsing the results?"}
{"id": "nczsk3a", "type": "comment", "parent_id": "t1_nc9954e", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nczsk3a/", "author": "johnkapolos", "created_utc": 1757287294, "score": 1, "content": "Quentin is obviously working hard to scrape all the footers he can find"}
{"id": "nc9eovi", "type": "comment", "parent_id": "t1_nc9emyh", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nc9eovi/", "author": "pixel-counter-bot", "created_utc": 1756933090, "score": 9, "content": "The image in this post has 50,176(224\u00d7224) pixels! ^(I am a bot. This action was performed automatically.)"}
{"id": "ncbkbwd", "type": "comment", "parent_id": "t1_ncawljg", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbkbwd/", "author": "_do_you_think", "created_utc": 1756961114, "score": 1, "content": "No idea what you mean."}
{"id": "nckni0h", "type": "comment", "parent_id": "t1_ncarhh4", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nckni0h/", "author": "Asvyr", "created_utc": 1757086650, "score": 2, "content": "JA3 is easy to bypass. JA4H and the whole JA4+ suite in general is a bit more tricky but still doable. You just need lower level control. Go has nice libraries you can build on."}
{"id": "ncbk70k", "type": "comment", "parent_id": "t1_ncarhh4", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbk70k/", "author": "_do_you_think", "created_utc": 1756961048, "score": 0, "content": "This is mostly a problem for plain headless http request scraping\u2026 browser automation will match the TLS signature of a real browser."}
{"id": "nc9roet", "type": "comment", "parent_id": "t1_nc962mo", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nc9roet/", "author": "arshad_ali1999", "created_utc": 1756937092, "score": 0, "content": "I think TOR also does the same"}
{"id": "ncau56y", "type": "comment", "parent_id": "t1_ncap7sr", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncau56y/", "author": "Patient-Bit-331", "created_utc": 1756950390, "score": 2, "content": "not at all, setup devices farm may be not cheaper than hire a RE but, it stable and hardly modify for every platforms, every systems"}
{"id": "ncbirwr", "type": "comment", "parent_id": "t1_ncap7sr", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbirwr/", "author": "_do_you_think", "created_utc": 1756960366, "score": 1, "content": "Reverse engineering the website is probably the best way to go. Is this something you have done yourself? We have managed to reverse engineer a few simple websites, but only by exploiting unprotected endpoints. We never attempted to get user session keys for making authenticated requests. What about reversing the JS obfuscation? Any tools you would recommend?"}
{"id": "ncf7nfv", "type": "comment", "parent_id": "t1_ncey913", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncf7nfv/", "author": "webscraping-ModTeam", "created_utc": 1757011825, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "ncb0iuu", "type": "comment", "parent_id": "t1_ncas09f", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncb0iuu/", "author": "StoicTexts", "created_utc": 1756952704, "score": 3, "content": "Facts"}
{"id": "ncb2og3", "type": "comment", "parent_id": "t1_ncb0h1h", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncb2og3/", "author": "Quentin_Quarantineo", "created_utc": 1756953527, "score": 3, "content": "Essentially yes. I use OCR for identifying UI elements and specific text attributes, then interact with them using the coordinates of those OCR items. No vision API is necessary for this, but I do use vision API along with OpenAI or Anthropic\u2019s computer use agent as a fallback in case the end result isn\u2019t what is expected by the scraper orchestrator agent. I also use vision API to triage scraped images extracted from each scraping run as part of a larger data collection workflow."}
{"id": "ncc0sud", "type": "comment", "parent_id": "t1_nc9eovi", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncc0sud/", "author": "electricsheep2013", "created_utc": 1756970039, "score": 5, "content": "Good bot! Now enhance!"}
{"id": "ncux9sx", "type": "comment", "parent_id": "t1_nckni0h", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncux9sx/", "author": "martianwombat", "created_utc": 1757223529, "score": 1, "content": "Aura!"}
{"id": "ncgrm40", "type": "comment", "parent_id": "t1_nc9roet", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncgrm40/", "author": "Valuable-Map6573", "created_utc": 1757029371, "score": 6, "content": "Lol. Tor is like dressing up as a suicide bomber when trying to sneak through airport cusomts"}
{"id": "ncd0v5b", "type": "comment", "parent_id": "t1_ncau56y", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncd0v5b/", "author": "HermaeusMora0", "created_utc": 1756988585, "score": 3, "content": "Sure, maintainability is hard, but every single \"big player\" is reversing, not using phone farms. Protections rarely change, I'm still using the same solvers I made years ago, by just changing a few hardcoded values. Datadome hasn't been updated in ages. FunCaptcha barely updates, and it's generally very easy to patch. In general, if you have the skills, reverse engineering is the ONLY way to go. Hundreds of times faster and way more scalable. Want to scale your farm? Buy another dozen phones. If you want to scale a reversed solution, you pay a $1K dedicated server that's equivalent to the requests of hundreds of phones."}
{"id": "ncc8ilj", "type": "comment", "parent_id": "t1_ncbirwr", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncc8ilj/", "author": "None", "created_utc": 1756974701, "score": 1, "content": "[removed]"}
{"id": "nccsl9c", "type": "comment", "parent_id": "t1_ncb2og3", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nccsl9c/", "author": "Atomic1221", "created_utc": 1756985353, "score": 5, "content": "At a large enough scale the easier solution with mobile phones is in fact more complex than just doing seleniumbase CDP mode in k8s I\u2019d say the mobile phones is a good medium scale option until the tools get easier for implementing large scale solutions as the pure software solution will always be lower cost. The problem is you often don\u2019t know what it is that\u2019s triggering the bot detection. Is it the typing? Is the mouse on the page? Is it clicking submit? Multiple things? All you get is a fail (if they aren\u2019t poisoning the well too in which case buy yourself a case of whiskey to get through it). I\u2019ve even seen pages that measure the latency of time stamped browser actions vs download latency to detect how far away your server is from the proxy. Sticking the data center IP near the local proxy IP worked. That bugger took me a month to figure out. I\u2019d still choose standard methods for prototyping solutions. Maybe there\u2019s something about rooted phones + custom roms that lets you operate on OS level instead of the browser level. If so my opinion might change"}
{"id": "ncbl26q", "type": "comment", "parent_id": "t1_ncb2og3", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbl26q/", "author": "_do_you_think", "created_utc": 1756961471, "score": 3, "content": "Isn\u2019t that really brittle? I built one of these before and it worked half of the time. I didn\u2019t use AI though. Maybe identifying UI elements would be easier with AI. Expensive though\u2026 I suppose some intelligent caching of element coordinates could help."}
{"id": "ncis1v9", "type": "comment", "parent_id": "t1_ncd0v5b", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncis1v9/", "author": "hackbyown", "created_utc": 1757059916, "score": 1, "content": "Can you please describe how you are able to bypass datadome at api level or direct html pages loads those are behind datadome"}
{"id": "nck30zs", "type": "comment", "parent_id": "t1_ncd0v5b", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nck30zs/", "author": "None", "created_utc": 1757080664, "score": 1, "content": "[removed]"}
{"id": "nccl28l", "type": "comment", "parent_id": "t1_ncc8ilj", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nccl28l/", "author": "webscraping-ModTeam", "created_utc": 1756981926, "score": 1, "content": "Please review the sub rules"}
{"id": "nceqkr2", "type": "comment", "parent_id": "t1_nccsl9c", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/nceqkr2/", "author": "Quentin_Quarantineo", "created_utc": 1757006922, "score": 1, "content": "Thats wild! I'll definitely remember this for when I inevitably run into this issue."}
{"id": "ncbmoat", "type": "comment", "parent_id": "t1_ncbl26q", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbmoat/", "author": "Quentin_Quarantineo", "created_utc": 1756962270, "score": 3, "content": "Valid question. The answer is yes and no. I don't store or cache the element coordinates, instead, they are generated on the fly in realtime during each scraping run, so no missed element interactions due to assumed locations. But its a little more robust than it sounds, as I use key anchor reference elements that I know will always be there, in order to locate the target elements, within a predefined search area that is defined in relation to the anchor element. Success rate is essentially 100% for repeatable workflows where you can define expected anchors, while using the reference region for elements which you do not know the contents/name of beforehand. This of course is not robust enough to be impervious to major UI changes. That's where the CUA backup comes in, allowing us to quickly respond to major UI updates on the scraping target side without any down time, as the CUA system is able to achieve close to 99% success rate for our use case."}
{"id": "ncj9i3t", "type": "comment", "parent_id": "t1_ncis1v9", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncj9i3t/", "author": "HermaeusMora0", "created_utc": 1757069721, "score": 3, "content": "Datadome generates a \"pass by cookie\". Their scripts haven't been updated in years, and deobfuscator and payload decryptions are public on Github. What you can do to generate a passing payload is: 1. Generate the fingerprint value yourself, on top of my head, Datadome has canvas, audio fingerprinting and a bunch of others. You can *mostly* generate those values, but some are more difficult to generate a valid one than others. I personally don't do that. 2. Make a website and a script to collect the necessary fingerprints of the visitors of the website. That's what most of the industry does because that's the easiest way to get high-quality fingerprints. Fingerprints can usually be reused for hundreds/thousands of requests depending on the provider/settings. Look things up on GitHub (Datadome Interstitial has a public solver, for example) and you'll find things. Maybe you won't find a straight-forward solver, but I've worked with Datadome by just finding an old, non-working solver and patching it."}
{"id": "ncnfioh", "type": "comment", "parent_id": "t1_nck30zs", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncnfioh/", "author": "webscraping-ModTeam", "created_utc": 1757117707, "score": 1, "content": "Please review the sub rules"}
{"id": "ncbnwuf", "type": "comment", "parent_id": "t1_ncbmoat", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbnwuf/", "author": "_do_you_think", "created_utc": 1756962909, "score": 2, "content": "One issue I have was with setting a threshold confidence for UI element matches. Often, because elements can change with content, screen width, my matches could be less than 100% confidence. Do you use minimum thresholds to identify when to execute your back up method? Or do you only calculated positions by using UI elements that display little to no variation from page to page, thus a binary solution?"}
{"id": "ncjb1r0", "type": "comment", "parent_id": "t1_ncj9i3t", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncjb1r0/", "author": "hackbyown", "created_utc": 1757070425, "score": 1, "content": "Thanks for the detailed explanation brother."}
{"id": "ncbtne3", "type": "comment", "parent_id": "t1_ncbnwuf", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbtne3/", "author": "Quentin_Quarantineo", "created_utc": 1756965966, "score": 2, "content": "I use a min threshold for ocr items, but with ocr configured for accuracy as opposed to speed, it\u2019s probably 99-100% accurate. If you use ocr, you shouldn\u2019t have those issues, especially if you can rely heavily on text to identify your elements. Even if you don\u2019t know what text the element that you intend to interact with will contain, the previously mentioned method should be able to reliably interact with those elements. If you are using selectors, xpath, css, etc, your system will be much more prone to breakage or failures. I have a somewhat limited understand of how exactly ocr works, but I believe ocr is deterministic, so dialing in your configuration should allow you to produce robust results. I don\u2019t necessarily use thresholds to trigger backup methods. Instead, I use small targeted screenshots or copying text specific to that task, to clipboard, then verifying with an LLM that the sequence of actions executed by the ocr based system resulted in the expected behavior in browser. If it doesn\u2019t pass review, it triggers backup CUA execution."}
{"id": "ncbx60c", "type": "comment", "parent_id": "t1_ncbtne3", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbx60c/", "author": "chrislbrown84", "created_utc": 1756967941, "score": 2, "content": "How are you keeping costs of vision down in order to deliver this economically at scale?"}
{"id": "ncbzj8r", "type": "comment", "parent_id": "t1_ncbx60c", "permalink": "https://www.reddit.com/r/webscraping/comments/1n7ovr1/browser_fingerprinting/ncbzj8r/", "author": "Quentin_Quarantineo", "created_utc": 1756969296, "score": 3, "content": "We run images at 256x256 and only costs us about $0.63/1000 images or something outrageously low. this equates to only around $5 a day for our current workload. It's a very small fraction of our overall costs. I believe Gemini is like half the cost as well. I haven't compared the performance but after our soft release and subsequent scaling phase, we will probably switch to gemini if the performance is comparable at a lower cost."}
{"id": "1gqiuk8", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/", "author": "0xReaper", "created_utc": 1731520211, "score": 140, "title": "Scrapling - Undetectable, Lightning-Fast, and Adaptive Web Scraping", "content": "Hello everyone, I have released version 0.2 of Scrapling with a lot of changes and am awaiting your feedback! New features include stuff like: * Introducing the `Fetchers` feature with 3 new main types to make Scrapling fetch pages for you with a LOT of options! * Added the completely new `find_all`/`find` methods to find elements easily on the page with dark magic! * Added the methods `filter` and `search` to the `Adaptors` class for easier bulk operations on `Adaptor` object groups. * Added methods `css_first` and `xpath_first` methods for easier usage. * Added the new class type `TextHandlers` which is used for bulk operations on `TextHandler` objects like the `Adaptors` class. * Added `generate_full_css_selector` , and `generate_full_xpath_selector` methods. And this is just the tip of the iceberg, check out the completely new page from here: ["}
{"id": "lwyd2s9", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwyd2s9/", "author": "anxman", "created_utc": 1731521726, "score": 13, "content": "Scrapling is mega fast"}
{"id": "lwylgy2", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwylgy2/", "author": "errdayimshuffln", "created_utc": 1731524243, "score": 2, "content": "I will try this out in my next python ws project. Right now I'm working on a react project that uses webscraping. Do you know of a javascript/typescript repo that is similar to yours? Open source that is.."}
{"id": "lxjt6qb", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxjt6qb/", "author": "soulhackerwang", "created_utc": 1731819916, "score": 2, "content": "awesome work"}
{"id": "lwya3ho", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwya3ho/", "author": "None", "created_utc": 1731520821, "score": 1, "content": "[removed]"}
{"id": "lwyxl2y", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwyxl2y/", "author": "Djkid4lyfe", "created_utc": 1731527908, "score": 1, "content": "Can this bypass cloudflare capachas and save cookies then use save cookies to do requests and save jsons of the page source?"}
{"id": "lwzosbu", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzosbu/", "author": "anxman", "created_utc": 1731536237, "score": 1, "content": "How does one pass a proxy?"}
{"id": "lwzv0kc", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzv0kc/", "author": "MrGreenyz", "created_utc": 1731538259, "score": 1, "content": "How it handle infinite scroll and can it manage logins?"}
{"id": "lx3con3", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3con3/", "author": "AdmirableCare6043", "created_utc": 1731595459, "score": 1, "content": "Thanks for sharing ! How could I send keys, click and actions like that ? It seems I can't use basic playwright actions"}
{"id": "lx3g3fe", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3g3fe/", "author": "mattyboombalatti", "created_utc": 1731596610, "score": 1, "content": "Will I still need to use a residential proxy, or can I use an ISP proxy? Basically, is the anti-bot stuff sophisticated enough where I can use an ISP proxy (and save a ton of money)?"}
{"id": "lx3hyx2", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3hyx2/", "author": "VFansss", "created_utc": 1731597232, "score": 1, "content": "Maybe I'm a fool (never truly done web scraping) so sorry for this question but: core differences between Scrapling and Beautiful Soup?"}
{"id": "lx5niku", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx5niku/", "author": "None", "created_utc": 1731621082, "score": 1, "content": "Does it require residential proxy rotating IP?"}
{"id": "lx892it", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx892it/", "author": "Healthy-Educator-289", "created_utc": 1731655870, "score": 1, "content": "How do I catch network requests with this"}
{"id": "lx8fks1", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx8fks1/", "author": "Key_Extension_6003", "created_utc": 1731659637, "score": 1, "content": "!remindme 5 days"}
{"id": "lxajk0g", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxajk0g/", "author": "0xReaper", "created_utc": 1731690516, "score": 1, "content": "Just released version 0.2.1 which adds proxy support, makes it easier and adds other stuff"}
{"id": "lxav06l", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxav06l/", "author": "kadirilgin", "created_utc": 1731693928, "score": 1, "content": "Could you please also compare with curl cffi? Thank you. ["}
{"id": "lxdd11p", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxdd11p/", "author": "blacktrepreneur", "created_utc": 1731723911, "score": 1, "content": "Are there any ts libraries like this?"}
{"id": "lxik4nu", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxik4nu/", "author": "anxman", "created_utc": 1731801839, "score": 1, "content": "Requesting Playwright Async API support. Unable to integrate this into my fastapi application :("}
{"id": "lxqn0xu", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxqn0xu/", "author": "0xReaper", "created_utc": 1731928289, "score": 1, "content": "I forgot to say but I released v0.2.2 too afterwards to fix a bug and add an easier logic for importing fetchers. Checkout the releases page for info :)"}
{"id": "ly35ltp", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/ly35ltp/", "author": "0xReaper", "created_utc": 1732104719, "score": 1, "content": "Version 0.2.4 just released fixing some bugs, check it out!"}
{"id": "m98e275", "type": "comment", "parent_id": "t3_1gqiuk8", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/m98e275/", "author": "vroemboem", "created_utc": 1737876739, "score": 1, "content": "Looks great. Any recommendations on where to host it?"}
{"id": "lwydn2c", "type": "comment", "parent_id": "t1_lwyd2s9", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwydn2c/", "author": "0xReaper", "created_utc": 1731521896, "score": 1, "content": "Thanks mate, glad you like it!"}
{"id": "lwyxhro", "type": "comment", "parent_id": "t1_lwylgy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwyxhro/", "author": "Djkid4lyfe", "created_utc": 1731527880, "score": 1, "content": "What project?"}
{"id": "lwyaz1u", "type": "comment", "parent_id": "t1_lwya3ho", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwyaz1u/", "author": "None", "created_utc": 1731521086, "score": 6, "content": "[deleted]"}
{"id": "lwzbswq", "type": "comment", "parent_id": "t1_lwyxl2y", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzbswq/", "author": "0xReaper", "created_utc": 1731532260, "score": 2, "content": "Yes it can do all of that but can\u2019t bypass the interactive captcha version, as per my knowledge nothing can click it right now other than paid AI proxies shit"}
{"id": "lwzp8ir", "type": "comment", "parent_id": "t1_lwzosbu", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzp8ir/", "author": "0xReaper", "created_utc": 1731536384, "score": 3, "content": "For the normal \u2018Fetcher\u2019 class you pass it like how you pass it to other browser-based fetchers are still unsupported but will be added in next update."}
{"id": "lxaj2ho", "type": "comment", "parent_id": "t1_lwzosbu", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxaj2ho/", "author": "0xReaper", "created_utc": 1731690370, "score": 1, "content": "Just released version 0.2.1 which adds proxy support and other stuff"}
{"id": "lx0k8dz", "type": "comment", "parent_id": "t1_lwzv0kc", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx0k8dz/", "author": "0xReaper", "created_utc": 1731547088, "score": 1, "content": "Yes it can, it really depends on the website but it\u2019s equipped with a lot of options that allows it to handle many scenarios like for example there\u2019s an argument called \u2018page_action\u2019 you can use it to do automation on the page before returning the response like for your example you can scroll till the end of the page."}
{"id": "lx3douf", "type": "comment", "parent_id": "t1_lx3con3", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3douf/", "author": "0xReaper", "created_utc": 1731595805, "score": 2, "content": "No, you can, check this example out: ```python def scroll_page(page): page.mouse.wheel(10, 0) page.mouse.move(100, 400) page.mouse.up() return page _ = fetcher.fetch(self.html_url, page_action=scroll_page) # Where fetcher can by StealthyFetcher or PlayWrightFetcher class ``` The page passed to the function that you pass to `page_action` is the same page object created by Playwright so you can do basically anything but you have to return `page` again at the end of the function."}
{"id": "lx3n4en", "type": "comment", "parent_id": "t1_lx3g3fe", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3n4en/", "author": "0xReaper", "created_utc": 1731598881, "score": 2, "content": "Most of the time yeah but for advanced protections when you look like a real person but behave strangely like a bot, the protections start looking for weak signals like your IP is it residential or a data center IP? A real person will have a residential IP most likely. Generally speaking, if your bot behaves like a bot, at some point, it won't matter what you are using in web scraping. With that said, currently for the normal \u2018Fetcher\u2019 class you can pass proxies but other browser-based fetchers are still unsupported but will be added in the next update."}
{"id": "lx3m02i", "type": "comment", "parent_id": "t1_lx3hyx2", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3m02i/", "author": "0xReaper", "created_utc": 1731598529, "score": 1, "content": "Scrapling can fetch the website for you, not only parse it like BeautifulSoup. When it comes to parsing differences then Scrapling is better at everything BeautifulSoup does while being up to 600x faster and having new features that BeautifulSoup and most libraries don't have."}
{"id": "lx6nl08", "type": "comment", "parent_id": "t1_lx5niku", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx6nl08/", "author": "0xReaper", "created_utc": 1731632764, "score": 1, "content": "Please check my other replies"}
{"id": "lx8fneg", "type": "comment", "parent_id": "t1_lx8fks1", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx8fneg/", "author": "RemindMeBot", "created_utc": 1731659679, "score": 1, "content": "I will be messaging you in 5 days on [**2024-11-20 08:33:57 UTC**]( to remind you of [**this link**]( [**1 OTHERS CLICKED THIS LINK**]( to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)]( ***** |[^(Info)]( Reminders)]( |-|-|-|-|"}
{"id": "lxqnb2q", "type": "comment", "parent_id": "t1_lxav06l", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxqnb2q/", "author": "0xReaper", "created_utc": 1731928458, "score": 1, "content": "The comparisons are on the parsing speed bro not the requests speed"}
{"id": "lxqn6yr", "type": "comment", "parent_id": "t1_lxik4nu", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lxqn6yr/", "author": "0xReaper", "created_utc": 1731928388, "score": 1, "content": "It is hard to add as I need to make the parser support async too. I will try to add it with version 0.3"}
{"id": "lwzdv4b", "type": "comment", "parent_id": "t1_lwyxhro", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzdv4b/", "author": "errdayimshuffln", "created_utc": 1731532874, "score": 1, "content": "A nextjs project that uses selenium server-side to scrape. It's slow and costly and I'm in thenlookout for another option."}
{"id": "lwzdfie", "type": "comment", "parent_id": "t1_lwzbswq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzdfie/", "author": "webscraping-ModTeam", "created_utc": 1731532746, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mcb9biv", "type": "comment", "parent_id": "t1_lx0k8dz", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/mcb9biv/", "author": "anakaine", "created_utc": 1739333497, "score": 1, "content": "Is there an example regarding how to handle logins specifically?"}
{"id": "lx5w38x", "type": "comment", "parent_id": "t1_lx3douf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx5w38x/", "author": "AdmirableCare6043", "created_utc": 1731623728, "score": 1, "content": "I keep having *Response.body: Protocol error (Network.getResponseBody): No resource with given identifier found*, do I need something more ?"}
{"id": "lx3r2j9", "type": "comment", "parent_id": "t1_lx3n4en", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3r2j9/", "author": "mattyboombalatti", "created_utc": 1731600119, "score": 1, "content": "Cool - look forward to playing around with this a bit."}
{"id": "lx3wazu", "type": "comment", "parent_id": "t1_lx3m02i", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lx3wazu/", "author": "VFansss", "created_utc": 1731601762, "score": 2, "content": "Oh, yes I can agree with that. Usualli with BS I see people that just does python fetch, but for sure Scrapling is able to provide a more powerful page retrieval. I'm going to build a webscraper (my first!) that for sure doesn't need Cloudflare bypass or other fancy things, but I will take Scrapling a chance. Regardless, keep up the good work and thanks for the good answer!"}
{"id": "lwze15g", "type": "comment", "parent_id": "t1_lwzdv4b", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwze15g/", "author": "Djkid4lyfe", "created_utc": 1731532925, "score": 2, "content": "Scrape with selenium for cookies and then use the cookies and headers to do requests aiohttp ot"}
{"id": "ly35gur", "type": "comment", "parent_id": "t1_lx5w38x", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/ly35gur/", "author": "0xReaper", "created_utc": 1732104653, "score": 1, "content": "If this is the issue caused while using \u2018network_idle\u2019 argument then it just got fixed with 0.2.4. Otherwise, please open an issue with the details"}
{"id": "lwzemg5", "type": "comment", "parent_id": "t1_lwze15g", "permalink": "https://www.reddit.com/r/webscraping/comments/1gqiuk8/scrapling_undetectable_lightningfast_and_adaptive/lwzemg5/", "author": "errdayimshuffln", "created_utc": 1731533102, "score": 1, "content": "I tried that but the websites that I'm scraping are big websites and still manage to interfere with the scraping. I mean it works but didn't work for one of the sites reliably. Either that or the headers are wrong or some other issue. I also found some internal api's and tried using those but again, these sites are pretty smart. Fyi, the sites are all the slmajor social media. I can't even scrape reddit without using selenium. Like I tried using the json endpoints and everything."}
{"id": "1jg94ha", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/", "author": "major_bluebird_22", "created_utc": 1742532887, "score": 135, "title": "How does a small team scrape data daily from 150k+ unique websites?", "content": "Was recently pitched on a real estate data platform that provides quite a large amount of comprehensive data on just about every apartment community in the country (pricing, unit mix, size, concessions + much more) with data refreshing daily. Their primary source for the data is the individual apartment communities websites', of which there are over 150k. Since these website are structured so differently (some Javascript heavy some not) I was just curious as to how a small team (less then twenty people working at the company including non-development folks) achieves this. How is this possible and what would they be using to do this? Selenium, scrappy, playwright? I work on data scraping as a hobby and do not understand how you could be consistently scraping that many websites - would it not require unique scripts for each property? Personally I am used to scraping pricing information from the typical, highly structured, apartment listing websites - occasionally their structure changes and I have to update the scripts. Have used beautifulsoup in the past and now using selenium, have had success with both. Any context as to how they may be achieving this would be awesome. Thanks!"}
{"id": "miy6k86", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miy6k86/", "author": "RedditCommenter38", "created_utc": 1742551680, "score": 15, "content": "Just my guess but although there is 150k+ different websites, most apartment websites are using 1 of maybe 7 or 8 highly popular \u201capartment listing\u201d web platforms, such as Rent Cafe, Entrata, etc. So they may built 7-8 different Python scripts as \u201ctemplates\u201d initially. So let\u2019s say 30% use rent Cafe, all of their websites are going to be structured pretty similarly if not identically, as those types of platform have little control over custom html/css selectors."}
{"id": "mixhg8r", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mixhg8r/", "author": "themasterofbation", "created_utc": 1742536080, "score": 33, "content": "Interested if someone can chime in, because I feel like there's a few possible answers here, but each has a reason why I think it's not the case. Answer 1: They are lying Reason: 150k unique websites is a TON. Just finding and validating 150k apartment complex websites would take ages. Some websites won't have their pricing on the site. Even though they will be failry static, something will break daily. Answer 2: They are taking the full HTML and using a \"locally\" hosted LLM to extract the specific data from that. Reason: This could be it. The sites will be static, won't change much. Still, finding the valid URLs of 150k apartment complex pricing tables would be tough. 6000 sites analysed per hour, every day. 100 per minute. At 150k, there's no way they built a specific scraper for each site. Using LLMs will give you bad outputs here and there though... Answer 3: They have an army of webscrapers maintaining the code in Pakistan Reason: Would be funny if that was the case OP: Can you share the URL of the data platform (feel free to DM). I'd like to check what they are actually promising"}
{"id": "mixn59n", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mixn59n/", "author": "das_war_ein_Befehl", "created_utc": 1742539484, "score": 8, "content": "They\u2019re likely combining cloud-based distributed scraping (e.g., Scrapy/Playwright/Selenium), AI-driven parsing (like LLM-based data extraction from HTML), proxy rotation, and modular code with intelligent error handling. Automating scraper creation via machine learning or dynamic templates would greatly reduce manual effort at this scale. It\u2019s a huge pain in the ass, but data platforms are very profitable if you\u2019re in a good niche, so I definitely can see it being worthwhile."}
{"id": "miy7zot", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miy7zot/", "author": "Careless-Party-5952", "created_utc": 1742552489, "score": 2, "content": "That is highly doubtful for me. 150k websites in 1 week it is beyond crazy. I really do not believe that this is possible to be done in such a short period."}
{"id": "mizp14z", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mizp14z/", "author": "alvincho", "created_utc": 1742571922, "score": 2, "content": "I think it\u2019s possible since the data is uniform and highly predictable format. Assuming all web pages can be refreshed in one day, says 150k x 10 pages = 1.5 millions pages in text or html. Use NER or regex to detect some keywords then try to identify more. Of course most of them can\u2019t be done in the automated phase but you can have a program smart enough to solve 60-70%. The others take time, not in one day."}
{"id": "miy678e", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miy678e/", "author": "AlexTakeru", "created_utc": 1742551472, "score": 1, "content": "Are you sure they are actually scraping websites in the traditional way? In our local market, real estate developers themselves provide feeds with the necessary information to platforms\u2014price, apartment parameters such as the number of bedrooms, bathrooms, square footage, price per square meter, etc. Since real estate developers get traffic from these platforms, they are interested in providing such feeds."}
{"id": "miz7m3m", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miz7m3m/", "author": "None", "created_utc": 1742566764, "score": 1, "content": "[removed]"}
{"id": "mizw08q", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mizw08q/", "author": "None", "created_utc": 1742573940, "score": 1, "content": "[removed]"}
{"id": "mizx3mk", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mizx3mk/", "author": "AdministrativeHost15", "created_utc": 1742574263, "score": 1, "content": "Load the page text into a RAG then ask the LLM to return the data of interest in JSON format. Then parse the JSON and insert it into your db."}
{"id": "mj071t5", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj071t5/", "author": "TechMaven-Geospatial", "created_utc": 1742577131, "score": 1, "content": "This is not something that's updated daily this is probably like every 6 months and updated pricing. And I guarantee they're probably tapping into some API that are already exist from apartments.com or realtor.com or one of these sites"}
{"id": "mj40sfy", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj40sfy/", "author": "Hot-Somewhere-980", "created_utc": 1742627778, "score": 1, "content": "Maybe the 150k websites use the same system / CMS. Then they only have to build a scraper one time and just scrape through all of them."}
{"id": "mj6ed7b", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj6ed7b/", "author": "Positive-Motor-5275", "created_utc": 1742665105, "score": 1, "content": "Can be done with some proxy + cheap llm or self hosted"}
{"id": "mj8b8i3", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj8b8i3/", "author": "thisguytucks", "created_utc": 1742687803, "score": 1, "content": "They are not lying, its quite possible and I am personally doing it. Not at that scale but I am scraping about 10000+ websites in a day using N8N and OpenAI. I can scale it up to 100k+ in a day if needed, all it will take is a beefier VPS."}
{"id": "mjjy3ra", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjjy3ra/", "author": "blacktrepreneur", "created_utc": 1742853401, "score": 1, "content": "I work in CRE. Would love to know this platform. Maybe they are scraping apartments.com. Or they found a way to get access to RealPage\u2019s apartment feed. Most apartment websites use RealPage on their website which does daily updates based on the supply and demand (it\u2019s the algorithmic system l they are being sued over). Or they\u2019re just pulling data from the rent cafe api"}
{"id": "mjpds0u", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjpds0u/", "author": "None", "created_utc": 1742929501, "score": 1, "content": "[removed]"}
{"id": "mjr1ks6", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjr1ks6/", "author": "ThatHappenedOneTime", "created_utc": 1742947670, "score": 1, "content": "Are there even 150,000+ unique websites about apartment communities in your country? Genuine question; it just sounds excessive."}
{"id": "mjabhzl", "type": "comment", "parent_id": "t3_1jg94ha", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjabhzl/", "author": "None", "created_utc": 1742722990, "score": 0, "content": "that could be doing what i do\u2026 i\u2019ve kind of perfected the art of scraping using ai for extraction. very cheaply and accurately, too"}
{"id": "miycwfm", "type": "comment", "parent_id": "t1_miy6k86", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miycwfm/", "author": "fabier", "created_utc": 1742555079, "score": 3, "content": "This was my first thought. Might be skipping the apartment websites altogether and figured out how to data mine the hosts directly."}
{"id": "miyyheq", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miyyheq/", "author": "Vegetable-Pea2016", "created_utc": 1742563805, "score": 5, "content": "Option 1 seems very likely A lot of these vendors promise a ton of breadth of data but then it turns out there are big gaps. They just assume you won\u2019t catch all of them because to validate you would also have to scrape every website"}
{"id": "mj00u38", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj00u38/", "author": "lgastako", "created_utc": 1742575360, "score": 10, "content": "There is an answer 4: Someone managed to assemble a team of smart, experienced engineers, communicate clearly the requirements and got out of their way. It's not that hard to build something like this if you have a clear vision of what you want to build and your team has already done something similar before. I co-founded milo.com back in 2008 and I built what we called the crawler construction kit in about two months, and within three we were scraping real-time product availability for all major products in all major zip codes from all major retailers. The tools available today before you even bring AI into the picture make it so much easier to do something like this today, even given the more client-side heavy nature of todays web \"apps\"."}
{"id": "miz609j", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miz609j/", "author": "the-wise-man", "created_utc": 1742566268, "score": 1, "content": "Answer 3 Can't be done as well. I am managing a team of web scrapers in Pakistan and the max custom scrapers we have managed for a single client was around 100 sites. Although I have a very small team but still 150k sites is too much. They are definitely lying or using LLM for parsing."}
{"id": "miz97hj", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miz97hj/", "author": "Mysterious_Sir_2400", "created_utc": 1742567244, "score": 1, "content": "\u201cUsing LLMs will give you bad outputs here and there\u201d In my experience, incorrect outputs can reach up to 100%, especially, if they use cheaper and quicker models. So the output needs to be checked constantly, which cannot be maintained in the long run. I also vote for \u201cthey are simply lying\u201d."}
{"id": "mj015oh", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj015oh/", "author": "None", "created_utc": 1742575452, "score": 1, "content": "[removed]"}
{"id": "mj0djjy", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj0djjy/", "author": "throw_away_17381", "created_utc": 1742578928, "score": 1, "content": "Answer 4: They're not scraping said sites, probably monitoring for changes and moving along if no changes detected."}
{"id": "mj1kwbw", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj1kwbw/", "author": "JabootieeIsGroovy", "created_utc": 1742591773, "score": 1, "content": "option 2 is a new interesting approach someone can take tho and maybe not 150k sites but like 500-1000 sites. i know this is how hiring.cafe gets their info."}
{"id": "mj58pyi", "type": "comment", "parent_id": "t1_mixhg8r", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj58pyi/", "author": "This_Cardiologist242", "created_utc": 1742651679, "score": 1, "content": "Option 2 is my opinion. If you cache your data correctly, you can get the llm to write a unique script per website, associate the script to the website url, and use this saved script the next time around when the url appears in your loop so that you don\u2019t api yourself out of a house."}
{"id": "mj5eu50", "type": "comment", "parent_id": "t1_mixn59n", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj5eu50/", "author": "chorao_", "created_utc": 1742653782, "score": 1, "content": "How would these data platforms monetize their services?"}
{"id": "mj053x9", "type": "comment", "parent_id": "t1_miy678e", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj053x9/", "author": "major_bluebird_22", "created_utc": 1742576576, "score": 1, "content": "To answer your question I am not sure. However I doubt the feeds are used, or used in a way that covers any meaningful percentage of the data that is actually gathered and served to customers. The platform's data was pitched to me as all being publicly available. Also I work in RE space. From my own experience we have found: \\- Most RE owners and developers are unsophisticated from a data standpoint (even the larger groups). they are not capable of providing any sort of feed to platforms like this. Maybe they can provide .csv or .xlsx files and even that is a stretch for these groups. \\- Property managers and owners that do provide data to platforms direct through a feed, doesn't guarantee that it results in that information 1.) showing up in the data platforms and 2.) being accurate. We pay for a data platform (separate from the one being discussed here) that uses direct feeds from property managers and data is often missing and inaccurate. We know because some of the properties we own are on these platforms and the data is flat out wrong or inexplicably not there."}
{"id": "mizcxal", "type": "comment", "parent_id": "t1_miz7m3m", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mizcxal/", "author": "webscraping-ModTeam", "created_utc": 1742568355, "score": 2, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mj0zis4", "type": "comment", "parent_id": "t1_mizw08q", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj0zis4/", "author": "webscraping-ModTeam", "created_utc": 1742585390, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mj2a47q", "type": "comment", "parent_id": "t1_mizx3mk", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj2a47q/", "author": "nizarnizario", "created_utc": 1742600052, "score": 1, "content": "You'll get bad output A LOT, LLMs are not very accurate, especially for 150K websites per day (tens of millions of pages)"}
{"id": "mjgwpq6", "type": "comment", "parent_id": "t1_mj8b8i3", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjgwpq6/", "author": "treeset", "created_utc": 1742820365, "score": 1, "content": "what services are you using to scrape 10000+ websites? Did you first manually set those sites"}
{"id": "mjptjn7", "type": "comment", "parent_id": "t1_mjpds0u", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjptjn7/", "author": "webscraping-ModTeam", "created_utc": 1742934094, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "miye632", "type": "comment", "parent_id": "t1_miycwfm", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/miye632/", "author": "RedditCommenter38", "created_utc": 1742555701, "score": 1, "content": "I actually want to go on and see for myself if I can scrape that many websites with my 8 year old HP. I was looking for a new \u201creason why I should build this\u201d and I think this is it haha I scraped the entire Keno gaming system last year. Over 2 millions lines of data total. That was fun, this seems easier in some ways based on the \u201chost template\u201d approach ."}
{"id": "mj6kvit", "type": "comment", "parent_id": "t1_miycwfm", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj6kvit/", "author": "dclets", "created_utc": 1742667113, "score": 1, "content": "Yes. There\u2019s a few that have their apis open to the public. Will need to do some reverse engineering and then get the request ls just right to not get blocked. It\u2019s doable"}
{"id": "mj01ii7", "type": "comment", "parent_id": "t1_miyyheq", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj01ii7/", "author": "major_bluebird_22", "created_utc": 1742575555, "score": 4, "content": "I'll be getting access to the platform. Will let you know what the results are as we will look to verify data on quite a number of properties."}
{"id": "mj08698", "type": "comment", "parent_id": "t1_mj00u38", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj08698/", "author": "themasterofbation", "created_utc": 1742577451, "score": 3, "content": "Great, thanks for the answer! Reddit is amazing for this, since I am basing my answers based upon my knowledge, which is limited to my experience... Can you see them doing it at such a scale? How would you even start amassing 150k sites to begin with? Which tools would you recommend I look into, if I were to replicate this?"}
{"id": "mj5b9hp", "type": "comment", "parent_id": "t1_mj00u38", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj5b9hp/", "author": "Botek", "created_utc": 1742652567, "score": 2, "content": "Yeah this. We do ~550k websites daily with a team of 10. Spent years building the framework, now reaping the rewards"}
{"id": "mizch62", "type": "comment", "parent_id": "t1_miz97hj", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mizch62/", "author": "das_war_ein_Befehl", "created_utc": 1742568224, "score": 1, "content": "V3/R1 is hella cheap if you\u2019re using a cloud host for inference, I think it all really depends on the profit margins of the platform"}
{"id": "mj0zgdv", "type": "comment", "parent_id": "t1_mj015oh", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj0zgdv/", "author": "webscraping-ModTeam", "created_utc": 1742585370, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mj5sp86", "type": "comment", "parent_id": "t1_mj5eu50", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj5sp86/", "author": "das_war_ein_Befehl", "created_utc": 1742658289, "score": 3, "content": "They sell access to companies on a per seat basis. Companies use this to identify other companies to market and sell to."}
{"id": "mj6lba8", "type": "comment", "parent_id": "t1_mj053x9", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj6lba8/", "author": "dclets", "created_utc": 1742667252, "score": 0, "content": "Are you open about the platform you guys use? If not I completely understand."}
{"id": "mj5ccam", "type": "comment", "parent_id": "t1_mj2a47q", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj5ccam/", "author": "AdministrativeHost15", "created_utc": 1742652936, "score": 1, "content": "LLM will produce some hallucinations but the only way to verify the data is for the user to visit the source site and then you can just say that the page changed since it was last crawled."}
{"id": "mj02lri", "type": "comment", "parent_id": "t1_miye632", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj02lri/", "author": "major_bluebird_22", "created_utc": 1742575867, "score": 1, "content": "I asked them this specific question on the demo. \"Is your team actually pulling data from the property specific websites? Or are you scraping from aggregator sites likes [apts.com]( and zillow.com?\" Their response \"Both. Data coming directly from the property website, if available, is presented to customer first. If that data is missing we go to the aggregators.\" Which surprised me even further as this means more scraping, more scripts etc. Unless of course the data that is being served to end users is grossly overweighted towards being aggregator site sourced... Definitely a possibility."}
{"id": "mqubhv6", "type": "comment", "parent_id": "t1_miye632", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mqubhv6/", "author": "Unlikely_Track_5154", "created_utc": 1746511586, "score": 1, "content": "You talking about cname mask websites? I recently learned about that term, super helpful to know for web scraping."}
{"id": "mj6kcgi", "type": "comment", "parent_id": "t1_mj01ii7", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj6kcgi/", "author": "dclets", "created_utc": 1742666944, "score": 1, "content": "Might be better to reverse engineer a property websites api and use that with an ip rotation service"}
{"id": "mj09ceq", "type": "comment", "parent_id": "t1_mj08698", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj09ceq/", "author": "major_bluebird_22", "created_utc": 1742577784, "score": 2, "content": "Agreed - this is my first time using Reddit and the response to this thread has been incredible. This is a great question, re: amassing of 150k sites I had a similar thought - how would you assemble this part of your pipeline? i.e. constantly scanning for new apartment communities as new projects are constantly delivering and coming online across the country."}
{"id": "mjetin0", "type": "comment", "parent_id": "t1_mj5b9hp", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mjetin0/", "author": "Accomplished_Glass79", "created_utc": 1742780687, "score": 1, "content": "Apartment sites?"}
{"id": "mizgnro", "type": "comment", "parent_id": "t1_mizch62", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mizgnro/", "author": "themasterofbation", "created_utc": 1742569459, "score": 1, "content": "Yeah but 150k sites PER DAY? 4.5 million per month? They could be self hosting, but then can you parse 100 sites with an LLM every minute? I mean everything is possible, but as you said, depends on profit margins"}
{"id": "mj6l09c", "type": "comment", "parent_id": "t1_mj02lri", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj6l09c/", "author": "None", "created_utc": 1742667155, "score": 1, "content": "[removed]"}
{"id": "mj0djpg", "type": "comment", "parent_id": "t1_mj09ceq", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj0djpg/", "author": "themasterofbation", "created_utc": 1742578929, "score": 3, "content": "I mean I can see using a SERP API and searching for \"Apartments + \\[City\\]\" for example to get results. That would work... Also, as someone mentioned, MOST of the sites would be from a few major template/app providers, which you should be able to tell via the code within the site...for those, you could skip the validation, as you'd know which page and which elements the pricing would be shown on"}
{"id": "mj0odwk", "type": "comment", "parent_id": "t1_mj09ceq", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj0odwk/", "author": "Ace2Face", "created_utc": 1742582115, "score": 2, "content": "Don't get used it to bro the rest of the site is a shit show"}
{"id": "mqubzk0", "type": "comment", "parent_id": "t1_mizgnro", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mqubzk0/", "author": "Unlikely_Track_5154", "created_utc": 1746511868, "score": 1, "content": "You don't need to parse that much. You use the llm to make the script to parse the data for you, and you kick any errors to the llm to see what is going on. Then if the llm can't get it, human time."}
{"id": "mj8vthf", "type": "comment", "parent_id": "t1_mj6l09c", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mj8vthf/", "author": "webscraping-ModTeam", "created_utc": 1742695508, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mqucvz3", "type": "comment", "parent_id": "t1_mqubzk0", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mqucvz3/", "author": "themasterofbation", "created_utc": 1746512395, "score": 1, "content": "You mean ask LLM to create 150k parsers? You'd still be looking at a huuuge error rate from my experience"}
{"id": "mqunzs6", "type": "comment", "parent_id": "t1_mqucvz3", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mqunzs6/", "author": "Unlikely_Track_5154", "created_utc": 1746519388, "score": 1, "content": "You chop away at it slowly. And there are not very many ways to present the information of 1 bed 1 bath 1500/ month, so that makes it a lot easier."}
{"id": "mquredf", "type": "comment", "parent_id": "t1_mqunzs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mquredf/", "author": "themasterofbation", "created_utc": 1746521634, "score": 1, "content": "150k is still too much for that IMO. I believe what another commenter mentioned is the real way they deal with this type of volume - 90%+ of the sites will be using a template from a handful of app providers. That way, you have a handful of scrapers for 135k+ websites... Then, you deal with the delta"}
{"id": "mqvsqmp", "type": "comment", "parent_id": "t1_mquredf", "permalink": "https://www.reddit.com/r/webscraping/comments/1jg94ha/how_does_a_small_team_scrape_data_daily_from_150k/mqvsqmp/", "author": "Unlikely_Track_5154", "created_utc": 1746538858, "score": 1, "content": "Can't argue with that. Different biz, same idea. They have one white label company that represents like 6k of ~20k sites that I watch daily. Another few are ~1k and then it just goes down from there. Luckily the information is very similar across all of the sites so I was able to make a thing that handles Luke 80% of the Delta, but unfortunately the other 20% was extra juicy, so I had to work for it."}
{"id": "1kqyyd5", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/", "author": "antvas", "created_utc": 1747726084, "score": 133, "title": "What a Binance CAPTCHA solver tells us about today\u2019s bot threats", "content": "Hi, author here. A few weeks ago, someone shared an open-source Binance CAPTCHA solver in this subreddit. It\u2019s a Python tool that bypasses Binance\u2019s custom slider CAPTCHA. No browser involved. Just a custom HTTP client, image matching, and some light reverse engineering. I decided to take a closer look and break down how it works under the hood. It\u2019s pretty rare to find a public, non-trivial solver targeting a real-world CAPTCHA, especially one that doesn\u2019t rely on browser automation. That alone makes it worth dissecting, particularly since similar techniques are increasingly used at scale for credential stuffing, scraping, and other types of bot attacks. The post is a bit long, but if you're interested in how Binance's CAPTCHA flow works, and how attackers bypass it without using a browser, here\u2019s the full analysis: ["}
{"id": "mt9kwwt", "type": "comment", "parent_id": "t3_1kqyyd5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mt9kwwt/", "author": "Lower_Compote_6672", "created_utc": 1747731660, "score": 3, "content": "Great article!"}
{"id": "mth2i1e", "type": "comment", "parent_id": "t3_1kqyyd5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mth2i1e/", "author": "Affectionate_View224", "created_utc": 1747835698, "score": 2, "content": "Really great article. Well written!"}
{"id": "mtswwot", "type": "comment", "parent_id": "t3_1kqyyd5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mtswwot/", "author": "_iamhamza_", "created_utc": 1747989920, "score": 2, "content": "I'm gonna read this while drinking my morning coffee!"}
{"id": "mtcgcyt", "type": "comment", "parent_id": "t3_1kqyyd5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mtcgcyt/", "author": "amemingfullife", "created_utc": 1747768110, "score": 1, "content": "Really good read and new information for me! Love it!"}
{"id": "mtcqsrc", "type": "comment", "parent_id": "t3_1kqyyd5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mtcqsrc/", "author": "RHiNDR", "created_utc": 1747771214, "score": 1, "content": "great write up! very interesting :)"}
{"id": "mtlytes", "type": "comment", "parent_id": "t3_1kqyyd5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mtlytes/", "author": "xkiiann", "created_utc": 1747893215, "score": 1, "content": "Awesome but you could\u2019ve mentioned my repository"}
{"id": "mt9lk4g", "type": "comment", "parent_id": "t1_mt9kwwt", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mt9lk4g/", "author": "antvas", "created_utc": 1747732077, "score": 2, "content": "Thank you"}
{"id": "mtcj2lf", "type": "comment", "parent_id": "t1_mtcgcyt", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mtcj2lf/", "author": "amemingfullife", "created_utc": 1747768914, "score": 1, "content": "One thing I\u2019ve always wondered: is there any point in obfuscation? I\u2019ve always found that minification does plenty of obfuscation anyway."}
{"id": "mtiai8l", "type": "comment", "parent_id": "t1_mtcj2lf", "permalink": "https://www.reddit.com/r/webscraping/comments/1kqyyd5/what_a_binance_captcha_solver_tells_us_about/mtiai8l/", "author": "amitchau1111", "created_utc": 1747848431, "score": 1, "content": "yes, it does play a role by making the researcher s life more difficult to get to the actual meaningful disassembled code"}
{"id": "1i0ndg2", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/", "author": "woodkid80", "created_utc": 1736799219, "score": 136, "title": "What are your most difficult sites to scrape?", "content": "What\u2019s the site that\u2019s drained the most resources - time, money, or sheer mental energy - when you\u2019ve tried to scrape it? Maybe it\u2019s packed with anti-bot scripts, aggressive CAPTCHAs, constantly changing structures, or just an insane amount of data to process? Whatever it is, I\u2019m curious to know which site really pushed your setup to its limits (or your patience). Did you manage to scrape it in the end, or did it prove too costly to bother with?"}
{"id": "m6zmjqi", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zmjqi/", "author": "cheddar_triffle", "created_utc": 1736803815, "score": 32, "content": "I'm trying to scrape an API that's behind cloudflare. And ideally I'd make over one millions requests a day. So far I'm struggling to come up with a good proxy provider who can help me with this task as Cloudflare seems to either already know about the IP's I'm using, or will cut off access after maybe 10k requests per IP"}
{"id": "m6zak2t", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zak2t/", "author": "bar_pet", "created_utc": 1736800334, "score": 36, "content": "LinkedIn is one of the hardest to scrape real-time."}
{"id": "m6ztc6n", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6ztc6n/", "author": "dimsumham", "created_utc": 1736805800, "score": 14, "content": "This is def no the most difficult, but it is the most \\*needlessly difficult\\* site. [sedarplus.ca]( this is a regulatory site for accessing Canadian public company filings. Similar to EDGAR. If anyone wants to lose their mind, try scraping perma links, hidden behind multiple 3-5 second round trips"}
{"id": "m70bg8p", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70bg8p/", "author": "1234backdoor12", "created_utc": 1736811574, "score": 13, "content": "Bet365"}
{"id": "m6zep3f", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zep3f/", "author": "bashvlas", "created_utc": 1736801543, "score": 8, "content": "Ticketmaster"}
{"id": "m6zocus", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zocus/", "author": "Pigik83", "created_utc": 1736804341, "score": 8, "content": "Tmall, shopee"}
{"id": "m70bn10", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70bn10/", "author": "rundef", "created_utc": 1736811638, "score": 9, "content": "Anything behind cloudflare. [wsj.com]( is highly protected [ft.com]( returning 404s or 406s when you scrape too much, even their rss urls (wtf!)"}
{"id": "m7058w2", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7058w2/", "author": "Key_Statistician6405", "created_utc": 1736809513, "score": 7, "content": "I\u2019ve been researching that for X, from what I gather it is not possible. Has anyone done it successfully recently?"}
{"id": "m70b0e6", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70b0e6/", "author": "Fun-Sample336", "created_utc": 1736811424, "score": 4, "content": "I did not try it yet, but I think scraping discussions of closed Facebook groups will be difficult."}
{"id": "m6zlc46", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zlc46/", "author": "ZeroOne001010", "created_utc": 1736803462, "score": 4, "content": "Apple reviews"}
{"id": "m6zo6q1", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zo6q1/", "author": "ChuckleBerryCheetah", "created_utc": 1736804290, "score": 3, "content": "Crunchbase"}
{"id": "m715c8y", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m715c8y/", "author": "worldtest2k", "created_utc": 1736821427, "score": 3, "content": "ESPN scoreboard is a pain as I had to search the html for a tag that contains JSON data, but it actually contains multiple chunks of JSON that need to be separated before loading into JSON parser. Also FotMob was great until they added their APIs to robots.txt and I've spent hours (unsuccessfully) trying workarounds"}
{"id": "m72jpap", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m72jpap/", "author": "seo_hacker", "created_utc": 1736844682, "score": 3, "content": "LinkedIn.com, Google SERP pages, Crunchbase, and sites protected by Cloudflare. But this doesn't mean they are unscrapable at all; you cannot simply send a large set of scraping requests."}
{"id": "m6ztudi", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6ztudi/", "author": "Potential_You42", "created_utc": 1736805955, "score": 2, "content": "Mobile.de"}
{"id": "m733jum", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m733jum/", "author": "Large_Soup452", "created_utc": 1736856708, "score": 2, "content": "Capterra, G2"}
{"id": "m76fkvj", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m76fkvj/", "author": "Puzzleheaded_Web551", "created_utc": 1736895143, "score": 2, "content": "An aspx site that I was trying to scrape had urls hidden behind JavaScript_doPostBack links. Wasn\u2019t worth the effort for me to figure it out. Seemed annoying to do."}
{"id": "m76hbus", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m76hbus/", "author": "ForrestDump6", "created_utc": 1736895700, "score": 2, "content": "Twitter/X requires playwright"}
{"id": "m6zdt7w", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zdt7w/", "author": "Resiakvrases", "created_utc": 1736801286, "score": 1, "content": "Follow"}
{"id": "m6zegr8", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zegr8/", "author": "01jasper", "created_utc": 1736801475, "score": 1, "content": "Following"}
{"id": "m6zzanq", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zzanq/", "author": "No_River_8171", "created_utc": 1736807625, "score": 1, "content": "Scraping with only requests and bs4 no selenium"}
{"id": "m703omb", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m703omb/", "author": "joeyx22lm", "created_utc": 1736809006, "score": 1, "content": "CAPTCHAs and things are easy. What is hard is reverse engineering the arbitrary WAF rules that duller organizations put in place to prevent scraping. Only Chrome 124 is allowed? Makes sense, got it."}
{"id": "m70gm4b", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70gm4b/", "author": "yyavuz", "created_utc": 1736813316, "score": 1, "content": "Following"}
{"id": "m70poq1", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70poq1/", "author": "whozzyurDaddy111", "created_utc": 1736816335, "score": 1, "content": "Is it possible to scrape kayak?"}
{"id": "m70ue0g", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70ue0g/", "author": "Rizzon1724", "created_utc": 1736817885, "score": 1, "content": "I would kiss anyone who is up for scraping all of MuckRack for me. Please and thank you <3."}
{"id": "m70wonc", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70wonc/", "author": "Groundbreaking_Fly36", "created_utc": 1736818632, "score": 1, "content": "booking.com still a problem for me"}
{"id": "m70yl5k", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70yl5k/", "author": "Stock_Debate6011", "created_utc": 1736819247, "score": 1, "content": "any country Vfs site. impossible to automate."}
{"id": "m7722ni", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7722ni/", "author": "luenwarneke", "created_utc": 1736902601, "score": 1, "content": "AllTrails can be annoying, but still possible."}
{"id": "m776dff", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m776dff/", "author": "onlytheeast99", "created_utc": 1736904093, "score": 1, "content": "Imperva protected sites"}
{"id": "m783p14", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m783p14/", "author": "None", "created_utc": 1736916518, "score": 1, "content": "Onlyfans I\u2019ve offered several scraping experts money to get a full database and no one will do it"}
{"id": "m7918sj", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7918sj/", "author": "Just_Daily_Gratitude", "created_utc": 1736936144, "score": 1, "content": "Scraping an artists discography (lyrics) from [genius.com]( has been tough for me but that may be because I don't know what I'm doing."}
{"id": "m79fg57", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m79fg57/", "author": "turingincarnate", "created_utc": 1736944095, "score": 1, "content": "Total wine and more!!!!! Hotel/travel sites!"}
{"id": "m79w1ab", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m79w1ab/", "author": "jcachat", "created_utc": 1736950671, "score": 1, "content": "i have been trying to find a web scraper able to scrap Google Cloud Documentation & simply have been unable to find anything that works"}
{"id": "m7fa0g9", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7fa0g9/", "author": "Ok-Engineering1606", "created_utc": 1737018786, "score": 1, "content": "Google trends.. extremely difficult"}
{"id": "m7ildrr", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7ildrr/", "author": "None", "created_utc": 1737061180, "score": 1, "content": "Walmart"}
{"id": "m7nzqe9", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7nzqe9/", "author": "Otherwise-Youth2025", "created_utc": 1737137126, "score": 1, "content": "For me it's trying to automate signup for wsj.com ... the bot detection protocols are unreal. I've wasted dozens of hours with no results to show"}
{"id": "m7onu5c", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7onu5c/", "author": "Tadpatri", "created_utc": 1737144102, "score": 1, "content": "Costar"}
{"id": "m7qoyhg", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7qoyhg/", "author": "skatastic57", "created_utc": 1737168276, "score": 1, "content": "Tibco"}
{"id": "m7qwlqa", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7qwlqa/", "author": "hollyjphilly", "created_utc": 1737171230, "score": 1, "content": "Stop & Shop grocery store. I just want to automate ordering my groceries gosh darn it,"}
{"id": "m7sndsv", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7sndsv/", "author": "theflyingdeer", "created_utc": 1737205325, "score": 1, "content": "Indeed.com...because of Cloudflare."}
{"id": "maadwod", "type": "comment", "parent_id": "t3_1i0ndg2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/maadwod/", "author": "woodkid80", "created_utc": 1738367114, "score": 1, "content": "Ok, so I think I have finally managed to create a tool that scrapes most of the websites listed here :) Still testing, but it looks very promising. Headless browser powered by a local LLM. Seems to do the job with some premium proxies. I am scraping thousands of URLs per hour now."}
{"id": "m71jo34", "type": "comment", "parent_id": "t1_m6zmjqi", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71jo34/", "author": "C_hyphen_S", "created_utc": 1736826224, "score": 3, "content": "I\u2019m in more or less the same situation. API behind cloudflare, need to make about half a million requests per day for it to be of value, proxy providers are just too expensive to pull that off"}
{"id": "m71klem", "type": "comment", "parent_id": "t1_m6zmjqi", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71klem/", "author": "None", "created_utc": 1736826557, "score": 1, "content": "[removed]"}
{"id": "m6zoza3", "type": "comment", "parent_id": "t1_m6zmjqi", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zoza3/", "author": "woodkid80", "created_utc": 1736804523, "score": -3, "content": "Yeah, Cloudflare"}
{"id": "m6zcz7e", "type": "comment", "parent_id": "t1_m6zak2t", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zcz7e/", "author": "intelligence-magic", "created_utc": 1736801041, "score": 6, "content": "Is it because you need to be signed in?"}
{"id": "m720cey", "type": "comment", "parent_id": "t1_m6zak2t", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m720cey/", "author": "ssfts", "created_utc": 1736833276, "score": 4, "content": "Totally agree I managed to create a local scraper using a legit account (login + 2FA via email + puppeteer stealth plugin), but I couldn't get it work on a ec2 with a fake account. Only one fake (but old) account managed to survive four about 4 months before getting banned. And then, every fake account I tried to set up was banned within 2-3 days."}
{"id": "m6zj72f", "type": "comment", "parent_id": "t1_m6zak2t", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zj72f/", "author": "woodkid80", "created_utc": 1736802840, "score": 2, "content": "Agreed, fully."}
{"id": "m7kqwnw", "type": "comment", "parent_id": "t1_m6zak2t", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7kqwnw/", "author": "Flat_Palpitation_158", "created_utc": 1737086954, "score": 2, "content": "Are you trying to scrape LinkedIn profiles? Because it\u2019s surprisingly easy to crawl LinkedIn company pages\u2026"}
{"id": "m6zuj9e", "type": "comment", "parent_id": "t1_m6ztc6n", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zuj9e/", "author": "woodkid80", "created_utc": 1736806165, "score": 1, "content": "Interesting! Thanks for sharing."}
{"id": "m70r17c", "type": "comment", "parent_id": "t1_m6ztc6n", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70r17c/", "author": "pica16", "created_utc": 1736816784, "score": 1, "content": "I'm very curious about this one. What are you trying to extract? Is it just because the site is poorly designed?"}
{"id": "m8c7ysh", "type": "comment", "parent_id": "t1_m6ztc6n", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8c7ysh/", "author": "seo_hacker", "created_utc": 1737463373, "score": 1, "content": "Can you share the excat url where the details are shown, let me try"}
{"id": "m74qzxu", "type": "comment", "parent_id": "t1_m70bg8p", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m74qzxu/", "author": "LocalConversation850", "created_utc": 1736876877, "score": 1, "content": "Currently im on a mission to automate the signup process, and successfully did it with an antidetect browser, Have time to share your experience with bet365 ?"}
{"id": "m7dsf9f", "type": "comment", "parent_id": "t1_m70bg8p", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7dsf9f/", "author": "kjsnoopdog", "created_utc": 1736993405, "score": 1, "content": "Have you tried fanduel?"}
{"id": "m7w3jdi", "type": "comment", "parent_id": "t1_m70bg8p", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7w3jdi/", "author": "josejuanrguez", "created_utc": 1737245038, "score": 1, "content": "Bet365 is a pain in the ass."}
{"id": "mzcylg3", "type": "comment", "parent_id": "t1_m6zep3f", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mzcylg3/", "author": "None", "created_utc": 1750698240, "score": 1, "content": "[removed]"}
{"id": "m7mhosa", "type": "comment", "parent_id": "t1_m6zocus", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7mhosa/", "author": "Healthy-Educator-289", "created_utc": 1737120569, "score": 2, "content": "Struggling with shopee"}
{"id": "m7gv0b9", "type": "comment", "parent_id": "t1_m6zocus", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7gv0b9/", "author": "obhuat", "created_utc": 1737043177, "score": 1, "content": "Feel the same about shopee"}
{"id": "m70a0kz", "type": "comment", "parent_id": "t1_m7058w2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70a0kz/", "author": "deliadam11", "created_utc": 1736811088, "score": 3, "content": "\\^I am curious about this"}
{"id": "m72o0mt", "type": "comment", "parent_id": "t1_m7058w2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m72o0mt/", "author": "KendallRoyV2", "created_utc": 1736847581, "score": 3, "content": "X changes the cookies with every request u make, so i guess the only option is to automate it with playwright or selenium cuz cookies won't stand a request :("}
{"id": "m70u8aa", "type": "comment", "parent_id": "t1_m7058w2", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70u8aa/", "author": "None", "created_utc": 1736817833, "score": 2, "content": "[removed]"}
{"id": "m73im6t", "type": "comment", "parent_id": "t1_m70b0e6", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m73im6t/", "author": "woodkid80", "created_utc": 1736862986, "score": 1, "content": "You just need to be in the group."}
{"id": "m7q1dfq", "type": "comment", "parent_id": "t1_m715c8y", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7q1dfq/", "author": "kicker3192", "created_utc": 1737159739, "score": 2, "content": "Just FYI, you can get a good amount of ESPN stuff with the \"Hidden ESPN API\" endpoint, very prominently on GitHub."}
{"id": "m7kqy8y", "type": "comment", "parent_id": "t1_m72jpap", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7kqy8y/", "author": "Flat_Palpitation_158", "created_utc": 1737086972, "score": 1, "content": "Are you trying to scrape LinkedIn profiles? Because it\u2019s surprisingly easy to crawl LinkedIn company pages\u2026"}
{"id": "m6zus7w", "type": "comment", "parent_id": "t1_m6ztudi", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zus7w/", "author": "woodkid80", "created_utc": 1736806240, "score": 2, "content": "What's the issue here?"}
{"id": "m70b07o", "type": "comment", "parent_id": "t1_m6ztudi", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70b07o/", "author": "Hidden_Bystander", "created_utc": 1736811422, "score": 2, "content": "Also interested in scrapping it soon - Why do you say that?"}
{"id": "m7s54uk", "type": "comment", "parent_id": "t1_m6ztudi", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7s54uk/", "author": "lieutenant_lowercase", "created_utc": 1737195509, "score": 1, "content": "I scrape the entire thing daily pretty quickly. What\u2019s the issue?"}
{"id": "myoit6g", "type": "comment", "parent_id": "t1_m76hbus", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/myoit6g/", "author": "Theredeemer08", "created_utc": 1750360239, "score": 1, "content": "how did you manage to get Twitter/X? even with playwright you can't seem to do any at scale scraping (e.g. 100k tweets a day)"}
{"id": "m7417e0", "type": "comment", "parent_id": "t1_m703omb", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7417e0/", "author": "iceman1234567890", "created_utc": 1736869239, "score": 1, "content": "How are you solving CAPTCHAs?"}
{"id": "m75lshu", "type": "comment", "parent_id": "t1_m703omb", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m75lshu/", "author": "phelippmichel", "created_utc": 1736885777, "score": 1, "content": "How are you solving CAPTCHAS?"}
{"id": "m7ueffe", "type": "comment", "parent_id": "t1_m70ue0g", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7ueffe/", "author": "jamesmundy", "created_utc": 1737225908, "score": 1, "content": "Do you mean you want a copy of every page on the site?"}
{"id": "mg3xkuo", "type": "comment", "parent_id": "t1_m70ue0g", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mg3xkuo/", "author": "None", "created_utc": 1741157664, "score": 1, "content": "[removed]"}
{"id": "m71u0dy", "type": "comment", "parent_id": "t1_m70wonc", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71u0dy/", "author": "intelligence-magic", "created_utc": 1736830309, "score": 4, "content": "What are the challenges there?"}
{"id": "m79fwmt", "type": "comment", "parent_id": "t1_m79fg57", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m79fwmt/", "author": "syphoon_data", "created_utc": 1736944306, "score": 1, "content": "Well, some of them like Qunar, CTrip can be challenging (mostly because they\u2019re Chinese), but we did fairly well getting around. As for the popular ones like booking, Expedia, agoda, kayak, VRBO, they aren\u2019t really that difficult."}
{"id": "m7uennc", "type": "comment", "parent_id": "t1_m79w1ab", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7uennc/", "author": "jamesmundy", "created_utc": 1737225976, "score": 1, "content": "what are the difficulties here?"}
{"id": "m7fja55", "type": "comment", "parent_id": "t1_m7fa0g9", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7fja55/", "author": "woodkid80", "created_utc": 1737024673, "score": 1, "content": "How so?"}
{"id": "m7ioc24", "type": "comment", "parent_id": "t1_m7ildrr", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7ioc24/", "author": "syphoon_data", "created_utc": 1737062035, "score": 1, "content": "Intrigued to know why you mentioned Walmart. Walmart (and Amazon, for that matter), is pretty doable as far as PDP level data is concerned. However, zip code and seller level data can be challenging."}
{"id": "m7552zp", "type": "comment", "parent_id": "t1_m71jo34", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7552zp/", "author": "cheddar_triffle", "created_utc": 1736880926, "score": 2, "content": "You have any luck in using any of the \"anti-Cloudflare\" type packages that are abundant on GitHub or via a google search?"}
{"id": "m71m16i", "type": "comment", "parent_id": "t1_m71klem", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71m16i/", "author": "webscraping-ModTeam", "created_utc": 1736827089, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m6zqa85", "type": "comment", "parent_id": "t1_m6zoza3", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zqa85/", "author": "Illustrious_King_397", "created_utc": 1736804905, "score": 2, "content": "any way of bypassing cloudflare?"}
{"id": "m6zhpek", "type": "comment", "parent_id": "t1_m6zcz7e", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zhpek/", "author": "520throwaway", "created_utc": 1736802412, "score": 21, "content": "It's because you need an account with legitimate history"}
{"id": "misso8w", "type": "comment", "parent_id": "t1_m720cey", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/misso8w/", "author": "None", "created_utc": 1742480509, "score": 1, "content": "Linkedin is kinda easy. I can scrape milions of accounts per day. I automate account generation. I automatically signup a bunch of accounts and distribue the scraping across them. If one get banned another service creates a new account. I try to keep a pool of accounts with a certain size for efficient scraping."}
{"id": "m720qk5", "type": "comment", "parent_id": "t1_m70r17c", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m720qk5/", "author": "dimsumham", "created_utc": 1736833469, "score": 8, "content": "Perma links for each regulatory documents. This can only be found by going to the search page, finding the right document, and clicking on \"Generate URL\" to reveal the link. Each click on this site, including Generate URL is a full page reload. The cookies / headers / whatever else gets sent along with request + complex server side state management + trigger happy captcha makes it very difficult to do this any other way than full scraping. The captchas are not your avg easy ones - not quite Twitter level but relatively difficult hCaptchas with distorted images etc. The fact that they put PUBLIC INFORMATION behind this much bullshit is unbelievable."}
{"id": "m8d35qj", "type": "comment", "parent_id": "t1_m8c7ysh", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8d35qj/", "author": "dimsumham", "created_utc": 1737474072, "score": 2, "content": "I cannot, as url is just session id with timestamp. Click on the link and go to search page Search for constellation software . The permalink is inside of generate url link in each row."}
{"id": "m792cv7", "type": "comment", "parent_id": "t1_m74qzxu", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m792cv7/", "author": "bli_b", "created_utc": 1736936856, "score": 1, "content": "Betting sites in general are insanely difficult. Even the HK jockey club, which looks like it comes out if the 90s, has decent guards. If you're trying to get odds, better to go through sites that aggregate those specifically"}
{"id": "m7kzfpu", "type": "comment", "parent_id": "t1_m74qzxu", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7kzfpu/", "author": "Ok-Engineering1606", "created_utc": 1737090635, "score": 1, "content": "which antidetect browser ?"}
{"id": "mzdo29r", "type": "comment", "parent_id": "t1_mzcylg3", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mzdo29r/", "author": "webscraping-ModTeam", "created_utc": 1750705282, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m714izz", "type": "comment", "parent_id": "t1_m70u8aa", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m714izz/", "author": "webscraping-ModTeam", "created_utc": 1736821158, "score": 2, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m73yhni", "type": "comment", "parent_id": "t1_m73im6t", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m73yhni/", "author": "Fun-Sample336", "created_utc": 1736868395, "score": 6, "content": "Yes, of course. But there is still the endless scrolling, which will eat up RAM sooner or later before you reach the bottom. This might be mitigated by deleting crawled posts from the DOM tree, but perhaps Facebook has scripts in place to detect this. The DOM-tree is also very obfuscated and I can imagine that they regularly change around on it. There might also be stuff like detection of mouse movements in order to tell real users and automated browsers apart. Unfortunately they removed access to [mbasic.facebook.com]( and [m.facebook.com]( which would have made scraping much easier."}
{"id": "m7seh6t", "type": "comment", "parent_id": "t1_m7q1dfq", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7seh6t/", "author": "worldtest2k", "created_utc": 1737200984, "score": 1, "content": "When I looked at that I didn't see one for live scores - is there one now?"}
{"id": "m8c8dpt", "type": "comment", "parent_id": "t1_m7kqy8y", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8c8dpt/", "author": "seo_hacker", "created_utc": 1737463549, "score": 1, "content": "How many pages were attempted?"}
{"id": "m7sm7pf", "type": "comment", "parent_id": "t1_m7s54uk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7sm7pf/", "author": "Potential_You42", "created_utc": 1737204811, "score": 1, "content": "Really? Can you send me the code?"}
{"id": "mypdxm0", "type": "comment", "parent_id": "t1_myoit6g", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mypdxm0/", "author": "ForrestDump6", "created_utc": 1750369429, "score": 1, "content": "I wasn\u2019t scraping tweets, just profiles"}
{"id": "mg4cabf", "type": "comment", "parent_id": "t1_mg3xkuo", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mg4cabf/", "author": "webscraping-ModTeam", "created_utc": 1741166471, "score": 1, "content": "Please review the sub rules"}
{"id": "m79g54z", "type": "comment", "parent_id": "t1_m79fwmt", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m79g54z/", "author": "turingincarnate", "created_utc": 1736944415, "score": 1, "content": "I guess my real point is, I work in econometrics, so I'm interested in panel data where we collect data on the same units over time. The site itself may be easy to scrape (and sometimes it is), but scaling it up to scrape everywhere daily, and clean the data.... not impossible, just haven't gotten around to it"}
{"id": "m84334w", "type": "comment", "parent_id": "t1_m7uennc", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m84334w/", "author": "jcachat", "created_utc": 1737348851, "score": 1, "content": "i have not found one scraper that could auto scrape say, all of BigQuery documentation. single, one off pages will work - although not great, usually a jumbled mess. and definitely nothing able to say scan every two weeks & scrape anything different from last scan"}
{"id": "m7ibm1d", "type": "comment", "parent_id": "t1_m7fja55", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7ibm1d/", "author": "Ok-Engineering1606", "created_utc": 1737058340, "score": 1, "content": "they are really good at detecting web scrapers"}
{"id": "m7ivk5m", "type": "comment", "parent_id": "t1_m7ioc24", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7ivk5m/", "author": "None", "created_utc": 1737064117, "score": 1, "content": "I was using the chrome drive to mimic the human operation, but the Walmart caught me all the time."}
{"id": "m78kox8", "type": "comment", "parent_id": "t1_m6zqa85", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m78kox8/", "author": "kev_11_1", "created_utc": 1736925439, "score": 5, "content": "Try this one it works sometimes:"}
{"id": "m6zr56r", "type": "comment", "parent_id": "t1_m6zqa85", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m6zr56r/", "author": "woodkid80", "created_utc": 1736805155, "score": 3, "content": "Yes, there are some solutions floating around."}
{"id": "m702xk2", "type": "comment", "parent_id": "t1_m6zqa85", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m702xk2/", "author": "joeyx22lm", "created_utc": 1736808764, "score": 3, "content": "Same way as every other captcha. Have a third party service farm it out to \"call-center\" workers \\[and sometimes maybe, probably not, actually use the AI they market\\]. Depends if challenge or turnstile, but tl;dr: have someone with the same user agent and user agent hints, and IP address calculate the cf\\_clearance cookie for you, then you're off the to the races. This typically involves sharing a proxy connection with a third party solver provider, having them solve, then taking the resulting token and using it."}
{"id": "m737d9y", "type": "comment", "parent_id": "t1_m6zqa85", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m737d9y/", "author": "ChallengeFull3538", "created_utc": 1736858484, "score": 3, "content": "Google cached version of the page."}
{"id": "m70fuw1", "type": "comment", "parent_id": "t1_m6zqa85", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70fuw1/", "author": "None", "created_utc": 1736813062, "score": 0, "content": "[removed]"}
{"id": "m71knuz", "type": "comment", "parent_id": "t1_m6zqa85", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71knuz/", "author": "None", "created_utc": 1736826582, "score": 0, "content": "[removed]"}
{"id": "m701cj1", "type": "comment", "parent_id": "t1_m6zhpek", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m701cj1/", "author": "das_war_ein_Befehl", "created_utc": 1736808263, "score": 9, "content": "No, you need\u2026lots of synthetic accounts. It\u2019s doable, there are a shit ton of cheap apis/providers for this that it\u2019s barely worth doing yourself from scratch."}
{"id": "m754qbp", "type": "comment", "parent_id": "t1_m73yhni", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m754qbp/", "author": "woodkid80", "created_utc": 1736880826, "score": 1, "content": "Yes, removing mbasic.\\* and m.\\* is a disaster :)"}
{"id": "m8111wu", "type": "comment", "parent_id": "t1_m73yhni", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8111wu/", "author": "None", "created_utc": 1737313399, "score": 1, "content": "[deleted]"}
{"id": "m7uagmr", "type": "comment", "parent_id": "t1_m7seh6t", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7uagmr/", "author": "kicker3192", "created_utc": 1737224705, "score": 1, "content": "What sport are you looking for?"}
{"id": "m8mb1ae", "type": "comment", "parent_id": "t1_m8c8dpt", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8mb1ae/", "author": "Flat_Palpitation_158", "created_utc": 1737584189, "score": 1, "content": "Like 100K a day. These are company pages like company/microsoft not individual profiles"}
{"id": "m79gxea", "type": "comment", "parent_id": "t1_m79g54z", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m79gxea/", "author": "syphoon_data", "created_utc": 1736944771, "score": 1, "content": "I get it. Haven\u2019t tried a lot, but processed a few million requests daily for the popular domains and it wasn\u2019t that difficult."}
{"id": "m8j2l24", "type": "comment", "parent_id": "t1_m84334w", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8j2l24/", "author": "jamesmundy", "created_utc": 1737551202, "score": 1, "content": "Interesting, what data format would you be looking for it to be in? Raw DOM, markdown, image? I'm working on a different product which doesn't yet offer whole directory crawling but does individual pages well so it is interesting to hear what challenges people are looking to solve"}
{"id": "m71j4d7", "type": "comment", "parent_id": "t1_m6zr56r", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71j4d7/", "author": "C_hyphen_S", "created_utc": 1736826024, "score": 2, "content": "Care to point me in a vague direction?"}
{"id": "m73ielg", "type": "comment", "parent_id": "t1_m737d9y", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m73ielg/", "author": "woodkid80", "created_utc": 1736862907, "score": 3, "content": "This actually worked sometimes, but google cache is no longer operational."}
{"id": "m70n7t8", "type": "comment", "parent_id": "t1_m70fuw1", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m70n7t8/", "author": "webscraping-ModTeam", "created_utc": 1736815516, "score": 0, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "m71m1zl", "type": "comment", "parent_id": "t1_m71knuz", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m71m1zl/", "author": "webscraping-ModTeam", "created_utc": 1736827097, "score": 2, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m74tcy4", "type": "comment", "parent_id": "t1_m701cj1", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m74tcy4/", "author": "deadcoder0904", "created_utc": 1736877549, "score": 2, "content": "as in?"}
{"id": "m814n0r", "type": "comment", "parent_id": "t1_m8111wu", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m814n0r/", "author": "Fun-Sample336", "created_utc": 1737314416, "score": 1, "content": "My first idea would be to scroll as far down as possible and collect the link for each thread. While doing so delete all threads from the DOM-tree, whose links were already collected. The hope would be that Facebook doesn't check for the continued presence of already loaded threads. If they do, delete as much content inside the DOM-tree of threads in order to minimize their memory footprint. Then open the link of each thread, click on every \"read more\" and similar links to get all posts, then copy the outer HTML of the whole thread and store in a database. Once all threads were collected in this way, we might look on how to convert the HTML into structured data. This may be the same for all threads, but Facebook might change the structure periodically, so in later crawls, queries might need to be adapted to the changes. It's probably important to space each interaction with Facebook with randomized and long time intervals to avoid detection. A real problem could be if Facebook also runs various other background checks, like detection of mouse movements."}
{"id": "m7xz2ho", "type": "comment", "parent_id": "t1_m7uagmr", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m7xz2ho/", "author": "worldtest2k", "created_utc": 1737268925, "score": 1, "content": "Soccer"}
{"id": "m8jroh1", "type": "comment", "parent_id": "t1_m8j2l24", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8jroh1/", "author": "jcachat", "created_utc": 1737559462, "score": 1, "content": "goal would be markdown, but really any format best prepared for embedding/vectorizing. goal is to have a chat app that contains the most recent/current GCP documentation"}
{"id": "m72f9zb", "type": "comment", "parent_id": "t1_m71j4d7", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m72f9zb/", "author": "UnlikelyLikably", "created_utc": 1736841795, "score": -1, "content": "Ulixee Hero"}
{"id": "m74tact", "type": "comment", "parent_id": "t1_m73ielg", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m74tact/", "author": "deadcoder0904", "created_utc": 1736877529, "score": 6, "content": "Yep, they stopped it for some reason. Was useful even without scraping to read paywalled sites lol."}
{"id": "mb9xzbl", "type": "comment", "parent_id": "t1_m814n0r", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mb9xzbl/", "author": "None", "created_utc": 1738841531, "score": 2, "content": "[deleted]"}
{"id": "m8juv10", "type": "comment", "parent_id": "t1_m8jroh1", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8juv10/", "author": "None", "created_utc": 1737560360, "score": 1, "content": "[removed]"}
{"id": "mba0tnx", "type": "comment", "parent_id": "t1_mb9xzbl", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/mba0tnx/", "author": "Fun-Sample336", "created_utc": 1738842974, "score": 1, "content": "I didn't try myself so far, but I would take a close look into the dev tools while scrolling down in order to find out. I would not only look at the DOM tree, but also at the tabs \"memory\" and \"network\". For example the latter one contains whatever ressources (for example images) are dynamically loaded along the way and perhaps they are not automatically discarded, even when the elements that contain them are deleted from the DOM tree. If you scroll down too fast, you may also get blocked from loading. This happened to me on mbasic, when I clicked on \"see more\" (or whatever it used to be called) too fast for an extended period of time and got an error message at some point."}
{"id": "m8l76x6", "type": "comment", "parent_id": "t1_m8juv10", "permalink": "https://www.reddit.com/r/webscraping/comments/1i0ndg2/what_are_your_most_difficult_sites_to_scrape/m8l76x6/", "author": "webscraping-ModTeam", "created_utc": 1737573447, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "1fea485", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/", "author": "jpjacobpadilla", "created_utc": 1726061284, "score": 133, "title": "Stay Undetected While Scraping the Web | Open Source Project", "content": "Hey everyone, I just released my new open-source project[ Stealth-Requests]( Stealth-Requests is an all-in-one solution for web scraping that seamlessly mimics a browser's behavior to help you stay undetected when sending HTTP requests. Here are some of the main features: * Mimics Chrome or Safari headers when scraping websites to stay undetected * Keeps tracks of dynamic headers such as Referer and Host * Masks the TLS fingerprint of requests to look like a browser * Automatically extract metadata from HTML responses including page title, description, author, and more * Lets you easily convert HTML-based responses into [lxml]( and [BeautifulSoup]( objects Hopefully some of you find this project helpful. Consider checking it out, and let me know if you have any suggestions!"}
{"id": "lmnic88", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmnic88/", "author": "None", "created_utc": 1726082967, "score": 11, "content": "[deleted]"}
{"id": "lmmix84", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmix84/", "author": "NopeNotHB", "created_utc": 1726071686, "score": 8, "content": "Can you tell me the difference between this and curl-cffi?"}
{"id": "lmmdygc", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmdygc/", "author": "Odd-Investigator6684", "created_utc": 1726070090, "score": 6, "content": "Can this be integrated with playwright so I can also scrape dynamic websites?"}
{"id": "lmmtxkp", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmtxkp/", "author": "AncientEnthusiasm583", "created_utc": 1726075189, "score": 3, "content": "It works! Damn, good job man."}
{"id": "lmo52zd", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmo52zd/", "author": "kabelman93", "created_utc": 1726090275, "score": 2, "content": "I would assume it also uses without 2 support, correct? I guess integrating a good client hello with TLS masking together with support would be pretty good. I build something like that for my specific usecase, but not a general one. Thank you for Open sourcing your solution. Will test it."}
{"id": "lmmy9dd", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmy9dd/", "author": "Project_Nile", "created_utc": 1726076551, "score": 1, "content": "Will this work on LinkedIN?"}
{"id": "lmmz0kx", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmz0kx/", "author": "serverloading101", "created_utc": 1726076788, "score": 1, "content": "Will this work on instagram"}
{"id": "lmppbnl", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmppbnl/", "author": "rudeyjohnson", "created_utc": 1726111951, "score": 1, "content": "Sweet stuff"}
{"id": "lmq2u8k", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmq2u8k/", "author": "C0ffeeface", "created_utc": 1726119033, "score": 1, "content": "Sounds cool. Does it have any features related to simply spidering efficiently and extracting basic info like ahrefs to follow?"}
{"id": "lmqo096", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmqo096/", "author": "renegat0x0", "created_utc": 1726133455, "score": 1, "content": "Great job, but... where is status code, where is header information? I want to be able to see that. What I want to handle 403 in some way?"}
{"id": "lmwh7r8", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmwh7r8/", "author": "RHiNDR", "created_utc": 1726218873, "score": 1, "content": "~~When sending a request, or creating a StealthSession, you can specify the type of browser that you want the request to mimic - either chrome, which is the default, or safari. If you want to change which browser to mimic, set the impersonate argument, either in requests.get or when initializing StealthSession to safari or chrome.~~ ~~Do we need to always have the impersonate flag or its only used if we want to change from the default chrome option :)~~ Ignore, I just looked at the code and seems default is chrome unless we choose to change :)"}
{"id": "lmoiupy", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmoiupy/", "author": "bRUNSKING", "created_utc": 1726095185, "score": 1, "content": "Can we use it with selenium?"}
{"id": "lmnqfzn", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmnqfzn/", "author": "None", "created_utc": 1726085531, "score": 0, "content": ""}
{"id": "lmpq3ad", "type": "comment", "parent_id": "t3_1fea485", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmpq3ad/", "author": "RacoonInThePool", "created_utc": 1726112307, "score": 0, "content": "What i need to know if i want to fully understand your open source, i have used a lot open source to bypass bot detecstion. And now i want to understand magic-thing behind it. How great you can come up with the idea to bypass these bots. Thank you."}
{"id": "lmsgm2w", "type": "comment", "parent_id": "t1_lmnic88", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmsgm2w/", "author": "Rc202402", "created_utc": 1726159585, "score": 2, "content": "TLS Fingerprint usually bypasses the normal cloudflare security For the IAM Cloudflare websites you require to solve JavaScript. Source: I was writing a golang TLS fingerprinting one few month ago"}
{"id": "lmmted2", "type": "comment", "parent_id": "t1_lmmix84", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmted2/", "author": "jpjacobpadilla", "created_utc": 1726075019, "score": 8, "content": "The idea for creating this project was to create a layer on top of curl_cffi that handles the HTTP headers. And then I thought that it would be nice to automatically parse the meta tags in HTML responses, since I needed that for one of my own projects, so I added that and some other parsing features to the project!"}
{"id": "lmmjwiw", "type": "comment", "parent_id": "t1_lmmix84", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmmjwiw/", "author": "rik-no", "created_utc": 1726072002, "score": 0, "content": "yes same ques"}
{"id": "lmol7u0", "type": "comment", "parent_id": "t1_lmmdygc", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmol7u0/", "author": "jpjacobpadilla", "created_utc": 1726096069, "score": 6, "content": "I would first see if a dynamic website has a private API that can be used. If it does, then you can just send HTTP requests to their private API (in the same way that the client-side javascript for the website would) to get the data that you want. You may need Playwright or Selenium to do more complex tasks like getting tokens or cookies, but once you've gotten them, in general, I would transfer the cookies/tokens/headers to a requests object and then just go directly to their private API. So to answer your question, I would say that yes you could use my project to scrape a dynamic site, but really any HTTP requests library would do like curl\\_cffi, requests, aio or Stealth-Requests is more geared towards making very realistic browser-based requests like when you click on an \\`a\\` tag or use a browser to load a website. The default headers sent in say the fetch() javascript function are slightly different than the ones sent when using a browser to go to a website, so this project wouldn't be as useful since you would still need to alter the request headers. Just for an example, when going to a website through Chrome, the \\`accept\\` header is \\`text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,\\*/\\*;q=0.8,application/signed-exchange;v=b3;q=0.7\\` but when sending a request via client-side Javascript using fetch(), the default headers set \\`accept\\` to \\`\\*/\\*\\`."}
{"id": "lmo14tp", "type": "comment", "parent_id": "t1_lmmdygc", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmo14tp/", "author": "nextdoorNabors", "created_utc": 1726088957, "score": 2, "content": "Seconding"}
{"id": "lmnag1r", "type": "comment", "parent_id": "t1_lmmted2", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmnag1r/", "author": "NopeNotHB", "created_utc": 1726080427, "score": 3, "content": "That's nice! I will try to use it. Thanks! Edit: I guess I'm gonna start using this since it's basically curl-cffi which I use, but upgraded. Starred!"}
{"id": "lmohnpz", "type": "comment", "parent_id": "t1_lmnag1r", "permalink": "https://www.reddit.com/r/webscraping/comments/1fea485/stay_undetected_while_scraping_the_web_open/lmohnpz/", "author": "jpjacobpadilla", "created_utc": 1726094749, "score": 2, "content": "Thanks! That's exactly why I made it - I use curl\\_cffi a lot (great project) but always had to write lots of code around it to handle the headers, which is really repetitive."}
{"id": "1gbmjvq", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/", "author": "___xXx__xXx__xXx__", "created_utc": 1729832289, "score": 128, "title": "How are you making money from web scraping?", "content": "And more importantly, how much? Are there people (perhaps not here, but in general) making quite a lot of money from web scraping? I consider myself an upper intermediate web scraper. Looking on freelancer sites, it seems I'm competing south Asian people offering what I do for less than minimum wage. How do you cash grab at this?"}
{"id": "ltnmo8b", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnmo8b/", "author": "iaseth", "created_utc": 1729847747, "score": 79, "content": "I make money time to time doing \"background checks\". Earlier this year, an investor paid me a generous amount to find out how many users a startup had. The founder was claiming 20k active users on linkedin. I found an api endpoint that gave details by userid starting with 1. Turned out, they had only 1500 user accounts and less than 100 of them had logged in the last month."}
{"id": "ltn4uve", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltn4uve/", "author": "throwawaylmaoxd123", "created_utc": 1729836213, "score": 26, "content": "I'm not one of those people that are making a lot from web scraping alone. I use it occasionally in my full time work as a Data Scientist but for purely Web Scraping work I had a couple of gigs in the past in web scraping and I usually charge per project instead of per hour. My usual price per project is 400-1000 usd depending on the complexity/size. I live in the Philippines so thats a pretty decent amount of money for a gig."}
{"id": "ltn5w7j", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltn5w7j/", "author": "WebDev_Ben", "created_utc": 1729836835, "score": 14, "content": "- People who need data will pay you for web scraping jobs - data sale - create tools with data + sale membership"}
{"id": "ltoo733", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltoo733/", "author": "JUKELELE-TP", "created_utc": 1729864965, "score": 8, "content": "Some people scrape publicly available data, (often government data) and pack it into a nicely formatted / cleaned dataset. Often the government doesn't provide full datasets but you can search their systems for a specific query and get data for a single instance. E.g. a dataset containing ALL adresses in my country is sold for around 100 euros. Just cheaper for a company to buy than to scrape it themselves."}
{"id": "ltq4imv", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltq4imv/", "author": "Infamous_Land_1220", "created_utc": 1729881074, "score": 6, "content": "Webscraping is a part of what my company does. The business as a whole pays about 120k a month right now. I have a guy that maintains the scraping tools, just making sure that they still work and adding new ones. And I pay him about 4K a month. Im too paranoid to elaborate further, it\u2019s nothing illegal, but I\u2019d rather not comment on any specifics. So yeah, you can realistically get paid about 4K a month just for optimizing webscraping tools."}
{"id": "ltqbbez", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltqbbez/", "author": "WebDev_Ben", "created_utc": 1729883171, "score": 5, "content": "Another way to making money - selling webscraper scripts - maintaining webscraper scripts for clients"}
{"id": "ltqxc4z", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltqxc4z/", "author": "None", "created_utc": 1729890150, "score": 8, "content": "[removed]"}
{"id": "ltqqhsx", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltqqhsx/", "author": "Mubs", "created_utc": 1729887942, "score": 3, "content": "we scrape supply chain data, it's a pillar of our business"}
{"id": "ltttud1", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltttud1/", "author": "smutaduck", "created_utc": 1729937099, "score": 3, "content": "Back in the day (early 2000s) I stumbled across a gig finding every lawyer in Australia. And after that every electrician. It paid good cash for a side gig - the second one, a days work paid for a family holiday. I have a pricing tool I wrote that runs daily for my employer. That job enhanced my reputation in the company."}
{"id": "ltv70ds", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltv70ds/", "author": "fidelivision", "created_utc": 1729957933, "score": 3, "content": "I thought web scraping is generally $0.25 / hour. Maybe I\u2019m competing with bots that do it now."}
{"id": "lu2r6iy", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu2r6iy/", "author": "lowlua", "created_utc": 1730064624, "score": 3, "content": "I've made money selling data I've scraped to linguistics researchers. For the one project I made $1500 to scrape abstracts and titles from scholarly journals. For another I made somewhere in the ballpark of $4000 to randomly sample GitHub repos that were mostly Python and extract all of the comments from the py files. On another I scraped Wikipedia pages from the links on pages that list articles by categories (the purpose was to make a convenience sample for a study comparing sampling methods); I forget how much that one paid. These were all years ago, from 2016-2017. I was in grad school then and pitched my services to faculty and doctoral students. Sometimes they have grant money they need to spend on something and it was easy for them to work out payment to me because I was employed by the university. At my job now I've scraped government and business partner websites a few times to get content the company I work at originally developed but failed to adequately archive."}
{"id": "ltmz9mu", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltmz9mu/", "author": "dannybrown96", "created_utc": 1729833062, "score": 2, "content": "im also curious..."}
{"id": "ltq4m4t", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltq4m4t/", "author": "error1212", "created_utc": 1729881104, "score": 2, "content": "I make decent money blocking web scrapers/crawlers AMA"}
{"id": "ltpraqt", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpraqt/", "author": "siaosiaos", "created_utc": 1729877062, "score": 3, "content": "i interned at a hedge fund. web scraping was the task given to me. i was paid 1.5 usd per hour which was so looow but i was based in the PH so my choices for a paid internship was limited. i was absorbed by the hedge fund and im now earning 4.5 usd per hour. it\u2019s an easy job so i grabbed the chance while im still attending uni. it gets boring with time because everything kinda feels like the same. but i got to work with LLMs for data enrichment and parsing unstructured data so that was nice."}
{"id": "lto0td6", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lto0td6/", "author": "Unhappy_Bathroom_767", "created_utc": 1729855851, "score": 3, "content": "Also you can learn how to make money online and make tools to automatice process to find opportunities like price errors, limited items\u2026 etc. There are people making a lot of money with this."}
{"id": "ltn1b6u", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltn1b6u/", "author": "EarlyPlantain7810", "created_utc": 1729834177, "score": 1, "content": "its hard, like to know as well."}
{"id": "ltocdos", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltocdos/", "author": "None", "created_utc": 1729860766, "score": 1, "content": "[removed]"}
{"id": "ltom0ef", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltom0ef/", "author": "SockYeh", "created_utc": 1729864235, "score": 1, "content": "i also need some of this ;)"}
{"id": "ltsrg0n", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltsrg0n/", "author": "GoingGeek", "created_utc": 1729914986, "score": 1, "content": ":3. im also curious"}
{"id": "ltz6nwy", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltz6nwy/", "author": "naik_g99", "created_utc": 1730014519, "score": 1, "content": "RemindMe! 1 day"}
{"id": "lu10n50", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu10n50/", "author": "Mountain-Concern3967", "created_utc": 1730045879, "score": 1, "content": "Remind me in 24 hours"}
{"id": "lukij4t", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lukij4t/", "author": "None", "created_utc": 1730314966, "score": 1, "content": "[removed]"}
{"id": "ltz7g70", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltz7g70/", "author": "None", "created_utc": 1730015059, "score": 0, "content": "[removed]"}
{"id": "ltna6sb", "type": "comment", "parent_id": "t3_1gbmjvq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltna6sb/", "author": "Middle-Chard-4153", "created_utc": 1729839502, "score": -3, "content": "Information is power. Scraping is money"}
{"id": "ltnnbug", "type": "comment", "parent_id": "t1_ltnmo8b", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnnbug/", "author": "___xXx__xXx__xXx__", "created_utc": 1729848181, "score": 9, "content": "Thanks, that's interesting. How did you get that job, and what did it pay, ballpark, if you don't mind my asking?"}
{"id": "ltubhd4", "type": "comment", "parent_id": "t1_ltnmo8b", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltubhd4/", "author": "Arrival117", "created_utc": 1729946409, "score": 2, "content": "Have you checked other ids or did you just stop at 1500?"}
{"id": "ltywvl5", "type": "comment", "parent_id": "t1_ltnmo8b", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltywvl5/", "author": "SGaba_", "created_utc": 1730008146, "score": 1, "content": "How would you do this? Can you please mention the technology. It's very impressive"}
{"id": "ltnqk43", "type": "comment", "parent_id": "t1_ltn4uve", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnqk43/", "author": "MedicalCellist8802", "created_utc": 1729850252, "score": 1, "content": "what kind of scraping are you doing for that kind of money, just a newb starting out. thanks for any info."}
{"id": "ltnixpa", "type": "comment", "parent_id": "t1_ltn5w7j", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnixpa/", "author": "___xXx__xXx__xXx__", "created_utc": 1729845266, "score": 3, "content": "> People who need data will pay you for web scraping jobs For more than minimum wage? Because I'm not really seeing a lot of that."}
{"id": "ltpkysi", "type": "comment", "parent_id": "t1_ltoo733", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpkysi/", "author": "___xXx__xXx__xXx__", "created_utc": 1729875151, "score": 1, "content": "Where do you sell that?"}
{"id": "ltq73ox", "type": "comment", "parent_id": "t1_ltq4imv", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltq73ox/", "author": "SnuggleWuggleSleep", "created_utc": 1729881873, "score": 1, "content": "How many hours?"}
{"id": "mgxvo89", "type": "comment", "parent_id": "t1_ltq4imv", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/mgxvo89/", "author": "hamideddix", "created_utc": 1741565230, "score": 1, "content": "I am from Iran and for all the sanctions that we just the ordinary people has to suffer from had no boundary what so ever for doing illegal scraping."}
{"id": "mhjp8lb", "type": "comment", "parent_id": "t1_ltq4imv", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/mhjp8lb/", "author": "professorchaosishere", "created_utc": 1741864733, "score": 1, "content": "Does it also scrap LinkedIn?"}
{"id": "m0n5olq", "type": "comment", "parent_id": "t1_ltqxc4z", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/m0n5olq/", "author": "major_MM", "created_utc": 1733451403, "score": 1, "content": "where do you find business need your saas ?"}
{"id": "mhjpcuj", "type": "comment", "parent_id": "t1_ltqxc4z", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/mhjpcuj/", "author": "None", "created_utc": 1741864790, "score": 1, "content": "[removed]"}
{"id": "lu5pfnl", "type": "comment", "parent_id": "t1_ltqqhsx", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu5pfnl/", "author": "BenDemaj", "created_utc": 1730114280, "score": 1, "content": "Yeah it seems there is a potential on both sides, for those scraping and for those blocking scrappers."}
{"id": "lu9xgvp", "type": "comment", "parent_id": "t1_ltq4m4t", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu9xgvp/", "author": "startup_biz_36", "created_utc": 1730163909, "score": 3, "content": "I havent found a site i couldnt scrape yet lmao"}
{"id": "ltq6ul9", "type": "comment", "parent_id": "t1_ltq4m4t", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltq6ul9/", "author": "SnuggleWuggleSleep", "created_utc": 1729881795, "score": 2, "content": "How much do you make and how can I get that job?"}
{"id": "ltr1zb5", "type": "comment", "parent_id": "t1_ltq4m4t", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltr1zb5/", "author": "ShyWillySyndrome", "created_utc": 1729891698, "score": 1, "content": "What would be the first obstacle a noob web scraper potentially could pass given limited knowledge? I find this cat and mouse game very interesting! Good luck keeping us noobs out haha :) Have a good day."}
{"id": "lu5r4w5", "type": "comment", "parent_id": "t1_ltpraqt", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu5r4w5/", "author": "adamjonah", "created_utc": 1730115124, "score": 7, "content": "A hedge fund paying a skilled worker 4.5$/hour is madness to me"}
{"id": "lto4em4", "type": "comment", "parent_id": "t1_lto0td6", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lto4em4/", "author": "___xXx__xXx__xXx__", "created_utc": 1729857501, "score": 2, "content": "The problem I've had with those approaches - the kind of digital panning for gold - is that the ideas are either saturated, or don't work."}
{"id": "ltoeghb", "type": "comment", "parent_id": "t1_ltocdos", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltoeghb/", "author": "webscraping-ModTeam", "created_utc": 1729861552, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "ltz6rgr", "type": "comment", "parent_id": "t1_ltz6nwy", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltz6rgr/", "author": "RemindMeBot", "created_utc": 1730014587, "score": 1, "content": "I will be messaging you in 1 day on [**2024-10-28 07:35:19 UTC**]( to remind you of [**this link**]( [**CLICK THIS LINK**]( to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)]( ***** |[^(Info)]( Reminders)]( |-|-|-|-|"}
{"id": "lwndy8w", "type": "comment", "parent_id": "t1_lukij4t", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lwndy8w/", "author": "None", "created_utc": 1731361746, "score": 1, "content": "[removed]"}
{"id": "ltzjf5p", "type": "comment", "parent_id": "t1_ltz7g70", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltzjf5p/", "author": "___xXx__xXx__xXx__", "created_utc": 1730022940, "score": 2, "content": "I wonder if I can make money by getting ChatGPT to write posts for me on reddit."}
{"id": "ltnivow", "type": "comment", "parent_id": "t1_ltna6sb", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnivow/", "author": "___xXx__xXx__xXx__", "created_utc": 1729845229, "score": 10, "content": "Cool. Anything specific?"}
{"id": "ltnokkr", "type": "comment", "parent_id": "t1_ltnnbug", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnokkr/", "author": "iaseth", "created_utc": 1729848992, "score": 31, "content": "A mutual friend connected us. Around $4k. I have found that a lot of people in the startup world have these kind of \"errands\" and are willing to spend on it, but they can't really post it online as it would make them look bad. Same for us. It is difficult to show off the \"interesting\" stuff we find as webscraping is often a legal gray area."}
{"id": "ltuf3ou", "type": "comment", "parent_id": "t1_ltubhd4", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltuf3ou/", "author": "iaseth", "created_utc": 1729947912, "score": 3, "content": "Their userids started serially from 1. I created a new account and the my user id was around 1400. I then collected all user profiles and matched dates and checked for gaps, and it confirmed that there were only that many users."}
{"id": "ltz29kj", "type": "comment", "parent_id": "t1_ltywvl5", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltz29kj/", "author": "iaseth", "created_utc": 1730011594, "score": 4, "content": "Some websites have a 2-step login. When you enter just your email/username, it loads some details about your account to show you a welcome message like, \"Welcome back John! You are logging in after 10 days. We have new exciting features for you. Please enter password to continue\". For this to work, there has to be an api call that gives some basic user details like name, last-login, first-login, subscription-status, etc without needing the password. This is the one I was able to exploit."}
{"id": "ltnqw4w", "type": "comment", "parent_id": "t1_ltnqk43", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnqw4w/", "author": "throwawaylmaoxd123", "created_utc": 1729850453, "score": 14, "content": "The most common requests (in fiverr) are business info scraping. I don't quite remember the exact term but basically the clients usually provide what type of business info they need, usually in a specific country/region. I just crawl google maps using Selenium to get those info"}
{"id": "ltqapfz", "type": "comment", "parent_id": "t1_ltnixpa", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltqapfz/", "author": "WebDev_Ben", "created_utc": 1729882981, "score": 1, "content": "Freelancer platforms have some jobs but good projects are rarely. Some small businesses willing to pay for data. So far my best jobs was for real estate companies scraping contact data and other information"}
{"id": "lttwg4u", "type": "comment", "parent_id": "t1_ltpkysi", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lttwg4u/", "author": "JUKELELE-TP", "created_utc": 1729938703, "score": 2, "content": "I don\u2019t do it myself but the companies that do just sell that type of data on their websites. It\u2019s perfectly legal here as long as it\u2019s public data that doesn\u2019t fall under GDPR restrictions."}
{"id": "ltqa3ch", "type": "comment", "parent_id": "t1_ltq73ox", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltqa3ch/", "author": "Infamous_Land_1220", "created_utc": 1729882790, "score": 7, "content": "All my employees are remote, so I\u2019m not hundred percent sure, but on paper it\u2019s a full-time position. My guess he probably doesn\u2019t work more than a couple of hours a day. The downside is that if something fails, he has to fix it right away, even if it\u2019s middle of the night, there are tests always running to make sure that apis are online."}
{"id": "mhjrbhv", "type": "comment", "parent_id": "t1_mhjpcuj", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/mhjrbhv/", "author": "webscraping-ModTeam", "created_utc": 1741865717, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "lwo5s7h", "type": "comment", "parent_id": "t1_lwndy8w", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lwo5s7h/", "author": "webscraping-ModTeam", "created_utc": 1731371100, "score": 1, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "ltp6yfq", "type": "comment", "parent_id": "t1_ltnokkr", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltp6yfq/", "author": "Allpurposelife", "created_utc": 1729870880, "score": 2, "content": "How do you find a market that is interested in this? What would the keyword be?"}
{"id": "lu06f86", "type": "comment", "parent_id": "t1_ltuf3ou", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu06f86/", "author": "OfficeSalamander", "created_utc": 1730035395, "score": 2, "content": "Yeah I did this when trying to analyze a competitor some time ago"}
{"id": "ltux6uy", "type": "comment", "parent_id": "t1_ltuf3ou", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltux6uy/", "author": "SuccessfulBee7049", "created_utc": 1729954631, "score": 1, "content": "Yo, can you teach your skills to me, i could not find it anywhere where I could learn how do people scrape enterprise/commercial sites. All i know is how scraping works when it\u2019s without any restrictions. Please :("}
{"id": "lu2lgqe", "type": "comment", "parent_id": "t1_ltz29kj", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu2lgqe/", "author": "BPAnimal", "created_utc": 1730062830, "score": 2, "content": "Damn, sounds like they really failed to correctly design or implement their API authorization scheme. Kudos to you for figuring that out!"}
{"id": "lue7hi2", "type": "comment", "parent_id": "t1_ltz29kj", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lue7hi2/", "author": "SGaba_", "created_utc": 1730228095, "score": 1, "content": "Did you use selenium to automate this?"}
{"id": "ltnsd99", "type": "comment", "parent_id": "t1_ltnqw4w", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltnsd99/", "author": "MedicalCellist8802", "created_utc": 1729851347, "score": 1, "content": "cool thanks, I thought you were running your own business."}
{"id": "ltoinz2", "type": "comment", "parent_id": "t1_ltnqw4w", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltoinz2/", "author": "___xXx__xXx__xXx__", "created_utc": 1729863070, "score": 1, "content": "Sorry to interrogate you, but I'm curious about a few specific things. How many man hours are you putting in to one of those requests? How often do you get one? And how prominent is your fiverr/upwork/whatever profile? On peopleperhour when I look at the webscraping projects (as in freelancers making offers), the top 20% have any sales at all, and not until the top one or two tasks do you see a decent amount of sales."}
{"id": "ltpgex6", "type": "comment", "parent_id": "t1_ltp6yfq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpgex6/", "author": "iaseth", "created_utc": 1729873775, "score": 18, "content": "There is no sure shot way of finding such clients. I have a tendency to brag about what I scraped recently, which often leads to someone asking \"Can you do this?\". If you want to make a living off this, I would suggest you focus on financial data. The market is big, traders are very willing to pay for anything that can give them an edge and often the data you find once can be sold to many people."}
{"id": "luhanli", "type": "comment", "parent_id": "t1_ltp6yfq", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/luhanli/", "author": "thelandofficial", "created_utc": 1730267671, "score": 2, "content": "You should cold outreach to VCs/Investors. Getting their email should be proof enough that you're able to secure info they're looking for"}
{"id": "ltw4vvg", "type": "comment", "parent_id": "t1_ltux6uy", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltw4vvg/", "author": "iaseth", "created_utc": 1729969035, "score": 1, "content": "Try to build scraping projects to solve a problem that you yourself have. I started out because I wanted to scrape all wallpapers from a website. Youtube has some introductory videos and chatgpt helps in one-off scripts, but you'd have to start writing scrapers yourself to learn it."}
{"id": "lu2mv5s", "type": "comment", "parent_id": "t1_lu2lgqe", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu2mv5s/", "author": "BPAnimal", "created_utc": 1730063263, "score": 2, "content": "Now that I think of it, it kind of reminds me of this incident."}
{"id": "lugyeno", "type": "comment", "parent_id": "t1_lue7hi2", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lugyeno/", "author": "iaseth", "created_utc": 1730261004, "score": 3, "content": "No. This did not require any rendering."}
{"id": "ltp0huj", "type": "comment", "parent_id": "t1_ltnsd99", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltp0huj/", "author": "Toilet-B0wl", "created_utc": 1729868887, "score": 4, "content": "I worked for a data collection company and these were my tasks anyway - get the location data and hours for all the walmart pharmacy locations in this state. Collect all the spotify job postings for a certain position - this kind of thing."}
{"id": "ltok19o", "type": "comment", "parent_id": "t1_ltoinz2", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltok19o/", "author": "throwawaylmaoxd123", "created_utc": 1729863556, "score": 3, "content": "Its fine I like answering questions about my work. > How many man hours ... A lot of hours. Mainly due to the fact that I manually monitor my crawlers. Its just sometimes things go wrong regardless of how many catch you have. > How often do you get one When I was just starting it took me more than a year to actually land a web crawling job then after my first successful project, new requests will come a few times every month but not all of them result to anything. Sometimes I reject jobs that I think are beyond my skills, like crawling tiktok content for example, sometimes clients just find someone else better, etc. Its not stable but I would say in my peak Fiverr days I could get a project every two or three months. I'm not sure how to answer your question on profile prominence. I havent done fiverr in a year since I'm focusing on my full time job"}
{"id": "ltpguip", "type": "comment", "parent_id": "t1_ltpgex6", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpguip/", "author": "Allpurposelife", "created_utc": 1729873910, "score": -10, "content": "Brah, what? I asked for the keyword? There is always some sure shot because anything can be measured\u2026 how did the investor come to find you out is what I\u2019m really asking."}
{"id": "ltwf4ie", "type": "comment", "parent_id": "t1_ltw4vvg", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltwf4ie/", "author": "None", "created_utc": 1729972407, "score": 1, "content": "[removed]"}
{"id": "lu85a4d", "type": "comment", "parent_id": "t1_ltok19o", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu85a4d/", "author": "Toastedpubes", "created_utc": 1730143459, "score": 1, "content": "Hell yeah another Selenium user"}
{"id": "ltpii03", "type": "comment", "parent_id": "t1_ltpguip", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpii03/", "author": "iaseth", "created_utc": 1729874413, "score": 4, "content": "Not sure how to answer this. I had an exam results dataset and was helping my friend with resume verification for internship applicants. He connected me to the client."}
{"id": "lu06d8z", "type": "comment", "parent_id": "t1_ltpguip", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/lu06d8z/", "author": "OfficeSalamander", "created_utc": 1730035372, "score": 1, "content": "There is no keyword. He\u2019s said, many many times over that there\u2019s no one singular way he gets clients"}
{"id": "ltwnfnf", "type": "comment", "parent_id": "t1_ltwf4ie", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltwnfnf/", "author": "webscraping-ModTeam", "created_utc": 1729975118, "score": 1, "content": "Please review the sub rules"}
{"id": "ltpit61", "type": "comment", "parent_id": "t1_ltpii03", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpit61/", "author": "Allpurposelife", "created_utc": 1729874506, "score": -10, "content": "That\u2019s a great answer! That\u2019s what I\u2019m talking about! Why did he connect you to or think you could help him?"}
{"id": "ltpnw1w", "type": "comment", "parent_id": "t1_ltpit61", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpnw1w/", "author": "iaseth", "created_utc": 1729876034, "score": 7, "content": "Yes"}
{"id": "ltpqf8s", "type": "comment", "parent_id": "t1_ltpnw1w", "permalink": "https://www.reddit.com/r/webscraping/comments/1gbmjvq/how_are_you_making_money_from_web_scraping/ltpqf8s/", "author": "Allpurposelife", "created_utc": 1729876800, "score": -9, "content": "Pfft English or Spanish"}
{"id": "1ip0jvj", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/", "author": "recdegem", "created_utc": 1739500845, "score": 125, "title": "The first rule of web scraping is...", "content": "The first rule of web scraping is... do NOT talk about web scraping! But if you must spill the beans, you've found your tribe. Just remember: when your script crashes for the 47th time today, it's not you - it's Cloudflare, bots, and the other 900 sites you\u2019re stealing from. Welcome to the club!"}
{"id": "mcpjfvu", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcpjfvu/", "author": "RobSm", "created_utc": 1739528192, "score": 65, "content": "?? Who is stealing what? If I put my website online, I give my data to the public voluntarily. I always have option to disable my website and no-one will get anything from me."}
{"id": "mcwi48v", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcwi48v/", "author": "macmany", "created_utc": 1739628620, "score": 7, "content": "Lol I had about 17 years of flawless scraping of which happened to kill over yesterday. I quickly checked the source, and there was an access denied message. It was such a minuscule amount of data, so I rebuilt it in 2 hours. I remember thinking if this breaks again in a week\u2019s time, then I\u2019m going to get annoyed. Haha"}
{"id": "mcvci1h", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcvci1h/", "author": "kobaasama", "created_utc": 1739605594, "score": 5, "content": "Parsing html since birth."}
{"id": "mcvdzu6", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcvdzu6/", "author": "graph-crawler", "created_utc": 1739606529, "score": 5, "content": "Nobody's stealing anything, we just copy."}
{"id": "md0l14q", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/md0l14q/", "author": "fasti-au", "created_utc": 1739676408, "score": 3, "content": "It is illegal until its profitable"}
{"id": "mco69rn", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mco69rn/", "author": "matty_fu", "created_utc": 1739502999, "score": 2, "content": "what"}
{"id": "md3dh0v", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/md3dh0v/", "author": "DENSELY_ANON", "created_utc": 1739723423, "score": 2, "content": "this is a fabulous post. GL to all the Scrapers out there."}
{"id": "mevrqiz", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mevrqiz/", "author": "Corgi-Ancient", "created_utc": 1740576695, "score": 2, "content": "captchas are our daily puzzles and ip bans are our badges of honor!"}
{"id": "mcuciwe", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcuciwe/", "author": "Ariwawa", "created_utc": 1739587927, "score": 1, "content": "Check the robots.txt file"}
{"id": "md6334c", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/md6334c/", "author": "zeamp", "created_utc": 1739752975, "score": 1, "content": "Cool."}
{"id": "md79gl4", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/md79gl4/", "author": "brukutu10", "created_utc": 1739768860, "score": 1, "content": "Why would the average person wanna scrape the whole internet ? I understand a few cases in between such as the \u201cTime Machine\u201d etc. but what\u2019s the interest in it so much?"}
{"id": "mequqln", "type": "comment", "parent_id": "t3_1ip0jvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mequqln/", "author": "Current_Perception39", "created_utc": 1740508809, "score": 1, "content": "What's the best way to scrape emails?"}
{"id": "mcplzo1", "type": "comment", "parent_id": "t1_mcpjfvu", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcplzo1/", "author": "UnlikelyLikably", "created_utc": 1739529736, "score": -32, "content": "Ever heard of copyright?"}
{"id": "mcwaixj", "type": "comment", "parent_id": "t1_mcvdzu6", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcwaixj/", "author": "None", "created_utc": 1739625622, "score": -2, "content": "[deleted]"}
{"id": "mevrrrm", "type": "comment", "parent_id": "t1_mevrqiz", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mevrrrm/", "author": "haikusbot", "created_utc": 1740576708, "score": 2, "content": "*Captchas are our daily* *Puzzles and ip bans are our* *Badges of honor!* \\- Corgi-Ancient --- ^(I detect haikus. And sometimes, successfully.) ^[Learn&#32;more&#32;about&#32;me.]( ^(Opt out of replies: \"haikusbot opt out\" | Delete my comment: \"haikusbot delete\")"}
{"id": "mct3ab3", "type": "comment", "parent_id": "t1_mcplzo1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mct3ab3/", "author": "None", "created_utc": 1739571659, "score": 31, "content": "[deleted]"}
{"id": "mcpzqmy", "type": "comment", "parent_id": "t1_mcplzo1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcpzqmy/", "author": "ZMech", "created_utc": 1739536920, "score": 29, "content": "You mean the right to not have your work copied? Sure. Scraping content to republish it as your own would violate that (like some AI art legal cases), but using scraped data to make a business decision doesn't."}
{"id": "mcrigw8", "type": "comment", "parent_id": "t1_mcplzo1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcrigw8/", "author": "its_a_gibibyte", "created_utc": 1739554777, "score": 8, "content": "Copyright applies to reselling creative, not using them. Otherwise, people wouldnt be able to read Harry Potter unless they own the copyright. How were you expecting people to visit websites in the first place?"}
{"id": "mcptv81", "type": "comment", "parent_id": "t1_mcplzo1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcptv81/", "author": "matty_fu", "created_utc": 1739534165, "score": 9, "content": "Do CDNs perform copyright violation when they store an HTML document and serve it from their cache?"}
{"id": "mcpwbk6", "type": "comment", "parent_id": "t1_mcplzo1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcpwbk6/", "author": "RobSm", "created_utc": 1739535364, "score": 5, "content": "You don't post copyright on the public website. And if you do, then you allow http request recipient to receive it. Your webserver is built that way. Ever heard of status 200?"}
{"id": "mcscg2h", "type": "comment", "parent_id": "t1_mcplzo1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcscg2h/", "author": "PeachScary413", "created_utc": 1739563571, "score": 1, "content": "It's the year of our lord 2025.. imagine caring about copyright"}
{"id": "md4ugkp", "type": "comment", "parent_id": "t1_mcwaixj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/md4ugkp/", "author": "temptuer", "created_utc": 1739738759, "score": 0, "content": "Yeah?"}
{"id": "mct82nl", "type": "comment", "parent_id": "t1_mct3ab3", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mct82nl/", "author": "UnlikelyLikably", "created_utc": 1739573203, "score": -5, "content": "Yeaaah, not in EU."}
{"id": "mcpxtnr", "type": "comment", "parent_id": "t1_mcpwbk6", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcpxtnr/", "author": "UnlikelyLikably", "created_utc": 1739536061, "score": -18, "content": "So what youre saying is that everything that is public doesn't belong to anyone :D congrats mate, you won the bullshit award 2025"}
{"id": "mctldmq", "type": "comment", "parent_id": "t1_mcpxtnr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mctldmq/", "author": "IreplyToIncels", "created_utc": 1739577797, "score": 5, "content": "Man you really crashed out in this thread"}
{"id": "mcv2446", "type": "comment", "parent_id": "t1_mcpxtnr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcv2446/", "author": "iCameToLearnSomeCode", "created_utc": 1739599515, "score": 3, "content": "I'm starting to think you're not a copyright lawyer at all. It's starting to sound like you're just completely making things up off the top of your head."}
{"id": "mcq0tip", "type": "comment", "parent_id": "t1_mcpxtnr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ip0jvj/the_first_rule_of_web_scraping_is/mcq0tip/", "author": "RobSm", "created_utc": 1739537388, "score": 5, "content": "No, you are just too stupid to understand what is being said. The content belongs to website owner, he chooses to share it with the world. You are too young to understand the meaning of internet."}
{"id": "1gfmadf", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/", "author": "the_bigbang", "created_utc": 1730294229, "score": 120, "title": "27.6% of the Top 10 Million Sites Are Dead", "content": "In a recent project, I ran a high-performance web scraper to analyze the top 10 million domains\u2014and the results are surprising: over a quarter of these sites (27.6%) are inactive or inaccessible. This research dives into the infrastructure needed to process such a massive dataset, the technical approach to handling 16,667 requests per second, and the significance of \"dead\" sites in our rapidly shifting web landscape. Whether you're into large-scale scraping, Redis queue management, or DNS optimization, this deep dive has something for you. Check out the full write-up and leave your feedback here Full [article]( & [code]("}
{"id": "luipoy3", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luipoy3/", "author": "None", "created_utc": 1730295470, "score": 30, "content": "Bruh I imagine you just pissed cloudflare off because you basically just tried to perform a ddos."}
{"id": "lujg2pb", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lujg2pb/", "author": "Classic-Dependent517", "created_utc": 1730303621, "score": 16, "content": "Are you sure those websites are inactive/dead ? Any chances your scraper just got detected and the web servers are not returning any responses? Because my webservers also dont respond or return non found response to make scrapers believe my domain does not exist"}
{"id": "luiombq", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luiombq/", "author": "Bedbathnyourmom", "created_utc": 1730295107, "score": 5, "content": "Did I hear dead internet? Go on, do tell!"}
{"id": "lujd198", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lujd198/", "author": "p3r3lin", "created_utc": 1730302723, "score": 5, "content": "I applaud the effort! But without knowing how DomCop compiled their data set, this has very little significance. The linked file lists 10mil domains ranked by OpenPageRank. But are these really the TOP 10mil page ranks of all domains worldwide \u200d\u2640\ufe0f? But even though the method and data quality is debatable I agrre with the base premise: dont trust data/information that is relevant to you to third parties you have no control over."}
{"id": "luiniog", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luiniog/", "author": "salestoolsss", "created_utc": 1730294729, "score": 3, "content": "NICE"}
{"id": "luji7ch", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luji7ch/", "author": "None", "created_utc": 1730304266, "score": 3, "content": "When you say dead, are you saying parked pages?"}
{"id": "lujhup9", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lujhup9/", "author": "Similar-Attorney-656", "created_utc": 1730304152, "score": 2, "content": "Have you do sanity check to verify your outcome?"}
{"id": "luln3ux", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luln3ux/", "author": "Worldly_Water_911", "created_utc": 1730327105, "score": 2, "content": "Are you using AWS for hosting, I'm confused at how you were able to run all this and this many workers for $50."}
{"id": "lum9gc4", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lum9gc4/", "author": "georgehotelling", "created_utc": 1730334577, "score": 2, "content": "You might want to tweak your syntax highlighting."}
{"id": "lujquf7", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lujquf7/", "author": "StarTop5606", "created_utc": 1730306862, "score": 1, "content": "Phase 2 running the entire .com zone file? Awesome stuff."}
{"id": "lujshhc", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lujshhc/", "author": "jibbscat", "created_utc": 1730307344, "score": 1, "content": "More like the_bigWang , amirite"}
{"id": "lunphf0", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunphf0/", "author": "juannikin", "created_utc": 1730356564, "score": 1, "content": "This is amazing. Thanks so much for sharing!!"}
{"id": "lunv9bx", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunv9bx/", "author": "RobSm", "created_utc": 1730360580, "score": 1, "content": "And how many are dead from Bottom 10 Million? 99%?"}
{"id": "lusqjia", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lusqjia/", "author": "Adam302", "created_utc": 1730427661, "score": 1, "content": "seems you didnt account for parked/for-sale domains - they are just as 'dead' as domains that do not resolve."}
{"id": "lusr1g3", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lusr1g3/", "author": "Adam302", "created_utc": 1730427861, "score": 1, "content": "I'm curious to know. Does a random sample of 1000 of those domains yield a similar percentage of dead domains?"}
{"id": "lutxgw0", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lutxgw0/", "author": "acgfbr", "created_utc": 1730453002, "score": 1, "content": "your code is really great but you are not using a headless browser and proxies, just a HTTP GET request is not good enough to validate this kind of stuff friend req, err := \"GET\", \" nil)"}
{"id": "luv23nh", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luv23nh/", "author": "deeper-diver", "created_utc": 1730471452, "score": 1, "content": "What criteria makes it to that \u201ctop 10 million domains\u201d? If it\u2019s a top domain, that to me says it\u2019s popular. But if it\u2019s inactive or inaccessible, that tells me no one uses it. Can someone please explain?"}
{"id": "luxmjh8", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luxmjh8/", "author": "Expert-Garbage-8817", "created_utc": 1730501177, "score": 1, "content": "I am a webmaster who gets some idiots hammering my sites with 100-1000+ hits a second. They all blocked for 24 hours and if they come back often - banned for 30, 90, then 180 and then 365 days. I post frequency of the visits in my robots.txt files. If you ignoring it - my sites will be in those 27% of dead sites. Optimize, optimize and once again optimize your crawler."}
{"id": "lv4mujv", "type": "comment", "parent_id": "t3_1gfmadf", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lv4mujv/", "author": "bhushankumar_fst", "created_utc": 1730605745, "score": 0, "content": "Most likely possible that it is because of firewall security and other bot detection techniques. Are you rotating proxy while each request?"}
{"id": "luj0qnt", "type": "comment", "parent_id": "t1_luipoy3", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luj0qnt/", "author": "Morstraut64", "created_utc": 1730298999, "score": 6, "content": "I see so many scrapers hammering websites spidering/scraping. That's the quickest way to get a tiny fraction of data. There's no harm in slowing down and randomizing sleep time between requests. I haven't read the linked post but I imagine all of those requests are hitting different sites rather than the same. But to your point Cloudflare would definitely be aware of the volume and block a significant number of requests."}
{"id": "lulq23u", "type": "comment", "parent_id": "t1_luipoy3", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lulq23u/", "author": "Particular-Sea2005", "created_utc": 1730328090, "score": 7, "content": "User Agent: TenMillionDomainsBot"}
{"id": "lunfmxm", "type": "comment", "parent_id": "t1_lujg2pb", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunfmxm/", "author": "the_bigbang", "created_utc": 1730350561, "score": 4, "content": "It queries a group of DNS servers first; about 19% of the 10M have no DNS records. The rest result in timeouts, 404, and 5xx errors. So the more accurate result falls between 19% and 27.6%, closer to the latter in reality, given that the top 10M might be aggregated based on historical data from Common Crawl that could date back 5 years or even longer"}
{"id": "lul6rjh", "type": "comment", "parent_id": "t1_lujg2pb", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lul6rjh/", "author": "NicCage4life", "created_utc": 1730322043, "score": 3, "content": "Is there a dataset available?"}
{"id": "lun08bq", "type": "comment", "parent_id": "t1_luji7ch", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lun08bq/", "author": "the_bigbang", "created_utc": 1730343876, "score": 1, "content": "The majority of the \"dead\" are those with no DNS records at all. Parked pages are not treated as dead, as they have a \"website\". So the number could be higher if those were included."}
{"id": "lumzfrq", "type": "comment", "parent_id": "t1_luln3ux", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lumzfrq/", "author": "the_bigbang", "created_utc": 1730343572, "score": 1, "content": "It's hosted on Rackspace Spot, which is about 100 times cheaper. You can checkout how cheap they could be [here]("}
{"id": "lumx2v1", "type": "comment", "parent_id": "t1_lum9gc4", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lumx2v1/", "author": "giwidouggie", "created_utc": 1730342691, "score": 2, "content": "I saw that too and was like \"What in the indentation-hell is this?\""}
{"id": "lumzoz4", "type": "comment", "parent_id": "t1_lum9gc4", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lumzoz4/", "author": "the_bigbang", "created_utc": 1730343669, "score": 1, "content": "Sorry for the inconvenience, will fix it soon"}
{"id": "lunezr0", "type": "comment", "parent_id": "t1_lum9gc4", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunezr0/", "author": "the_bigbang", "created_utc": 1730350232, "score": 1, "content": "u/georgehotelling u/giwidouggie Hey, Just updated, could you please double check if it's better this time? If not could you please let me know the step to reproduce it, like what extension or config to change your light grey (I guess) background? Thanks"}
{"id": "lun109z", "type": "comment", "parent_id": "t1_lujquf7", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lun109z/", "author": "the_bigbang", "created_utc": 1730344172, "score": 1, "content": "Thanks for your idea, that would be interesting to dive deeply into it"}
{"id": "lun1578", "type": "comment", "parent_id": "t1_lujshhc", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lun1578/", "author": "the_bigbang", "created_utc": 1730344226, "score": 2, "content": "lolll, I'm fan of The Big Bang Theory"}
{"id": "lusu2wr", "type": "comment", "parent_id": "t1_lusqjia", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lusu2wr/", "author": "the_bigbang", "created_utc": 1730429100, "score": 1, "content": "Yeah, the number could be much higher if include that. It's quite challenging to figure out a simple solution for the parked or for-sale domains since there are quite a lot of different providers"}
{"id": "lusudve", "type": "comment", "parent_id": "t1_lusr1g3", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lusudve/", "author": "the_bigbang", "created_utc": 1730429217, "score": 1, "content": "ofc, the result of the sample will be closer to the full dataset as the sample gets bigger"}
{"id": "luua5rq", "type": "comment", "parent_id": "t1_lutxgw0", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luua5rq/", "author": "the_bigbang", "created_utc": 1730460580, "score": 2, "content": "The resources consumed by a headless browser are about hundreds of times higher than those of a simple HTTP request. The error might be caused by anti-bot mechanisms in less than 1% of cases, as you can see from the error rate of 403 status codes. Though some may use more sophisticated anti-bot policies to detect even headless browsers, given the percentage, it's not worth the time"}
{"id": "lvagbb5", "type": "comment", "parent_id": "t1_lv4mujv", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lvagbb5/", "author": "the_bigbang", "created_utc": 1730688897, "score": 1, "content": "Firewall blocking typically results in a 401 or 403 status code, but these responses are not treated as 'dead' in my case. Other status codes may also be returned depending on specific reasons. The proportion of 404 and 5xx errors is minimal\u2014around 1% of the 10 million requests\u2014and has no significant impact on the overall conclusion"}
{"id": "lujcmhe", "type": "comment", "parent_id": "t1_luj0qnt", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lujcmhe/", "author": "HelloYesThisIsFemale", "created_utc": 1730302600, "score": 4, "content": "Adding random sleeps would cost CPU time and you'd probably have to add a heck of a sleep because even 10 requests per second is probably more than normal."}
{"id": "lunu086", "type": "comment", "parent_id": "t1_lunfmxm", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunu086/", "author": "scrapecrow", "created_utc": 1730359679, "score": 5, "content": "So how did you classify 404 and 5xx errors as those can sometimes mean scraper blocking. Though I'd imagine that wouldn't be a major skew on the entire dataset as most small domains don't care about scraping."}
{"id": "lunfv80", "type": "comment", "parent_id": "t1_lul6rjh", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunfv80/", "author": "the_bigbang", "created_utc": 1730350681, "score": 2, "content": "Yes, please check the article for the dataset, which is compiled by DomCop, and the source code for implementation."}
{"id": "luo2hql", "type": "comment", "parent_id": "t1_lunezr0", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luo2hql/", "author": "giwidouggie", "created_utc": 1730365766, "score": 1, "content": "unchanged for me. I'm on Firefox for Manjaro 131.0.3. Even disabling all add-ons/plugins (including ad blockers) does not affect appearance..... weird"}
{"id": "luog341", "type": "comment", "parent_id": "t1_lunezr0", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luog341/", "author": "georgehotelling", "created_utc": 1730373896, "score": 1, "content": "No change. I'm on Firefox on macOS, dark mode."}
{"id": "lunfgjk", "type": "comment", "parent_id": "t1_lun109z", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunfgjk/", "author": "StarTop5606", "created_utc": 1730350469, "score": 1, "content": "Actually you could run all zones fairly easy. It's 1 click apply on ICANN."}
{"id": "lutfb4p", "type": "comment", "parent_id": "t1_lusu2wr", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lutfb4p/", "author": "Adam302", "created_utc": 1730439885, "score": 1, "content": "I would log the http status code, 30x location, IP address... You can filter out a high percentage of parked domains with that. I.e. exclude Bodis, afternic etc redirects, you can probably work that out just by grouping the 30x domains by popularity"}
{"id": "lutfyk9", "type": "comment", "parent_id": "t1_lusudve", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lutfyk9/", "author": "Adam302", "created_utc": 1730440317, "score": 1, "content": "1000 is often considered large enough for most sampling , so just wondered"}
{"id": "lulqain", "type": "comment", "parent_id": "t1_lujcmhe", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lulqain/", "author": "Morstraut64", "created_utc": 1730328168, "score": 1, "content": "That depends on what you are doing. If you are trying to fly under the radar scraping one website you might want to use a couple of VMs that wait from a few milliseconds to a few seconds. I've done that a great number of times to great success. If they detected I was pulling a bunch of data they never tried stopping me."}
{"id": "lunv5fi", "type": "comment", "parent_id": "t1_lunu086", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunv5fi/", "author": "the_bigbang", "created_utc": 1730360502, "score": 1, "content": "Blocking usually returns 401 or 403, though they may return other status codes for various reasons. The percentage of 404 and 5xx errors is around 1% of the 10M, quite a small portion without any significant impact on the final conclusion"}
{"id": "lupgz9n", "type": "comment", "parent_id": "t1_luog341", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lupgz9n/", "author": "the_bigbang", "created_utc": 1730387529, "score": 1, "content": "u/giwidouggie u/georgehotelling Thanks for your feedback, updated , hope it works this time ["}
{"id": "lungs2i", "type": "comment", "parent_id": "t1_lunfgjk", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lungs2i/", "author": "the_bigbang", "created_utc": 1730351174, "score": 1, "content": "Can you elaborate on that a little bit? As I understand, ICANN provides a webpage to query registration data, but it\u2019s not feasible to use it for such a large number of requests. That\u2019s why I chose multiple public DNS servers to reduce pressure on any single server"}
{"id": "lutprsn", "type": "comment", "parent_id": "t1_lutfyk9", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lutprsn/", "author": "the_bigbang", "created_utc": 1730447375, "score": 1, "content": "I see. Processing 10 million data is quite cheap and fast in my case, so there\u2019s no need to sample it"}
{"id": "luphbrq", "type": "comment", "parent_id": "t1_lupgz9n", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luphbrq/", "author": "georgehotelling", "created_utc": 1730387641, "score": 2, "content": "Yup, I'm seeing dark mode now."}
{"id": "lurj9yl", "type": "comment", "parent_id": "t1_lupgz9n", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lurj9yl/", "author": "giwidouggie", "created_utc": 1730411197, "score": 2, "content": "nice!"}
{"id": "luz63sm", "type": "comment", "parent_id": "t1_lupgz9n", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/luz63sm/", "author": "Grouchy_Brain_1641", "created_utc": 1730524461, "score": 0, "content": "Good enough for the girls I go out with."}
{"id": "lunmcqh", "type": "comment", "parent_id": "t1_lungs2i", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunmcqh/", "author": "StarTop5606", "created_utc": 1730354500, "score": 2, "content": "Make an account and fill out a form click all the TLDs you want. Once they approve you, you can download the daily files for each TLD."}
{"id": "lunvce2", "type": "comment", "parent_id": "t1_lunmcqh", "permalink": "https://www.reddit.com/r/webscraping/comments/1gfmadf/276_of_the_top_10_million_sites_are_dead/lunvce2/", "author": "the_bigbang", "created_utc": 1730360643, "score": 1, "content": "that's amazing, thanks very much for your info!"}
{"id": "1j2wi03", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/", "author": "Excellent-Two1178", "created_utc": 1741043865, "score": 112, "title": "Create web scrapers using AI", "content": "just launched a free website today that lets you generate web scrapers in seconds for free. Right now, it's tailored for JavaScript-based scraping You can create a scraper with a simple prompt or a custom schema-your choice! I've also added a community feature where users can share their scripts, vote on the best ones, and search for what others have built. Since it's brand new as of today, there might be a few hiccups-I'm open to feedback and suggestions for improvements! The first three uses are free (on me!), but after that, you'll need your own Claude API key to keep going. The free uses use 3.5 haiku, but I recommend selecting a better model on the settings page after entering api key. Check it out and let me know what you think! Link :"}
{"id": "mg2ri13", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg2ri13/", "author": "EconomySuch7621", "created_utc": 1741140870, "score": 3, "content": "Great app, OP! What stack did you use? I have a similar project, but I built it with Streamlit since I don\u2019t know much about front-end. I'm looking for a framework to learn and use for small projects."}
{"id": "mfvo06m", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvo06m/", "author": "trueliberator", "created_utc": 1741049005, "score": 2, "content": "Thank you! I needed this to get my OpenScroll.me app rolling faster. Need chatgpt, grok etc. Convos saved to .json hopefully this will sopes up my cumbersome process"}
{"id": "mfwu7ys", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfwu7ys/", "author": "throw_away_17381", "created_utc": 1741064230, "score": 2, "content": "Really impressive job well done :)"}
{"id": "mfwuu4d", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfwuu4d/", "author": "None", "created_utc": 1741064505, "score": 2, "content": "This is nice. Is it possible to use Ollama with this?"}
{"id": "mfx94vr", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfx94vr/", "author": "Excellent-Two1178", "created_utc": 1741071832, "score": 2, "content": "Thank you to everybody for the support so far! I just started coding this project ~24 hours ago, so please bear with me. Quick update: the first three uses I cover now use 3.7 Sonnet instead of 3.5 Haiku\u2014it\u2019s a lot more reliable for scraper generation. With that being said, here are my current upcoming plans: - Add support for browser-based fetching of websites to make browser scraping scripts for trickier sites. - Improve error handling\u2014bad proxies, AI API providers hitting rate limits, or APIs being overloaded can cause problems, and I don\u2019t do a good job letting the person know what\u2019s up. - I need to get new proxies. If anybody has feedback or suggestions, it\u2019s much appreciated!"}
{"id": "mfz4w70", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfz4w70/", "author": "Fabulous_Custard7047", "created_utc": 1741102829, "score": 2, "content": "haha was just looking for one of these, godsend"}
{"id": "mg3g338", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg3g338/", "author": "PreparationOutside49", "created_utc": 1741149484, "score": 2, "content": "Wow"}
{"id": "mg3trtv", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg3trtv/", "author": "StoicTexts", "created_utc": 1741155646, "score": 2, "content": "Really great job man. I\u2019ve been scraping a while and this is stellar. Would love to know more about how you were able to make this? I recently build a site the scrapes a lot of data and then posts the analytics to my backend. Would love to kick ideas around"}
{"id": "mg95w1o", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg95w1o/", "author": "Excellent-Two1178", "created_utc": 1741224541, "score": 2, "content": "Just added a new feature. You can now use a browser to analyze a websites requests, and get a breakdown of each request with an example code snippet, as well as generate a script to automate a websites api directly."}
{"id": "mfvbbum", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvbbum/", "author": "DmitryPapka", "created_utc": 1741044836, "score": 1, "content": "Application error: a client-side exception has occurred while loading [www.scriptsage.xyz]( (see the browser console for more information)."}
{"id": "mfvgyyc", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvgyyc/", "author": "None", "created_utc": 1741046703, "score": 1, "content": "[deleted]"}
{"id": "mfxkld7", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfxkld7/", "author": "4Spartah", "created_utc": 1741078891, "score": 1, "content": "Just tried it out and it failed miserably... I pressed the Start Scraping button and nothing was loading, so I pressed it few times in some intervals and then I got informed that I used all the free points... No errors or anything."}
{"id": "mfxuy5b", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfxuy5b/", "author": "Befreeman", "created_utc": 1741085213, "score": 1, "content": "Nothing happens when hit scraping."}
{"id": "mfy8sw9", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfy8sw9/", "author": "ProgrammerForsaken45", "created_utc": 1741092003, "score": 1, "content": "Can we scrape Linkedin Posts interaction by inputting the cookies ?"}
{"id": "mfyir3h", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfyir3h/", "author": "hyma", "created_utc": 1741095790, "score": 1, "content": "Does it have any mitigation for bot blocking?"}
{"id": "mgana7q", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mgana7q/", "author": "None", "created_utc": 1741247147, "score": 1, "content": "[removed]"}
{"id": "mgenpsb", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mgenpsb/", "author": "_marcuth", "created_utc": 1741298428, "score": 1, "content": "Muito legal. Eu estava tentando criar uma plataforma de web scraping baseada em uma biblioteca que eu venho desenvolvendo que define modelos de parsing e transforma\u00e7\u00e3o de dados... mas at\u00e9 agora n\u00e3o saiu algo t\u00e3o bom xD"}
{"id": "mgpge50", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mgpge50/", "author": "nazrultohin", "created_utc": 1741452291, "score": 1, "content": "Thank you"}
{"id": "mhn6hga", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mhn6hga/", "author": "None", "created_utc": 1741903024, "score": 1, "content": "[removed]"}
{"id": "n06x24f", "type": "comment", "parent_id": "t3_1j2wi03", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/n06x24f/", "author": "Infinite-Ask5534", "created_utc": 1751086319, "score": 1, "content": "what if I made an ai that made a SaaS to make ai's that web scrape... using ai?"}
{"id": "mg31c5t", "type": "comment", "parent_id": "t1_mg2ri13", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg31c5t/", "author": "Excellent-Two1178", "created_utc": 1741144127, "score": 1, "content": "NextJs. It\u2019s great for small projects since you can easily build full stack in a single repo. At scale you probably should host backend separately though since vercel can get quite expensive"}
{"id": "mfwznkg", "type": "comment", "parent_id": "t1_mfwu7ys", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfwznkg/", "author": "Excellent-Two1178", "created_utc": 1741066789, "score": 1, "content": "Thank you much appreciated"}
{"id": "mfwzlff", "type": "comment", "parent_id": "t1_mfwuu4d", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfwzlff/", "author": "Excellent-Two1178", "created_utc": 1741066759, "score": 1, "content": "It should be possible to use all models and I can definitely add! Just will likely require a bit of work on my end to get it working well consistently."}
{"id": "mfxijmo", "type": "comment", "parent_id": "t1_mfx94vr", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfxijmo/", "author": "d3rf0x", "created_utc": 1741077572, "score": 1, "content": "login options for sites that you need to login to scrape ex: linkedin, youtube, google etc"}
{"id": "mfznr9v", "type": "comment", "parent_id": "t1_mfx94vr", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfznr9v/", "author": "Excellent-Two1178", "created_utc": 1741108031, "score": 1, "content": "Just upgraded Proxies\u2019s to some non mid resis. Should perform a bit better sites w heavy antibot protection now"}
{"id": "mg5slx1", "type": "comment", "parent_id": "t1_mg3trtv", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg5slx1/", "author": "None", "created_utc": 1741188838, "score": 1, "content": "[removed]"}
{"id": "mfvbqsx", "type": "comment", "parent_id": "t1_mfvbbum", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvbqsx/", "author": "Excellent-Two1178", "created_utc": 1741044978, "score": 1, "content": "Man sorry fixing. Should be good in few min"}
{"id": "mfvlvae", "type": "comment", "parent_id": "t1_mfvgyyc", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvlvae/", "author": "Excellent-Two1178", "created_utc": 1741048313, "score": 2, "content": "Any suggestions? Believe this is just what nextauth uses by default"}
{"id": "mfxuwmx", "type": "comment", "parent_id": "t1_mfxkld7", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfxuwmx/", "author": "Befreeman", "created_utc": 1741085189, "score": 1, "content": "Same"}
{"id": "mg1k38j", "type": "comment", "parent_id": "t1_mfxkld7", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg1k38j/", "author": "thatapanydude", "created_utc": 1741127029, "score": 1, "content": "I had this too, have no free points left!"}
{"id": "mfz2w9s", "type": "comment", "parent_id": "t1_mfyir3h", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfz2w9s/", "author": "Excellent-Two1178", "created_utc": 1741102260, "score": 2, "content": "Some but it could use more. The proxies I\u2019m using right now are also some not so good resis"}
{"id": "mgboo9k", "type": "comment", "parent_id": "t1_mgana7q", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mgboo9k/", "author": "webscraping-ModTeam", "created_utc": 1741267220, "score": 1, "content": "Please review the sub rules"}
{"id": "mhnurdr", "type": "comment", "parent_id": "t1_mhn6hga", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mhnurdr/", "author": "webscraping-ModTeam", "created_utc": 1741910573, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "mfx106r", "type": "comment", "parent_id": "t1_mfwzlff", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfx106r/", "author": "None", "created_utc": 1741067467, "score": 1, "content": "alright"}
{"id": "mfx10oa", "type": "comment", "parent_id": "t1_mfwzlff", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfx10oa/", "author": "None", "created_utc": 1741067474, "score": 1, "content": "good job"}
{"id": "mg5wuel", "type": "comment", "parent_id": "t1_mg5slx1", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg5wuel/", "author": "webscraping-ModTeam", "created_utc": 1741190068, "score": 1, "content": "Please review the sub rules"}
{"id": "mfvd69f", "type": "comment", "parent_id": "t1_mfvbqsx", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvd69f/", "author": "travel-nurse-guru", "created_utc": 1741045452, "score": 1, "content": "Website looks great! But I'm getting the same error. Looking forward to trying it out"}
{"id": "mfve8ng", "type": "comment", "parent_id": "t1_mfvbqsx", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfve8ng/", "author": "DmitryPapka", "created_utc": 1741045801, "score": 1, "content": "What is used to extract data from HTML by prompt?"}
{"id": "mfz2qt9", "type": "comment", "parent_id": "t1_mfxuwmx", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfz2qt9/", "author": "Excellent-Two1178", "created_utc": 1741102216, "score": 1, "content": "Error handling can be a bit rough still. Will try and add some more transparency on why a generation attempt may fail shortly"}
{"id": "mg1lbil", "type": "comment", "parent_id": "t1_mg1k38j", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mg1lbil/", "author": "Excellent-Two1178", "created_utc": 1741127389, "score": 1, "content": "What is email I\u2019ll add some more for you. I\u2019m currently traveling so likely won\u2019t get better error handling in until tonight at earliest"}
{"id": "mfvdwor", "type": "comment", "parent_id": "t1_mfvd69f", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvdwor/", "author": "Excellent-Two1178", "created_utc": 1741045692, "score": 2, "content": "Should be fixed soon sorry about that will add you guys some extra free api uses on me. Sometimes shipping directly to main with minimal testing has its downfalls"}
{"id": "mfvmlmb", "type": "comment", "parent_id": "t1_mfve8ng", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvmlmb/", "author": "Excellent-Two1178", "created_utc": 1741048550, "score": 2, "content": "It doss not use a prompt alone to extract data. It runs actual code to extract the data which eliminates the issue of hallucinated data, and provides you a script to replicate it without needing AI going forwards"}
{"id": "mfvjkhj", "type": "comment", "parent_id": "t1_mfvdwor", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvjkhj/", "author": "Excellent-Two1178", "created_utc": 1741047560, "score": 1, "content": "Is fixed sorry about that"}
{"id": "mfvnwgr", "type": "comment", "parent_id": "t1_mfvmlmb", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvnwgr/", "author": "DmitryPapka", "created_utc": 1741048972, "score": 1, "content": "If \"Describe what to extract\" is not prompt, then what is that exactly? What does your program do with that text?"}
{"id": "mfvphvk", "type": "comment", "parent_id": "t1_mfvnwgr", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvphvk/", "author": "Excellent-Two1178", "created_utc": 1741049492, "score": 2, "content": "It does use a prompt at some point yes. It uses the prompt to generate scraper code, which is then ran to get the data"}
{"id": "mfvqsoo", "type": "comment", "parent_id": "t1_mfvphvk", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvqsoo/", "author": "DmitryPapka", "created_utc": 1741049919, "score": 1, "content": "Is there any AI tool behind this?"}
{"id": "mfvr8ho", "type": "comment", "parent_id": "t1_mfvqsoo", "permalink": "https://www.reddit.com/r/webscraping/comments/1j2wi03/create_web_scrapers_using_ai/mfvr8ho/", "author": "Excellent-Two1178", "created_utc": 1741050065, "score": 3, "content": "It uses the Claude api, no other third party ai service is used though."}
{"id": "1hyue1v", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/", "author": "Ammar__", "created_utc": 1736596656, "score": 100, "title": "I fell in love with it but is it still profitable?", "content": "To be honest, this active sub is already an evidence that web scraping is still a blooming business. Probably always will be. But I guess being new to this. Also I'm about to embark on a long learning journey where I'll be investing a lot of time and effort. I fell in love after delivering a couple of scripts to a client. I think I'll be giving this my best in 2025. I'm always jumping from one project to another. So, I hope this sub don't mind some hand-holding for a newbie who really needs the extra encouragements."}
{"id": "m6kzg6s", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6kzg6s/", "author": "Rich-Independent1202", "created_utc": 1736607301, "score": 32, "content": "If you\u2019re in love with web scraping, that\u2019s great, but let\u2019s get real is it profitable? The answer is yes, but only if you can consistently find high-value clients with real data needs. This is not a one-time gig. It\u2019s about offering something clients can\u2019t get elsewhere or providing a service so crucial that they\u2019ll pay for it repeatedly. In 2025, web scraping is still thriving, but with competition out there, it\u2019s not about just writing scripts. You need to master the niches, keep up with the legalities, and most importantly, provide value consistently. Forget about jumping from project to project if you really want to make this a solid income stream."}
{"id": "m6ksvcy", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6ksvcy/", "author": "phatBleezy", "created_utc": 1736604801, "score": 5, "content": "Any tips on getting started?"}
{"id": "m6n2ael", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6n2ael/", "author": "Difficult-Value-3145", "created_utc": 1736631207, "score": 5, "content": "It's all automation web scraping automated to ya know and the only problem with scraping I see going forward is more company's are offering api for the same data"}
{"id": "m6za45v", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6za45v/", "author": "Numinousfox", "created_utc": 1736800205, "score": 3, "content": "There will always be Web scraping. It looks to be getting easier, particularly with Ai to assist with the clean up.but I wouldn't be surprised if new tech tips the scales back the otherway at some point. You probably need to assess what you're trying to do for people. What is the value. Because it certainly isn't in just 'data'. It's the decisions they could make, the answers they will get or the people they will sell that matters. Find the often asked but potentially unique answers business need and scrape it Curiously, what language / stack are yoy using?"}
{"id": "m72xb4b", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m72xb4b/", "author": "ArchipelagoMind", "created_utc": 1736853385, "score": 3, "content": "I imagine one challenge is how ethical you want to be in your business too. I'm very lucky right now to have a job where I get paid to do a lot of web scraping and, for what I believe, to be an ethical good. There's a lot of fairly innocuous web scraping wants out there. But I also way too often see people looking to, say, web scrape all users on a series of Discord users, or web scrape the personal info of people on a fetish site. Even some of the \"lead generation\" requests are just hunting for people to send a bunch of crappy spam too. There's some legit business web scraping to be had out there, but a lot of it leaves me feeling very icky."}
{"id": "m76tcj5", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m76tcj5/", "author": "ReDraXon", "created_utc": 1736899686, "score": 3, "content": "Absolutely. I work at a competitive intelligence SaaS, and we have a variety of enterprise clients with different data needs, many of which pay really well. For example, property firms often want competitor pricing data, retailers look for product and pricing insights, and plenty of companies track job postings from their competitors. We also work with clients in industries like financial services and even online casinos. That said, I get that enterprise-level work might not compare directly to what an individual can do, but honestly, our data team is pretty small\u2014we could be working out of a garage for all it matters. The real challenge is landing these types of clients, which is tough unless maybe you have an agency or something. Still, there\u2019s a lot of potential in smaller gigs or specific niches if you\u2019re consistent and keep learning."}
{"id": "m6xf33m", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6xf33m/", "author": "spcman13", "created_utc": 1736780371, "score": 2, "content": "As someone who is always looking for scraping/crawling, it\u2019s difficult to find someone reliable. While costs are going down from what I see, reliability has gone with it."}
{"id": "m77k1t5", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m77k1t5/", "author": "webdevcode", "created_utc": 1736908865, "score": 2, "content": "Where would someone go to find clients who need data from scraping?"}
{"id": "m6lazz2", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6lazz2/", "author": "IamFromNigeria", "created_utc": 1736611224, "score": 1, "content": "I think it's relatively profitable especially depending on your location and it's more competitive getting clients but don't solely depend on it..You can have other skills as well just in case"}
{"id": "m6pinsz", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6pinsz/", "author": "Chemical-Height8888", "created_utc": 1736664121, "score": 1, "content": "How much do you charge?"}
{"id": "m723ayw", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m723ayw/", "author": "Tech-Sapien18", "created_utc": 1736834792, "score": 1, "content": "How does one earn from web scraping?"}
{"id": "m7a6pfg", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m7a6pfg/", "author": "None", "created_utc": 1736954237, "score": 1, "content": "[removed]"}
{"id": "m6slpk0", "type": "comment", "parent_id": "t3_1hyue1v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6slpk0/", "author": "syphoon_data", "created_utc": 1736710379, "score": 1, "content": "It\u2019s definitely profitable, and depends on how complex a problem you\u2019re solving for your client. Getting live data from certain websites can be extremely challenging. So yes, like everything else in the world, it\u2019s a demand and supply thing."}
{"id": "m6m16lt", "type": "comment", "parent_id": "t1_m6kzg6s", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6m16lt/", "author": "None", "created_utc": 1736619429, "score": 1, "content": "Any exemple of datas a client constantly need from you ?"}
{"id": "m6m6igh", "type": "comment", "parent_id": "t1_m6kzg6s", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6m6igh/", "author": "Ammar__", "created_utc": 1736621081, "score": 1, "content": "Thank you for the detailed response. I get your points."}
{"id": "m6m6vu4", "type": "comment", "parent_id": "t1_m6ksvcy", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6m6vu4/", "author": "Ammar__", "created_utc": 1736621197, "score": 2, "content": "Getting started learning or getting started scoring clients?"}
{"id": "m720t1b", "type": "comment", "parent_id": "t1_m6n2ael", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m720t1b/", "author": "Ammar__", "created_utc": 1736833503, "score": 2, "content": "They can't cover all bases. And web scraping has more than leads generation to it. My first client was a researcher who needed scanned documents off a certain site."}
{"id": "m72175a", "type": "comment", "parent_id": "t1_m6za45v", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m72175a/", "author": "Ammar__", "created_utc": 1736833703, "score": 2, "content": "For now, I'm using python and famous scraping library like bs4, lxml, selenium. I'm interested in scrapy but didn't get to learn it yet."}
{"id": "m75vb8t", "type": "comment", "parent_id": "t1_m72xb4b", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m75vb8t/", "author": "Ammar__", "created_utc": 1736889071, "score": 1, "content": "This can't be worse than being a lawyer. XD I don't mind feeling icky. That's part of being a professional in any field. You gotta cut them some slack too. Generating leads often require spamming at first. But some people are more gracious than others doing it. We just provide the data. Which already available for public. We're only harvesters."}
{"id": "m720wkj", "type": "comment", "parent_id": "t1_m6xf33m", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m720wkj/", "author": "Ammar__", "created_utc": 1736833552, "score": 1, "content": "Find a pro scraper and stick with him. There are plenty of talented people out there."}
{"id": "m77nz5p", "type": "comment", "parent_id": "t1_m77k1t5", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m77nz5p/", "author": "Ammar__", "created_utc": 1736910266, "score": 1, "content": "For starter, keep an eye on subreddit like slavelabour and its sisters. That's how I got my firsts. You can setup a upwork account or fiverr account and showcase your skills."}
{"id": "m6m6rv5", "type": "comment", "parent_id": "t1_m6lazz2", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6m6rv5/", "author": "Ammar__", "created_utc": 1736621163, "score": 0, "content": "I'll be also focusing on automation with python. But I don't want to stretch farther. Cause I've known myself not to commit to anything long enough till I see its fruition. Never again. I will do my best to see this through."}
{"id": "m6s67zp", "type": "comment", "parent_id": "t1_m6pinsz", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6s67zp/", "author": "None", "created_utc": 1736705972, "score": 1, "content": "[removed]"}
{"id": "m7241de", "type": "comment", "parent_id": "t1_m723ayw", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m7241de/", "author": "Ammar__", "created_utc": 1736835185, "score": 1, "content": "You either sell the data, use the data to provide useful product to clients like generating leads, or you sell customizable tools so client can scrape his own data. I was only lucky to earn with the latter so far. But I've only started. There are some courses out there will show you all the ways you can earn money with web scraping. Just search for them."}
{"id": "m7c7hex", "type": "comment", "parent_id": "t1_m7a6pfg", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m7c7hex/", "author": "webscraping-ModTeam", "created_utc": 1736975257, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m6n42p0", "type": "comment", "parent_id": "t1_m6m6vu4", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6n42p0/", "author": "phatBleezy", "created_utc": 1736631772, "score": 4, "content": "Learning the methods and determining valuable info to scrape"}
{"id": "m724ty9", "type": "comment", "parent_id": "t1_m720t1b", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m724ty9/", "author": "Difficult-Value-3145", "created_utc": 1736835614, "score": 1, "content": "Ya a lot of stuff like that just looking for the next two the k around while someone else is trying to stop it and half the time it's some same folks on both sides whoever's hiring or paying the most"}
{"id": "m797nd9", "type": "comment", "parent_id": "t1_m77nz5p", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m797nd9/", "author": "thanksforcomingout", "created_utc": 1736940066, "score": 2, "content": "I\u2019d pay for a service I could customize and refine based on sector/location/source."}
{"id": "m6sk5no", "type": "comment", "parent_id": "t1_m6s67zp", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6sk5no/", "author": "webscraping-ModTeam", "created_utc": 1736709931, "score": 2, "content": "Please review the sub rules"}
{"id": "m6s6crs", "type": "comment", "parent_id": "t1_m6s67zp", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m6s6crs/", "author": "None", "created_utc": 1736706009, "score": 1, "content": "[removed]"}
{"id": "m720mgr", "type": "comment", "parent_id": "t1_m6n42p0", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m720mgr/", "author": "Ammar__", "created_utc": 1736833413, "score": 2, "content": "There are some good courses on udemy if you wanna invest. There are free courses on youtube. Just search \"web scraping course\" on youtube or google. Most course I saw are in python which is good since it is simple enough. You can also use some browser extensions to scrap data even more easily."}
{"id": "m714h1o", "type": "comment", "parent_id": "t1_m6n42p0", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m714h1o/", "author": "Curious_Plum_3715", "created_utc": 1736821140, "score": 1, "content": "Remindme! 3 days"}
{"id": "m74ss7u", "type": "comment", "parent_id": "t1_m720mgr", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m74ss7u/", "author": "phatBleezy", "created_utc": 1736877386, "score": 2, "content": "Great starting point, thank you. Any specific creators or lessons you'd recommend?"}
{"id": "m714knb", "type": "comment", "parent_id": "t1_m714h1o", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m714knb/", "author": "RemindMeBot", "created_utc": 1736821174, "score": 1, "content": "I will be messaging you in 3 days on [**2025-01-17 02:19:00 UTC**]( to remind you of [**this link**]( [**1 OTHERS CLICKED THIS LINK**]( to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)]( ***** |[^(Info)]( Reminders)]( |-|-|-|-|"}
{"id": "m75u087", "type": "comment", "parent_id": "t1_m74ss7u", "permalink": "https://www.reddit.com/r/webscraping/comments/1hyue1v/i_fell_in_love_with_it_but_is_it_still_profitable/m75u087/", "author": "Ammar__", "created_utc": 1736888671, "score": 2, "content": "I'm just starting myself and I'm liking this course: [ He started from the basics. HTTP protocol. Web page ingredients. It's a long course. But I feel like it's gonna cover all bases. But it's 50$ I think. So please start with free ones on youtube first. I've haven't checked those yet."}
{"id": "mj4oka", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/", "author": "Kaligule", "created_utc": 1617439404, "score": 104, "title": "waiting for the data to flow in", "content": ""}
{"id": "gt8c51k", "type": "comment", "parent_id": "t3_mj4oka", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt8c51k/", "author": "yoohoooos", "created_utc": 1617443284, "score": 3, "content": "How many pages you are scraping that's taking weeks?"}
{"id": "gt9cp38", "type": "comment", "parent_id": "t3_mj4oka", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt9cp38/", "author": "None", "created_utc": 1617468201, "score": 5, "content": "What data are you scraping? What\u2019s the project for?"}
{"id": "gt8yo9m", "type": "comment", "parent_id": "t3_mj4oka", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt8yo9m/", "author": "SpaceZZ", "created_utc": 1617460682, "score": -1, "content": "There is nothing that takes weeks to scrape unless you put wait times yourself. Are you using some parallelism or async in your code?"}
{"id": "gt8cl44", "type": "comment", "parent_id": "t1_gt8c51k", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt8cl44/", "author": "Kaligule", "created_utc": 1617443777, "score": 8, "content": "It's more that I track the changes of the pages over a longer time. It will take some time until there is enough data to see some meaningful patterns."}
{"id": "gt9demr", "type": "comment", "parent_id": "t1_gt9cp38", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt9demr/", "author": "Kaligule", "created_utc": 1617468569, "score": 6, "content": "I want to see how long a company's job postings stay online. It is also a learning project."}
{"id": "gt95z3v", "type": "comment", "parent_id": "t1_gt8yo9m", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt95z3v/", "author": "Kaligule", "created_utc": 1617464657, "score": 6, "content": "The script runs needs only a few minutes. It is the data that is changing slowly. I scan the data once a day and I will need at least a few weeks of data to get meaningful results."}
{"id": "gt9t4a1", "type": "comment", "parent_id": "t1_gt8yo9m", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/gt9t4a1/", "author": "emirhodzic92", "created_utc": 1617476505, "score": 2, "content": "I am currently scraping some website from some local government branch that has a lot of data on land use. Their servers suck. When I start the script, you can barely open their website. If I try parallel, it is just slower in both processes. So, it can take weeks."}
{"id": "keh9dly", "type": "comment", "parent_id": "t1_gt9demr", "permalink": "https://www.reddit.com/r/webscraping/comments/mj4oka/waiting_for_the_data_to_flow_in/keh9dly/", "author": "ATG-NNN-TGA", "created_utc": 1703261610, "score": 1, "content": "How did this go? Do you have any results? I am curious to know"}
{"id": "1ihcrqr", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/", "author": "xkiiann", "created_utc": 1738655993, "score": 102, "title": "I reverse engineered the cloudflare jsd challenge", "content": "Its the most basic version (/cdn-cgi/challenge-platform/h/b/jsd), but it\u2018s something\u200d\u2642\ufe0f ["}
{"id": "mazlw3g", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mazlw3g/", "author": "ronoxzoro", "created_utc": 1738702585, "score": 5, "content": "any guide jow u did it"}
{"id": "maw5228", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/maw5228/", "author": "RobSm", "created_utc": 1738658186, "score": 3, "content": "Good job"}
{"id": "mazlwa4", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mazlwa4/", "author": "shoebill_homelab", "created_utc": 1738702587, "score": 2, "content": "I can pretty much do anything under web scraping, but man, reversing js... I just cannot do... nice"}
{"id": "mb1i51q", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb1i51q/", "author": "L4z3x", "created_utc": 1738724166, "score": 2, "content": "Is there sme blogs explaining this ?"}
{"id": "maw5qry", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/maw5qry/", "author": "musaspacecadet", "created_utc": 1738658613, "score": 1, "content": "nice"}
{"id": "maxkhx6", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/maxkhx6/", "author": "SayIt2Gart", "created_utc": 1738681742, "score": 1, "content": "Woaa, fo real"}
{"id": "mb11xew", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb11xew/", "author": "rundef", "created_utc": 1738718897, "score": 1, "content": "Well done. I'll put it to the test tomorrow"}
{"id": "mb3yvb3", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb3yvb3/", "author": "cerisawa", "created_utc": 1738765778, "score": 1, "content": "Any idea how to implement this into selenium ?"}
{"id": "mb4ok7o", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb4ok7o/", "author": "HolidayFinancial3336", "created_utc": 1738773387, "score": 1, "content": "To be able to do this type of reverse engineering js is it necessary to have a deep knowledge of the language?"}
{"id": "mb75i8n", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb75i8n/", "author": "StickAffectionate769", "created_utc": 1738798164, "score": 1, "content": "Good Job! But im having issues using the cf clearance cookie. I send a request to the same website (with the same IP and same headers) using a request repeater and I get a 403 forbidden. I compare the cookie returned by the script vs the real cookie returned by the browser and the one returned by the browser has always more characters. Any idea why?"}
{"id": "mb7glby", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb7glby/", "author": "i7solar", "created_utc": 1738801706, "score": 1, "content": "nice! whats the diff between h/b and h/g?"}
{"id": "mb7nwu5", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb7nwu5/", "author": "Pauloedsonjk", "created_utc": 1738804045, "score": 1, "content": "I need test it. Look very good."}
{"id": "mc2xga8", "type": "comment", "parent_id": "t3_1ihcrqr", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mc2xga8/", "author": "adrianhorning", "created_utc": 1739223665, "score": 1, "content": "Whoa"}
{"id": "mb58ksq", "type": "comment", "parent_id": "t1_mb3yvb3", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb58ksq/", "author": "xkiiann", "created_utc": 1738778940, "score": 1, "content": "run the get\\_fingerprint code in your selenium browser and then set the cookie manually"}
{"id": "mb58ne2", "type": "comment", "parent_id": "t1_mb4ok7o", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb58ne2/", "author": "xkiiann", "created_utc": 1738778959, "score": 3, "content": "yes?"}
{"id": "mb7giv5", "type": "comment", "parent_id": "t1_mb75i8n", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb7giv5/", "author": "i7solar", "created_utc": 1738801684, "score": 1, "content": "UA and IP needs to be the same throughout the clearance session"}
{"id": "mb8vhmj", "type": "comment", "parent_id": "t1_mb75i8n", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb8vhmj/", "author": "xkiiann", "created_utc": 1738819722, "score": 1, "content": "What site you on?"}
{"id": "mb8v2d3", "type": "comment", "parent_id": "t1_mb7glby", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb8v2d3/", "author": "xkiiann", "created_utc": 1738819531, "score": 2, "content": "I don't know actually"}
{"id": "mb67xbm", "type": "comment", "parent_id": "t1_mb58ksq", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb67xbm/", "author": "cerisawa", "created_utc": 1738788620, "score": 1, "content": "I'd have to use something similar to Cloudflare's solve from [cfhb.py]( to get the cookies first no ? I'm kinda new to this... Basically the website I'm trying to run it into uses first turnstile, that I can pass normally, and then some other cf challenge that blocks any proxied connection by giving 1015 error."}
{"id": "mbdtlw5", "type": "comment", "parent_id": "t1_mb7giv5", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mbdtlw5/", "author": "StickAffectionate769", "created_utc": 1738884019, "score": 1, "content": "Yes, its the same"}
{"id": "mbdtn1k", "type": "comment", "parent_id": "t1_mb8vhmj", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mbdtn1k/", "author": "StickAffectionate769", "created_utc": 1738884030, "score": 1, "content": "The one on [example.py]("}
{"id": "mb8vcld", "type": "comment", "parent_id": "t1_mb67xbm", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb8vcld/", "author": "xkiiann", "created_utc": 1738819659, "score": 1, "content": "Yea. But make sure the website actually uses the jsd challenge and not something more difficult"}
{"id": "mbftjv3", "type": "comment", "parent_id": "t1_mbdtn1k", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mbftjv3/", "author": "xkiiann", "created_utc": 1738910429, "score": 1, "content": "Can you access the site manually?"}
{"id": "mb9l1dn", "type": "comment", "parent_id": "t1_mb8vcld", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb9l1dn/", "author": "VeePeeMoba", "created_utc": 1738833896, "score": 1, "content": "sorry to chime in, how can I tell? this is what my target is showing: \" thanks"}
{"id": "mb9tpfh", "type": "comment", "parent_id": "t1_mb9l1dn", "permalink": "https://www.reddit.com/r/webscraping/comments/1ihcrqr/i_reverse_engineered_the_cloudflare_jsd_challenge/mb9tpfh/", "author": "xkiiann", "created_utc": 1738839185, "score": 2, "content": "This is way harder than jsd. Can\u2019t help you with that"}
{"id": "iy7or8", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/", "author": "UtopianCorps", "created_utc": 1600857416, "score": 98, "title": "I made a Python bot that scrapes Udemy Coupons and then AUTOMATICALLY ENROLLS YOU to those PAID COURSES for FREE. Check the comments below for the GitHub! Do Star and Fork the repository :)", "content": ""}
{"id": "g6b0h64", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6b0h64/", "author": "UtopianCorps", "created_utc": 1600857430, "score": 6, "content": "GitHub: ["}
{"id": "ggdz8as", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/ggdz8as/", "author": "Gurthen", "created_utc": 1608404051, "score": 3, "content": "Hey there, was trying to use it and I keep getting this error: ModuleNotFoundError: No module named 'ruamel' I'm new to Python so can't seem to fix it although running pip list it shows I have ruamel-yaml instead of ruamel.yaml? A bit lost :P"}
{"id": "g6gmbwe", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6gmbwe/", "author": "None", "created_utc": 1600977788, "score": 2, "content": "[deleted]"}
{"id": "g6gwh4c", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6gwh4c/", "author": "None", "created_utc": 1600981837, "score": 2, "content": "[removed]"}
{"id": "jnmm9kf", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/jnmm9kf/", "author": "AggressiveRub9434", "created_utc": 1686376947, "score": 2, "content": "Beautiful"}
{"id": "g6b13i5", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6b13i5/", "author": "None", "created_utc": 1600858111, "score": 1, "content": "[removed]"}
{"id": "g6b2qur", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6b2qur/", "author": "brycedavies", "created_utc": 1600859820, "score": 1, "content": "Hell yeah this is so good!"}
{"id": "g6c0ivo", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6c0ivo/", "author": "yaphott", "created_utc": 1600879712, "score": 1, "content": "Strong work! :) Thank you for sharing"}
{"id": "g6ci2ax", "type": "comment", "parent_id": "t3_iy7or8", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6ci2ax/", "author": "SnowPenguin_", "created_utc": 1600888266, "score": 1, "content": "Wow, that's a brilliant idea. Usually people use web scraping for collecting data & the like :D"}
{"id": "ggg8386", "type": "comment", "parent_id": "t1_ggdz8as", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/ggg8386/", "author": "UtopianCorps", "created_utc": 1608446422, "score": 1, "content": "Ahh, I see... I suggest you to put up an issue on the GitHub repo since the updated code isn't maintained by me anymore and I'm not totally sure about how it works. The collaborators will guide you through it. :)"}
{"id": "g6gnqcx", "type": "comment", "parent_id": "t1_g6gmbwe", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6gnqcx/", "author": "None", "created_utc": 1600978380, "score": 2, "content": "[deleted]"}
{"id": "g6ih8l9", "type": "comment", "parent_id": "t1_g6gwh4c", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6ih8l9/", "author": "UtopianCorps", "created_utc": 1601016464, "score": 1, "content": "Thanks you! :)"}
{"id": "g6b7fw3", "type": "comment", "parent_id": "t1_g6b13i5", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6b7fw3/", "author": "UtopianCorps", "created_utc": 1600863801, "score": 1, "content": "Thank you so much! Do fork the repo! :)"}
{"id": "g6b7io3", "type": "comment", "parent_id": "t1_g6b2qur", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6b7io3/", "author": "UtopianCorps", "created_utc": 1600863858, "score": 1, "content": "Thanks a lot Bryce, do fork! :)"}
{"id": "g6c955f", "type": "comment", "parent_id": "t1_g6c0ivo", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6c955f/", "author": "UtopianCorps", "created_utc": 1600883890, "score": 1, "content": "Thanks for the comment! Please share it with your coding buddies if you like it \u270c"}
{"id": "g6cj5yx", "type": "comment", "parent_id": "t1_g6ci2ax", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6cj5yx/", "author": "UtopianCorps", "created_utc": 1600888805, "score": 3, "content": "Glad that you liked it! Thank you! :D"}
{"id": "g6ih7rk", "type": "comment", "parent_id": "t1_g6gnqcx", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6ih7rk/", "author": "UtopianCorps", "created_utc": 1601016445, "score": 1, "content": "Someone has reported me the same error, related to the zipcode; I'm trying to fix it but unfortunately even the VPN can't change the region to the United States. I will get back to you if I find the solution"}
{"id": "g6qt30o", "type": "comment", "parent_id": "t1_g6cj5yx", "permalink": "https://www.reddit.com/r/webscraping/comments/iy7or8/i_made_a_python_bot_that_scrapes_udemy_coupons/g6qt30o/", "author": "SnowPenguin_", "created_utc": 1601162704, "score": 2, "content": "Always welcome ^ ^"}
{"id": "1mymxs6", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/", "author": "laataisu", "created_utc": 1756012116, "score": 98, "title": "Tried AI for real-world scraping\u2026 it\u2019s basically useless", "content": "AI scraping is kinda a joke**.** Most demos just scrape *toy websites* with no bot protection. The moment you throw it at a real, dynamic site with proper defenses, it faceplants hard. Case in point: I asked it to grab data from [ by searching **\u201cPrabowo Subianto\u201d** and pulling the dataset. What I got back? * Endless scripts that don\u2019t work * Wasted tokens & time * Zero progress on bypassing captcha So yeah\u2026 if your site has more than static HTML, AI scrapers are basically cosplay coders right now. Anyone here actually managed to get reliable results from AI for *real* scraping tasks, or is it just snake oil?"}
{"id": "nads4x0", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nads4x0/", "author": "Virtual-Landscape-56", "created_utc": 1756024344, "score": 37, "content": "My experience: On the production level, LLMs can be used as a light reasoning layer for data extraction and labeling of the already extracted DOM elements. I could not find any other part of the scraping operation that they can show reliability."}
{"id": "naeq1uy", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naeq1uy/", "author": "beachguy82", "created_utc": 1756041585, "score": 19, "content": "I\u2019ve scraped over 10M pages so far. You need to use a tool to grab the webpage, convert it to markdown, then process with AI."}
{"id": "nae9em3", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nae9em3/", "author": "theskd1999", "created_utc": 1756034399, "score": 7, "content": "Reliability is still a major issue, I myself tried multiple open source project, but the amount of token it consume and reliability is still a major issue I was also facing, for now I have switched to other non ai tools"}
{"id": "naexjg5", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naexjg5/", "author": "sleepWOW", "created_utc": 1756044241, "score": 7, "content": "I used AI to build my own script and then tweak it based on my needs. Now my script can bypass cloud flare protection and scrape data 24/7. Literally, I was copying and pasting errors in cline bot in my Cursor and I gradually built a fully functional scraper."}
{"id": "nae2lhq", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nae2lhq/", "author": "bigzyg33k", "created_utc": 1756030665, "score": 11, "content": "This is a more a reflection on your ability to scrape rather than a limitation of LLMs. Your scraping infrastructure should handle captchas and bot protection, the LLM shouldn\u2019t play a role at all."}
{"id": "napsot3", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/napsot3/", "author": "Motor-Glad", "created_utc": 1756186983, "score": 3, "content": "Lol I scraped all the most difficult sites in the world using Ai. Zero experience in coding 6 months ago. I know nothing of scraping and python but managed to do it anyway. It's the prompts, not the AI"}
{"id": "natl710", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/natl710/", "author": "yoperuy", "created_utc": 1756237475, "score": 2, "content": "Not only they give you crap results if you need to scrape million of pages the cost its absurd. I do scrape reatil stores to feed a marketplace with a custom built software. To locate the information im using, DOM/XPath queries + opengraph + jsonld markup + html microdata. We crawl and scrape 1 millon pages daily."}
{"id": "nad7805", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nad7805/", "author": "None", "created_utc": 1756012515, "score": 3, "content": "[removed]"}
{"id": "nad9lpf", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nad9lpf/", "author": "martinsbalodis", "created_utc": 1756013777, "score": 1, "content": "That is true! I am working on a tool that is trying to find relevant data in html. It finds about 70-80%. If it doesn't understand html that well, then writing code is probably ridiculous!"}
{"id": "nadv3b5", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadv3b5/", "author": "None", "created_utc": 1756026149, "score": 1, "content": "[removed]"}
{"id": "nadx2cs", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadx2cs/", "author": "KaviCamelCase", "created_utc": 1756027343, "score": 1, "content": "Lmao at the Prabowo search. What are you up too lol. Semoga sukses kak. What exactly are you doing? How are you instructing what AI to scrape a website?"}
{"id": "nadz9aj", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadz9aj/", "author": "hudahoeda", "created_utc": 1756028702, "score": 1, "content": "Not expecting someone scraping for Prabowo in this sub , hope you find your solution bro!"}
{"id": "nae00il", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nae00il/", "author": "ArtisticPsychology43", "created_utc": 1756029148, "score": 1, "content": "It's obvious that you can't tell the Ai agent \"scrape this page\" that's not how AI is used in technical form. I have used various Agents for scraping and there are really big differences in scraping and if used well it solves a lot of problems and reduces development time enormously. Practically the part of the scraping logic (apart from future maintenance) is now the part that takes me the least time ever"}
{"id": "naenotr", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naenotr/", "author": "AZ_Crush", "created_utc": 1756040691, "score": 1, "content": "Try Perplexity's Comet for one of your use cases. It might surprise you."}
{"id": "naexcf7", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naexcf7/", "author": "cyberpsycho999", "created_utc": 1756044176, "score": 1, "content": "Depends on the model, libs underlaying etc. Most llms without specific tools and prompts will fail. I had one task to prove that. There a few pieces of map where you want to recognize streets and then city. If you pass them as images to 4.1 you will get answer. When I just create a json file with streets it fails. In first example it may uses diff datasets and tools underlaying for ocr, maybe trained on maps. In 2nd it not used code interpreter tool. So even when I thought o simplify a job for gpt its not. Model will also give worse answer if you dont add a file as a context and pass it as a text."}
{"id": "naexihu", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naexihu/", "author": "Dry_Illustrator977", "created_utc": 1756044232, "score": 1, "content": "Yup"}
{"id": "naf5025", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naf5025/", "author": "charlesthayer", "created_utc": 1756046681, "score": 1, "content": "Lots of subtle tricks to getting things to work. Have a look at the MCP tools for playwright and puppeteer for dealing with javascript: [ ["}
{"id": "nafb9y1", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafb9y1/", "author": "None", "created_utc": 1756048658, "score": 1, "content": "[removed]"}
{"id": "nafuk0o", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafuk0o/", "author": "IgnisIncendio", "created_utc": 1756054612, "score": 1, "content": "That's not what AI scraping means. You use AI to read a screenshot of a web page. You don't use AI to code the scraper itself."}
{"id": "nag01i0", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nag01i0/", "author": "None", "created_utc": 1756056234, "score": 1, "content": "[removed]"}
{"id": "nag84v3", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nag84v3/", "author": "Waste_Explanation410", "created_utc": 1756058617, "score": 1, "content": "Ai does so well with selenium"}
{"id": "najj2og", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najj2og/", "author": "Crazy-Return3432", "created_utc": 1756103801, "score": 1, "content": "as for pure scrapping - no; as a code compiler where you provide instructions what to scrap in details - yes, as a code compiler where you pass all limitations triggered by advance bots recognition software - yes"}
{"id": "najwiyx", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najwiyx/", "author": "singlebit", "created_utc": 1756111917, "score": 1, "content": "!remindme 1month"}
{"id": "nauehlk", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nauehlk/", "author": "None", "created_utc": 1756245997, "score": 1, "content": "[removed]"}
{"id": "nawlmd4", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nawlmd4/", "author": "rohiitcodes", "created_utc": 1756277292, "score": 1, "content": "I'm currently working on one, let's see where we get it's a paid project so I'm afraid"}
{"id": "nbjsjuv", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nbjsjuv/", "author": "greggy187", "created_utc": 1756585081, "score": 1, "content": "That\u2019s not true. The out of the box scrapers are all weak but you can spin up your own script that can do anything. I even have my bot scrape then look for the contact form and try to get a lead for me"}
{"id": "nccaqqn", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nccaqqn/", "author": "fruitcolor", "created_utc": 1756976065, "score": 1, "content": "I guess AI is more useful for parsing already scraped content."}
{"id": "nadgkt0", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadgkt0/", "author": "bigtakeoff", "created_utc": 1756017629, "score": 1, "content": "wow, who and what are you scraping for Indo Boy?"}
{"id": "nad99ut", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nad99ut/", "author": "arika_ex", "created_utc": 1756013599, "score": 1, "content": "I've had some good results on a task I'm working on. Trying to perform an initial scrape whilst creating reusable scripts. The sites in question may not have robust anti-bot detection, but anyway the key point for me has been to break down the tasks into a detailed prompt and separate scripts (python + Selenium/BS4) and then closely monitor each process and output and adjust as needed. I of course can't see your full prompt/chat history, but if you're not doing so already I suggest you approach it one step at a time."}
{"id": "nadh4yr", "type": "comment", "parent_id": "t3_1mymxs6", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadh4yr/", "author": "Even_Description_776", "created_utc": 1756017944, "score": 0, "content": "yeah agreed been there... done that...."}
{"id": "nagbtdb", "type": "comment", "parent_id": "t1_nads4x0", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nagbtdb/", "author": "Vast_Yak_4147", "created_utc": 1756059749, "score": 4, "content": "this has been my experience too, it can be very helpful in giving a little bit of flexibility to a pipeline within a tightly defined context"}
{"id": "nafgy7s", "type": "comment", "parent_id": "t1_naeq1uy", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafgy7s/", "author": "Scared_Astronaut9377", "created_utc": 1756050429, "score": 16, "content": "You are talking about post-processing. They are talking about scrapping."}
{"id": "nah3jrb", "type": "comment", "parent_id": "t1_naeq1uy", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nah3jrb/", "author": "PM_ME_UR_ICT_FLAG", "created_utc": 1756068429, "score": 3, "content": "What do you recommend?"}
{"id": "nbhx2x4", "type": "comment", "parent_id": "t1_naeq1uy", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nbhx2x4/", "author": "None", "created_utc": 1756564457, "score": 1, "content": "[removed]"}
{"id": "najtt2s", "type": "comment", "parent_id": "t1_naexjg5", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najtt2s/", "author": "hackbyown", "created_utc": 1756110257, "score": 1, "content": "Can you share steps which you automated in your bot to bypass cloudflare protection 24*7"}
{"id": "nafafvo", "type": "comment", "parent_id": "t1_nae2lhq", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafafvo/", "author": "smoke4sanity", "created_utc": 1756048397, "score": 8, "content": "I mean, this post was written with AI, so I assume OP is the kind of person that expects AI to do every single task. I see too many devs using LLMs for things that automation has been doing efficiently for over a decade or two."}
{"id": "naezsmr", "type": "comment", "parent_id": "t1_nae2lhq", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naezsmr/", "author": "cyberpsycho999", "created_utc": 1756044990, "score": 1, "content": "Another funny thing about that is if you add http request tool it may do less request that task reuqire to gather necessary data. Sometimes you can convience llm to do more by saying its your server and it wont harm it. So better to have a normal crawler before."}
{"id": "nbsc9wo", "type": "comment", "parent_id": "t1_napsot3", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nbsc9wo/", "author": "No_Outside_9446", "created_utc": 1756702902, "score": 1, "content": "Any way to share the script for it, Like GitHub Thanks"}
{"id": "nadyxwx", "type": "comment", "parent_id": "t1_nad7805", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadyxwx/", "author": "DancingNancies1234", "created_utc": 1756028509, "score": 1, "content": "Agreed!"}
{"id": "naeyv0y", "type": "comment", "parent_id": "t1_nad9lpf", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naeyv0y/", "author": "cyberpsycho999", "created_utc": 1756044683, "score": 1, "content": "Openai api? I learned hard way that using raw model can give bullshit when you dont use api for upload file or code interpreter tool. Passing HTML within prompt is failing for me. Input output tokens were high. Once I used assistant api i was able to tune it for my needs with lower token usage and faster. In 2nd try i also asked him to give the code from code interpreter which worked and then i pass it in system prompt."}
{"id": "naewkfn", "type": "comment", "parent_id": "t1_nadv3b5", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/naewkfn/", "author": "webscraping-ModTeam", "created_utc": 1756043911, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "nadyx57", "type": "comment", "parent_id": "t1_nadx2cs", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nadyx57/", "author": "laataisu", "created_utc": 1756028496, "score": -2, "content": "I just need to get structured data for power research analysis. I was hoping some helpful person would give me a free script to scrape the site, but all I got was a comment lol"}
{"id": "nafkeip", "type": "comment", "parent_id": "t1_naf5025", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafkeip/", "author": "laataisu", "created_utc": 1756051504, "score": 1, "content": "Already did that; I tried Playwright MCP, Context7, and BrowserMCP, and none of them worked. Playwright, Selenium, Nodriver."}
{"id": "nafxtng", "type": "comment", "parent_id": "t1_nafb9y1", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafxtng/", "author": "webscraping-ModTeam", "created_utc": 1756055577, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "nagnomt", "type": "comment", "parent_id": "t1_nag01i0", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nagnomt/", "author": "webscraping-ModTeam", "created_utc": 1756063490, "score": 1, "content": "Please review the sub rules"}
{"id": "najwldg", "type": "comment", "parent_id": "t1_najwiyx", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najwldg/", "author": "RemindMeBot", "created_utc": 1756111956, "score": 1, "content": "I will be messaging you in 1 month on [**2025-09-25 08:51:57 UTC**]( to remind you of [**this link**]( [**CLICK THIS LINK**]( to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)]( ***** |[^(Info)]( Reminders)]( |-|-|-|-|"}
{"id": "navdipl", "type": "comment", "parent_id": "t1_nauehlk", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/navdipl/", "author": "webscraping-ModTeam", "created_utc": 1756257917, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "nafh51i", "type": "comment", "parent_id": "t1_nafgy7s", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafh51i/", "author": "beachguy82", "created_utc": 1756050487, "score": 9, "content": "I\u2019m talking about a working process. Both are just ways to collect data from websites. Use what works."}
{"id": "nbkbtak", "type": "comment", "parent_id": "t1_nbhx2x4", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nbkbtak/", "author": "webscraping-ModTeam", "created_utc": 1756591360, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "najuauv", "type": "comment", "parent_id": "t1_najtt2s", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najuauv/", "author": "sleepWOW", "created_utc": 1756110566, "score": 4, "content": "sure. first of all, im using undetected\\_chrome driver and i use headless browser. # Configure Chrome options for stealth and headless mode options = uc.ChromeOptions() # Enable headless mode options.add_argument('--headless=new') # Use new headless mode # Basic stealth options options.add_argument('--no-sandbox') options.add_argument('--disable-dev-shm-usage') options.add_argument('--disable-blink-features=AutomationControlled') # Additional anti-detection measures options.add_argument('--disable-web-security') options.add_argument('--allow-running-insecure-content') options.add_argument('--disable-extensions') options.add_argument('--disable-plugins') options.add_argument('--disable-images') # Faster loading"}
{"id": "nafg3t6", "type": "comment", "parent_id": "t1_nafafvo", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafg3t6/", "author": "bigzyg33k", "created_utc": 1756050173, "score": 4, "content": "Yep. Not sure why I was downvoted, \u201cAI web scraping\u201d just means using AI to analyse scraped data or orchestrate the scraping process. It doesn\u2019t mean \u201cI used AI to vibe code a scraper and it didn\u2019t work\u201d"}
{"id": "nafkl54", "type": "comment", "parent_id": "t1_nafafvo", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nafkl54/", "author": "laataisu", "created_utc": 1756051559, "score": 1, "content": "bro im from a third world country and not native speaker, if im using my grammar you wont understand it"}
{"id": "najuesm", "type": "comment", "parent_id": "t1_najuauv", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najuesm/", "author": "sleepWOW", "created_utc": 1756110631, "score": 4, "content": "below is my script for the bypass: def bypass_cloudflare( driver , url , max_retries =3): \"\"\"Attempt to bypass Cloudflare protection\"\"\" for attempt in range(max_retries): try : logger.info(f\"Attempting to load {url} (attempt {attempt + 1}/{max_retries})\") driver.get(url) human_like_delay(3, 7) # Wait for potential Cloudflare challenge # Check if we're on a Cloudflare challenge page if \"cloudflare\" in driver.current_url.lower() or \"checking your browser\" in driver.page_source.lower(): logger.info(\"Cloudflare challenge detected, waiting...\") # Wait for challenge to complete (up to 30 seconds) for i in range(30): time.sleep(1) if \"cloudflare\" not in driver.current_url.lower() and \"checking your browser\" not in driver.page_source.lower(): logger.info(\"Cloudflare challenge passed!\") break if i == 29: logger.warning(\"Cloudflare challenge timeout\") continue # Check if page loaded successfully if \"car.gr\" in driver.current_url: logger.info(\"Page loaded successfully\") return True except Exception as e: logger.error(f\"Error loading page (attempt {attempt + 1}): {e}\") human_like_delay(5, 10) logger.error(f\"Failed to load {url} after {max_retries} attempts\") return False"}
{"id": "nagdsk2", "type": "comment", "parent_id": "t1_nafkl54", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nagdsk2/", "author": "smoke4sanity", "created_utc": 1756060365, "score": 2, "content": "Fair enough. My apologies."}
{"id": "najupjd", "type": "comment", "parent_id": "t1_najuesm", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najupjd/", "author": "hackbyown", "created_utc": 1756110816, "score": 1, "content": "Nice bro, don't you face any issue in using proxies with undectected chromedriver"}
{"id": "najuxks", "type": "comment", "parent_id": "t1_najupjd", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najuxks/", "author": "sleepWOW", "created_utc": 1756110955, "score": 2, "content": "I have set up an Ubuntu VM on digital ocean so I guess it\u2019s working out well for me. Sometimes the website I scrape gives me a 402 error for exceeding limit request. I simply change the public IP of the VM and I continue scraping. Alright, I need to check the logs a few times a day to make sure it\u2019s running."}
{"id": "najv42f", "type": "comment", "parent_id": "t1_najuxks", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/najv42f/", "author": "hackbyown", "created_utc": 1756111066, "score": 2, "content": "Yes that's a very great approach for bypass rate limiting if you control vm."}
{"id": "nbsc4pc", "type": "comment", "parent_id": "t1_najuxks", "permalink": "https://www.reddit.com/r/webscraping/comments/1mymxs6/tried_ai_for_realworld_scraping_its_basically/nbsc4pc/", "author": "No_Outside_9446", "created_utc": 1756702831, "score": 1, "content": "Any way to share your script for web scraper? github Thanks"}
{"id": "1l2zcu8", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/", "author": "antvas", "created_utc": 1749021737, "score": 91, "title": "What TikTok\u2019s virtual machine tells us about modern bot defenses", "content": "**Author here:** There\u2019ve been a lot of Hacker News threads lately about scraping, especially in the context of AI, and with them, a fair amount of confusion about what actually works to stop bots on high-profile websites. In general, I feel like a lot of people, even in tech, don\u2019t fully appreciate what it takes to block modern bots. You\u2019ll often see comments like \u201cjust enforce JavaScript\u201d or \u201cuse a simple proof-of-work,\u201d without acknowledging that attackers won\u2019t stop there. They\u2019ll reverse engineer the client logic, reimplement the PoW in Python or Node, and forge a valid payload that works at scale. In my latest blog post, I use TikTok\u2019s obfuscated JavaScript VM (recently discussed on HN) as a case study to walk through what bot defenses actually look like in practice. It\u2019s not spyware, it\u2019s an anti-bot layer aimed at making life harder for HTTP clients and non-browser automation. Key points: * HTTP-based bots skip JS, so TikTok hides detection logic inside a JavaScript VM interpreter * The VM computes signals like webdriver checks and canvas-based fingerprinting * Obfuscating this logic in a custom VM makes it significantly harder to reimplement outside the browser (and thus harder to scale) The goal isn\u2019t to stop all bots. It\u2019s to force attackers into full browser automation, which is slower, more expensive, and easier to fingerprint. The post also covers why naive strategies like \u201cjust require JS\u201d don\u2019t hold up, and why defenders increasingly use VM-based obfuscation to increase attacker cost and reduce replayability."}
{"id": "mvwz8xk", "type": "comment", "parent_id": "t3_1l2zcu8", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvwz8xk/", "author": "p3r3lin", "created_utc": 1749024116, "score": 8, "content": "btw: the original repo is down (DMCA takedown by github). Any mirrors? ["}
{"id": "mvwxf8u", "type": "comment", "parent_id": "t3_1l2zcu8", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvwxf8u/", "author": "nib1nt", "created_utc": 1749023026, "score": 24, "content": "Stop calling web scrapers \"attackers\"."}
{"id": "mvwyh27", "type": "comment", "parent_id": "t3_1l2zcu8", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvwyh27/", "author": "p3r3lin", "created_utc": 1749023654, "score": 2, "content": "Thanks for sharing! Recently had to battle an SMS Pumping attack by a Russian bot net. Thankfully not super sophisticated. Still gave us headache. Especially worrisome that Cloudflare Turnstile Capcha did not help a lot."}
{"id": "mw1mbvj", "type": "comment", "parent_id": "t3_1l2zcu8", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw1mbvj/", "author": "MtSnowden", "created_utc": 1749080355, "score": 2, "content": "I\u2019m currently trying to get past Akamai\u2019s bot detection. Any tips lol"}
{"id": "mw3rzos", "type": "comment", "parent_id": "t3_1l2zcu8", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw3rzos/", "author": "ScraperAPI", "created_utc": 1749114729, "score": 1, "content": "Read this and thoroughly enjoyed the technical depth. This is understandable for Tiktok as they need to prevent sybils and other algorithmic manipulations on the platform. Also, the practical application of obsfucation into their VM is actually impressive -- even though it is not foolproof from a technical standpoint. But a question, how then do you think Tiktok can balance blocking attackers and allowing honest scrapers to get data from the platform?"}
{"id": "mwr8p6z", "type": "comment", "parent_id": "t3_1l2zcu8", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mwr8p6z/", "author": "No-Resolution4685", "created_utc": 1749432410, "score": 1, "content": "\\^\\^"}
{"id": "mvyh464", "type": "comment", "parent_id": "t1_mvwz8xk", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvyh464/", "author": "Historical_Yellow_17", "created_utc": 1749047314, "score": 4, "content": "[ yep here you go"}
{"id": "mvx0q4i", "type": "comment", "parent_id": "t1_mvwz8xk", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvx0q4i/", "author": "antvas", "created_utc": 1749025026, "score": 2, "content": "Unfortunately no :("}
{"id": "mvwzq0r", "type": "comment", "parent_id": "t1_mvwxf8u", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvwzq0r/", "author": "antvas", "created_utc": 1749024409, "score": 12, "content": "I see the confusion. When I talk about attackers, it's more like a generic term for unwanted (from the website's POV) bots making requests to a website. However, I do agree that from a legal and ethical POV, there is a huge difference between scraping/scalping and credential stuffing/payment fraud, for example."}
{"id": "mvwz4cr", "type": "comment", "parent_id": "t1_mvwxf8u", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvwz4cr/", "author": "p3r3lin", "created_utc": 1749024039, "score": 3, "content": "Where have you read this? I have not seen OP calling web scrapers attackers. Also: if a bot/automation (of whatever kind) is deemed an attacker and counter measures are needed is in the discretion of the bot/automation target. That being said: ethical / white hat web scraping is a relevant and necessary part of our information economy. And most jurisdictions deem it as such."}
{"id": "mwl4wou", "type": "comment", "parent_id": "t1_mvwxf8u", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mwl4wou/", "author": "Aidan_Welch", "created_utc": 1749346875, "score": 1, "content": "Well for a lot of sites you are effectively in that: you drain resources, you potentially undercut their business, you gather data that they don't want you to have that easily. That doesn't mean it should be illegal though, it shouldn't. But it totally makes sense that companies implemented anti-scraping measures. That's just part of the game."}
{"id": "mw3g35c", "type": "comment", "parent_id": "t1_mw1mbvj", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw3g35c/", "author": "antvas", "created_utc": 1749107554, "score": 1, "content": "As you can imagine, my answer is no ;)"}
{"id": "mw3t3qn", "type": "comment", "parent_id": "t1_mw3rzos", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw3t3qn/", "author": "antvas", "created_utc": 1749115401, "score": 2, "content": "Thanks for the feedback. \"But a question, how then do you think Tiktok can balance blocking attackers and allowing honest scrapers to get data from the platform?\" When it comes to good bot vs bad bots, particularly for scraping, it's more a matter of perspective from the website POV. Do they benefit from being scraped by a bot? In case of Google bots, most websites seem to agree that they benefit by allowing Google scrape them. For scrapers used to train LLMs, it's more blurry. Some websites consider they benefit from it and allow the scrapers, while others block them. By default most websites will block all bots from which they see no value, then then will allow scrapers from which they can benefit or partners using strong authentications mechanisms like IP address, reverse DNS or tokens. Companies like Cloudflare are also proposing new standards to make it safer and easier to authenticate good bots/AI agents: ["}
{"id": "mvyljev", "type": "comment", "parent_id": "t1_mvyh464", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvyljev/", "author": "p3r3lin", "created_utc": 1749048592, "score": 1, "content": "Very cool, thx!"}
{"id": "mvxhktt", "type": "comment", "parent_id": "t1_mvwz4cr", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvxhktt/", "author": "RobSm", "created_utc": 1749034501, "score": 1, "content": "Then read his post again. Not only here, but in his blogs he always emphasises web crawlers as attackers and applies negative badge. This is deliberately, to force readers (website owners) to think these evil bots are doing harm and so they need to buy his services then. So he pumps these posts, linking to his blogs, to promote his business of 'fighting attackers'."}
{"id": "mw4o537", "type": "comment", "parent_id": "t1_mw3t3qn", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw4o537/", "author": "ScraperAPI", "created_utc": 1749129138, "score": 1, "content": "Totally understandable from a business PoV. Was simply more concerned for genuine Tiktok SaaS or businesses that might need to scrape data to analyze sentiment and customer taste. For this set of people, we suppose their scraping endeavors is a net positive for Tiktok as they will also pump more content and probably paid ads on the platform."}
{"id": "mvyu5s9", "type": "comment", "parent_id": "t1_mvxhktt", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvyu5s9/", "author": "t0astter", "created_utc": 1749051042, "score": 4, "content": "Bots CAN and DO cause harm, though. Anything from unwanted server load/resource consumption (API credits?) to creating unfair advantages for certain customers to using data gained from a website that the author didn't want used in other ways."}
{"id": "mvzt5zy", "type": "comment", "parent_id": "t1_mvyu5s9", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mvzt5zy/", "author": "p3r3lin", "created_utc": 1749060841, "score": 3, "content": "Full agree here. Ethical Web Scraping is valuable and has its place, but most Bots and Bot-Nets are out there to cause economic harm and danger to small companies and their employees. As a web service operator Im quite thankful that there are services that I can deploy against fully automated attempts to create hundreds/thousands of accounts and cost me money by eg pumping my SMS bill or increasing my token cost. And as a webscraper myself (why else should I be in this sub) I have almost never seen a website (outside of hyper scalers) that can really protect their data against hand crafted, small scale, cautious simple data scraping. Because these are truly two different things. Is the automation using my resources and costing me money or is it just grabbing some data that everyone can access already easily? The r/webscraping Beginners Guide has a good guideline about ethical (and legal) behaviour: ["}
{"id": "mw0ad1m", "type": "comment", "parent_id": "t1_mvyu5s9", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw0ad1m/", "author": "RobSm", "created_utc": 1749065746, "score": 2, "content": "Some can, some cannot. Don't put everything under one umbrella. Also, you can get data without bots. Does this mean then it is OK? Bot is just technical implementation. You request data, server agrees and provides. Your chrome browser is your bot."}
{"id": "mw0anwr", "type": "comment", "parent_id": "t1_mvzt5zy", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw0anwr/", "author": "RobSm", "created_utc": 1749065833, "score": -2, "content": "> but most Bots and Bot-Nets are out there to cause economic harm and danger to small companies and their employees Total and utter BS. But antoine convinced you that 'they are out there to harm you' (no idea why, but who cares), so prepare to pay him for his 'anti-bot' services, IP ranges and other crap."}
{"id": "mw0e37o", "type": "comment", "parent_id": "t1_mw0anwr", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mw0e37o/", "author": "p3r3lin", "created_utc": 1749066814, "score": 2, "content": "Well just the other week I and my team spend multiple days to fight of a SMS pumping Bot-Net spamming us with thousands of malicious requests per day. It seems you are quite unaware of real world cyber security risks. Not sure what you are trying to defend here. I dont know who \"antoine\" is. Probably the owner of castle.io? Do you have a personal feud? I recommend spending some time in actual web businesses that pay peoples livelihoods before you are making such statements as \"Total and utter BS\" about things you obviously know little about. Quite immature tbh."}
{"id": "mwpxv5g", "type": "comment", "parent_id": "t1_mw0anwr", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mwpxv5g/", "author": "Warguy387", "created_utc": 1749416241, "score": 1, "content": "found the botnet owner"}
{"id": "mwshk1r", "type": "comment", "parent_id": "t1_mwpxv5g", "permalink": "https://www.reddit.com/r/webscraping/comments/1l2zcu8/what_tiktoks_virtual_machine_tells_us_about/mwshk1r/", "author": "RobSm", "created_utc": 1749452540, "score": 1, "content": "I see you are another victim of antoine sales tactics"}
{"id": "1kywx8z", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/", "author": "New_Needleworker7830", "created_utc": 1748585128, "score": 94, "title": "Project for fast scraping of thousands of websites", "content": "Ciao a tutti, I\u2019m working on a Python module for scraping/crawling/spidering. I needed something fast when you have 100-10000 of websites to scrape and it happened to me already 3-4 times - whether for email gathering or e-commerce or any kind of information - so I packed it till with just 2 simple lines of code you fetch all of them at high speed. It features a separated queue system to avoid congestion, spreads requests across the same domain, and supports retries with different backends (currently ** and **curl** via subprocess for HTTP/2; Seleniumbase support coming soon, but at last chance because would reduce the speed 1000 times). It also gets robots and sitemaps, provides full JSON logging for each request, and can run multiprocess and multithreaded workflows in parallel while collecting stats, and more. It works also just for one website, but it\u2019s more efficient when more websites are scraped. I tested it on 150 k websites on Linux and macOS, and it performed very well. If you want to have a look, join, test, suggest, you can look for \u201cispider\u201d on PyPI - \u201ci\u201d stands for \u201cItalian,\u201d because I\u2019m Italian and we\u2019re known for fast cars. Feedback and issue reports are welcome! Let me know if you spot any bugs or missing features. Or tell me your ideas!"}
{"id": "mv1vsjs", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv1vsjs/", "author": "renegat0x0", "created_utc": 1748605829, "score": 8, "content": "Thanks for sharing code. \\- thanks to you I have incorporated into my own project [ (but I will commit these changes after \\~3 hours) \\- I already support selenium. If you like you can check how I am doing it and use this knowledge \\- I also support selenium undetected, and curl-cffi. Selenium has some quirks about how full browser can be started, or how status code can be obtained. I am not laser focused on speed though. I am running a crawler on RPI5, so ... yeah... that's that. I also have no advanced support for proxies, or scraping, because I provide HTTP url data exchange. but maybe you will find something useful here."}
{"id": "mv0vjqj", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv0vjqj/", "author": "Silly-Fall-393", "created_utc": 1748586394, "score": 3, "content": "thanks, will check it out.. why not on github? btw"}
{"id": "mv1171c", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv1171c/", "author": "steb2k", "created_utc": 1748589609, "score": 2, "content": "why this over scrapy?"}
{"id": "mv1ymt5", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv1ymt5/", "author": "Unlikely_Track_5154", "created_utc": 1748606945, "score": 2, "content": "Funny, I built the same idea recently. Why no aiomultiprocess? Why no workers like aioredis or something similar?"}
{"id": "mv27vpy", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv27vpy/", "author": "shoebill_homelab", "created_utc": 1748610257, "score": 2, "content": "Great work."}
{"id": "mv11zh2", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv11zh2/", "author": "RobSm", "created_utc": 1748590070, "score": 1, "content": "I thought Germans had fast cars, not Italian fIat."}
{"id": "mv1ogk6", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv1ogk6/", "author": "External_Skirt9918", "created_utc": 1748602681, "score": 1, "content": "How you overcome captaca"}
{"id": "mv5hk0e", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv5hk0e/", "author": "bigcherish", "created_utc": 1748645054, "score": 1, "content": "Following"}
{"id": "mvdk8ty", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mvdk8ty/", "author": "Spare-Solution-787", "created_utc": 1748761048, "score": 1, "content": "How do you bypass or handle cloudflare?"}
{"id": "mviljf6", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mviljf6/", "author": "ThatMobileTrip", "created_utc": 1748828039, "score": 1, "content": "Now I want to try it. Forza Italia!"}
{"id": "n19fwt0", "type": "comment", "parent_id": "t3_1kywx8z", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/n19fwt0/", "author": "None", "created_utc": 1751609194, "score": 1, "content": "[removed]"}
{"id": "mv0vpej", "type": "comment", "parent_id": "t1_mv0vjqj", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv0vpej/", "author": "New_Needleworker7830", "created_utc": 1748586483, "score": 5, "content": "Sure! It's on GitHub too: But if you just want to try it out, you can install it with: pip install ispider In a virtual environment"}
{"id": "mv15t1b", "type": "comment", "parent_id": "t1_mv1171c", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv15t1b/", "author": "New_Needleworker7830", "created_utc": 1748592310, "score": 12, "content": "Out of the box - it is multicore, - It\u2019s around 10 times faster then scrapy (I got 35000 URLs/min on a hetzner server with 32 cores) - it\u2019s just 2 lines to execute - it just saves all the html files, parsing is in a separate stage - json logs are more complete than the scrapy out of the box, they can be inserted on a db table and analyzed to understand and solve connection errors (if needed) Scrapy is more customizable, and i use it for automations on pipelines, because i consider it more stable. But if you need \u201cone time run\u201d to get the complete websites, I think ispider is easier and faster"}
{"id": "mv3u18r", "type": "comment", "parent_id": "t1_mv1ymt5", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv3u18r/", "author": "New_Needleworker7830", "created_utc": 1748627084, "score": 1, "content": "Checking, I agree that aiomultiprocess would reduce 1 step complexity, because it manages multicore under the hood, but I never used it that's why I didnt take it consideration.. I'll check it. I had a version supporting kafka as a queue, but not with aioredis.. I tested this using kafka as a queue and was performig pretty well. I will check this too."}
{"id": "mv16pc4", "type": "comment", "parent_id": "t1_mv11zh2", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv16pc4/", "author": "New_Needleworker7830", "created_utc": 1748592842, "score": 2, "content": "![gif](giphy|1pJweMF8jbOMtmNz3m|downsized)"}
{"id": "mv3nvyd", "type": "comment", "parent_id": "t1_mv1ogk6", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv3nvyd/", "author": "New_Needleworker7830", "created_utc": 1748625396, "score": 1, "content": "It does not, spidering 100-10 billions domains, means to accept to don't overcome captcha.. It's a different approach of spidering, on big numbers with \"acceptable losses\" when websites has captchas, based on speed and not on quality. It depends on project you are working on."}
{"id": "n19g5ft", "type": "comment", "parent_id": "t1_n19fwt0", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/n19g5ft/", "author": "webscraping-ModTeam", "created_utc": 1751609320, "score": 1, "content": "Thank you for your interest in r/webscraping! We noticed your recent post lacks the detail necessary for our community to effectively help you. To maintain the quality of discussions and assistance, we have removed your post. Please take a moment to review the beginners guide at before posting again. When you're ready, ensure your next post includes: * **Website URL**: The specific page you're interested in. * **Data Points**: A clear list of the data you want to extract (e.g., product names, prices, descriptions). * **Project Description**: A brief overview of your project or the problem you're trying to solve. We look forward to your next post and are excited to help you with your web scraping needs!"}
{"id": "mv5n3kz", "type": "comment", "parent_id": "t1_mv3u18r", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv5n3kz/", "author": "Unlikely_Track_5154", "created_utc": 1748646937, "score": 2, "content": "It really isn't an issue. I was just wondering because you seem to have a less dependency driven version than I do, so I was wondering what the deal was. I can't remember why I decided against Kafka. I built mine to use cookie cutter, it basically works like scrapy when you make the new pipeline ( can't remember what it is called ), but for any code to plugin to the async multiprocessing scaffold. Other than that, basically the same idea, I just got tired of having 9000 different disparate scripts, so I built it into one system."}
{"id": "mvka6jn", "type": "comment", "parent_id": "t1_mv3u18r", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mvka6jn/", "author": "Tiny_Arugula_5648", "created_utc": 1748858688, "score": 1, "content": "Using Kafka as a queue is like using a Ferrari to move a couch.. Kafka isn't a message queue it's a real time streaming/processing engine. You could have written your scraper to run all in Kafka and not needed to handle any of the orchestration that you built."}
{"id": "mv25ty0", "type": "comment", "parent_id": "t1_mv16pc4", "permalink": "https://www.reddit.com/r/webscraping/comments/1kywx8z/project_for_fast_scraping_of_thousands_of_websites/mv25ty0/", "author": "RobSm", "created_utc": 1748609558, "score": 1, "content": "Great not italian car"}
{"id": "1i80fuk", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/", "author": "convicted_redditor", "created_utc": 1737628134, "score": 92, "title": "I just created an amazon product scraper", "content": "I developed a Python package called AmzPy, which is an Amazon product scraper. I created it for one of my SaaS projects that required Amazon product data. Despite having API credentials, Amazon didn\u2019t grant me access to its API, so I ended up scraping the data I needed and packaged it into a library. See it at [ Github: [ Currently, AmzPy scrapes product details, but I plan to add features like scraping reviews or search results. Developers can also fork the project and contribute by adding more features."}
{"id": "m8pgsv1", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8pgsv1/", "author": "Main-Position-2007", "created_utc": 1737631652, "score": 10, "content": "hey appreciate your effort, but it\u2019s not a scalable solution. You will run into bans very quickly, since no js is rendered. You could look into Also reviews could only be scraped when you have a logged in session."}
{"id": "m8qdms3", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8qdms3/", "author": "Commercial_Isopod_45", "created_utc": 1737644354, "score": 3, "content": "Can i know what isyour goal behind product scraper? To sell details or what?"}
{"id": "m8to8xz", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8to8xz/", "author": "russellvt", "created_utc": 1737677817, "score": 3, "content": "Pretty sure you'll run in to some Amazon ToS issues pretty quickly."}
{"id": "m8rpwsv", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8rpwsv/", "author": "pcshady", "created_utc": 1737657866, "score": 2, "content": "Can you tell more about anti bot protection? What are you using to achieve that"}
{"id": "m91958m", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m91958m/", "author": "TJ51097", "created_utc": 1737775831, "score": 2, "content": "That's great!!"}
{"id": "m8svz7l", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8svz7l/", "author": "None", "created_utc": 1737669323, "score": 1, "content": "[removed]"}
{"id": "m8vt93k", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8vt93k/", "author": "gangusgoose", "created_utc": 1737710182, "score": 1, "content": "Could amazons api, grant me access to getting product info, features and reviews for multiple store fronts? Or is this something I need to scrap"}
{"id": "mhtoexh", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/mhtoexh/", "author": "After_Foundation_207", "created_utc": 1741989818, "score": 1, "content": "Wondering if its possible to pull product Ingredients from an Amazon product listing if they exist?"}
{"id": "mnsddfn", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/mnsddfn/", "author": "convicted_redditor", "created_utc": 1744995678, "score": 1, "content": "Update: I did a revamp. Using curl\\_cffi instead of my own stealthkit (as it didn't work flawlessly) and with the help of Cursor, I added search page url functionality instead of just scraping product page. Check out here: [ New search feature: from amzpy import AmazonScraper # Create scraper for a specific Amazon domain scraper = AmazonScraper(country_code=\"in\") # Search by query - get up to 2 pages of results products = scraper.search_products(query=\"wireless earbuds\", max_pages=2)"}
{"id": "n1v91l4", "type": "comment", "parent_id": "t3_1i80fuk", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/n1v91l4/", "author": "None", "created_utc": 1751920204, "score": 1, "content": "[removed]"}
{"id": "m8pitb0", "type": "comment", "parent_id": "t1_m8pgsv1", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8pitb0/", "author": "convicted_redditor", "created_utc": 1737632713, "score": 0, "content": "Thanks for sharing your feedback. As i wrote in the post, I created it to solve my own problem of getting product data - reviews are not planned as of now\u2026 And yes, that's also true that it works today, and might not tomorrow :/"}
{"id": "m8qm1ba", "type": "comment", "parent_id": "t1_m8qdms3", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8qm1ba/", "author": "convicted_redditor", "created_utc": 1737646868, "score": 1, "content": "My use is to get product details in one click to add amazon product to my saas\u2019 posts in just one click. So that users dont have to upload and image, write title, and product price."}
{"id": "m8rxrr3", "type": "comment", "parent_id": "t1_m8rpwsv", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8rxrr3/", "author": "convicted_redditor", "created_utc": 1737659999, "score": 1, "content": "Different headers in each get request. See it here: ["}
{"id": "m8t4gs3", "type": "comment", "parent_id": "t1_m8svz7l", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8t4gs3/", "author": "webscraping-ModTeam", "created_utc": 1737671741, "score": 0, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m8vusal", "type": "comment", "parent_id": "t1_m8vt93k", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8vusal/", "author": "convicted_redditor", "created_utc": 1737711141, "score": 1, "content": "I have its API access, but they never work. So I had to scrape to make my app working."}
{"id": "n1vh1n6", "type": "comment", "parent_id": "t1_n1v91l4", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/n1vh1n6/", "author": "webscraping-ModTeam", "created_utc": 1751922911, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "m8r1e8i", "type": "comment", "parent_id": "t1_m8pitb0", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8r1e8i/", "author": "QuackDebugger", "created_utc": 1737651160, "score": 7, "content": "If I'm understanding correctly, that contradicts what you wrote in your post that you plan to add features like scraping reviews."}
{"id": "m8vp9on", "type": "comment", "parent_id": "t1_m8rxrr3", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8vp9on/", "author": "reyarama", "created_utc": 1737707692, "score": 1, "content": "But same IP for each request, right? Do you run into any rate limiting?"}
{"id": "m8rh5t2", "type": "comment", "parent_id": "t1_m8r1e8i", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8rh5t2/", "author": "convicted_redditor", "created_utc": 1737655468, "score": 0, "content": "Sorry, my bad. I meant reviews are not planned as of now."}
{"id": "m8vrjlx", "type": "comment", "parent_id": "t1_m8vp9on", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8vrjlx/", "author": "convicted_redditor", "created_utc": 1737709120, "score": 1, "content": "Yes. Never ran into rate limiting as I am not scraping a lot of products in one go."}
{"id": "m8ymo2k", "type": "comment", "parent_id": "t1_m8vrjlx", "permalink": "https://www.reddit.com/r/webscraping/comments/1i80fuk/i_just_created_an_amazon_product_scraper/m8ymo2k/", "author": "backflipbail", "created_utc": 1737746140, "score": 1, "content": "I don't know much about web scraping but wouldn't making multiple requests from the same IP with rotation UA headers look more suspicious? Wouldn't you be better changing the UA every 5 mins or at the start of a new process so it looks like a new user?"}
{"id": "1g40qy2", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/", "author": "Dapper-Profession552", "created_utc": 1728970781, "score": 89, "title": "I made a Cloudflare-Bypass", "content": "This cloudflare bypass consists of accessing the site and obtaining the cf_clearance cookie And it works with any website. If anyone tries this and gets an error, let me know."}
{"id": "ls5izhs", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5izhs/", "author": "collector-ai", "created_utc": 1729052414, "score": 10, "content": "Very cool! Can you explain a bit more regarding how cloudflare works and how the bypass works? Unsure of the internals of cloudflare."}
{"id": "ls4pltf", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls4pltf/", "author": "brianjenkins94", "created_utc": 1729040156, "score": 8, "content": "Why upload it obfuscated/minified?"}
{"id": "ls5qotc", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5qotc/", "author": "nostorian_", "created_utc": 1729056612, "score": 3, "content": "Last time I tried extracting cf clearance I don't remember coming across any obfuscated cloudfare js files iirc for discord it was just some url where on redirection you use regex to scrape params and then use them on another request to get the clearance cookie. It was the same way in another site as well is there something I am missing out on since that worked as well?"}
{"id": "ls5soe8", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5soe8/", "author": "ProgramerAnel", "created_utc": 1729057810, "score": 3, "content": "I was paying before to ["}
{"id": "ls4abcl", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls4abcl/", "author": "Zealousideal_Set_333", "created_utc": 1729034436, "score": 2, "content": "Perfect, thanks for sharing. This is exactly the solution I need for a project I'm currently working on. I'll try it out later and let you know if there's any error."}
{"id": "lse1czu", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lse1czu/", "author": "sage74", "created_utc": 1729184171, "score": 2, "content": "for what version of cf it works? I tried to use with these 2 examples and it does not work got `spli1 = r.split(\"ah='\")[1].split(',')` `IndexError: list index out of range` [ ["}
{"id": "ls5g761", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5g761/", "author": "SUPERMETROMAN", "created_utc": 1729051029, "score": 1, "content": "Can this be used with proxies? Afaik cf_clearance gets voided automatically when used by a different proxy"}
{"id": "ls5vcgk", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5vcgk/", "author": "Sp4rkiop", "created_utc": 1729059474, "score": 1, "content": "What time it takes to get the token after a request"}
{"id": "ls6t7u4", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls6t7u4/", "author": "Glittering_Push8905", "created_utc": 1729080577, "score": 1, "content": "You are a saviour"}
{"id": "ls7qpdy", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls7qpdy/", "author": "Throwawayforgainz99", "created_utc": 1729092760, "score": 1, "content": "Can you explain more about how you did this? I\u2019m familiar with web scraping and use Python daily. But this reverse engineering stuff seems really cool. Did you have to use some sort of decryption or something?"}
{"id": "ls8j3oy", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8j3oy/", "author": "M0le5ter", "created_utc": 1729101713, "score": 1, "content": "I tried this for the [gitlab.com/user/sign\\_in]( page. I opened the browser using Puppeteer and set the cookie 'cf\\_clearance' to the value generated by CF\\_Solver('[ After refreshing the page, Cloudflare still wasn't bypassed. Can anyone help me correct this?"}
{"id": "lsab9nx", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsab9nx/", "author": "UniqueAttourney", "created_utc": 1729123268, "score": 1, "content": "is it a python only lib ? or js also ?"}
{"id": "lse6azi", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lse6azi/", "author": "Unhappy_Bathroom_767", "created_utc": 1729185729, "score": 1, "content": "What should i do when obtain this cookie? Import in my navigator?"}
{"id": "lsft2jb", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsft2jb/", "author": "No_River_8171", "created_utc": 1729204922, "score": 1, "content": "Man i wished I did this code \u2026. C keeping me so buuuusy"}
{"id": "lsk77m0", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsk77m0/", "author": "s1ayer2309", "created_utc": 1729273654, "score": 1, "content": "This is not a bypass lol, this is just extracting the cookie. Bypassing cloudflare involves TLS configuration, captcha extraction, CF version detection, handshakes, and a whole lot more."}
{"id": "lv5d552", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lv5d552/", "author": "rezan_reddit", "created_utc": 1730621233, "score": 1, "content": "Hey, is he using a browser to get the cf cookie? and do i need a big server to generate multiple cookies?"}
{"id": "lvca9r4", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvca9r4/", "author": "jcgdata", "created_utc": 1730725490, "score": 1, "content": "For some reason, could not make it work. Whereas puppeteer-real-browser worked perfectly."}
{"id": "lviusi5", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lviusi5/", "author": "None", "created_utc": 1730815628, "score": 1, "content": "Thanks for sharing this. I had written a tool to get past Incapsula. But I never was able to figure out how to decode their js, so it was a one trick pony. Funny enough I was able to use the same approach, and work my way through an Oracle SSO sign on. But also a one trick pony. Cloudflare has been a tough nut to crack. I'm excited to try this."}
{"id": "lz4bbwg", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lz4bbwg/", "author": "syne01", "created_utc": 1732647889, "score": 1, "content": "does this still work when the turnstyle is in a state of error, such as when it has an error for invalid domain? Is there a way to bypass the turnstyle that has the invalid domain error?"}
{"id": "lzg9agk", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lzg9agk/", "author": "None", "created_utc": 1732822090, "score": 1, "content": "[deleted]"}
{"id": "m465h4k", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/m465h4k/", "author": "lazur2006", "created_utc": 1735377520, "score": 1, "content": "What\u2019s about Turnstile Bypass? Are there any news?"}
{"id": "mc19cn7", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/mc19cn7/", "author": "Acceptable_Quail4053", "created_utc": 1739204070, "score": 1, "content": "Does this work for cloudfare turnstiles ?"}
{"id": "mfjkqtf", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/mfjkqtf/", "author": "SubstantialTalk6198", "created_utc": 1740884299, "score": 1, "content": "is it working on CF WAF too?"}
{"id": "mhdgs8s", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/mhdgs8s/", "author": "Menxii", "created_utc": 1741782460, "score": 1, "content": "Thank you ! Can I use this with requests ? I have a cookie and i need to just add the cf\\_clearance part."}
{"id": "mkru972", "type": "comment", "parent_id": "t3_1g40qy2", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/mkru972/", "author": "ImportantExternal750", "created_utc": 1743464290, "score": 1, "content": "Is there any way to use this if I'm using playwrigth in my node.js project and not python?"}
{"id": "ls5o6h8", "type": "comment", "parent_id": "t1_ls5izhs", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5o6h8/", "author": "RacoonInThePool", "created_utc": 1729055185, "score": 4, "content": "I am really curious about the technique. How can they figure out the idea to bypass these"}
{"id": "lsaxf6d", "type": "comment", "parent_id": "t1_ls5izhs", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsaxf6d/", "author": "None", "created_utc": 1729131748, "score": 5, "content": "[removed]"}
{"id": "ls4stoe", "type": "comment", "parent_id": "t1_ls4pltf", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls4stoe/", "author": "Dapper-Profession552", "created_utc": 1729041387, "score": 2, "content": "Well, I found it easy to analyze and do it, that's why I didn't want to obfuscate it."}
{"id": "ls5rwj5", "type": "comment", "parent_id": "t1_ls5qotc", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5rwj5/", "author": "Dapper-Profession552", "created_utc": 1729057338, "score": 2, "content": "If you already had cf_clearance stored on the website, you won't be able to search Cloudflare JS files. Unless you delete data from the website, Cloudflare stores that cookie for the first time when you enter the site. What I did is extract 2 parameters needed to get the cf_clearance"}
{"id": "m7fsgvl", "type": "comment", "parent_id": "t1_ls5soe8", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/m7fsgvl/", "author": "PawsAndRecreation", "created_utc": 1737029600, "score": 1, "content": "Is it still up? Cause I remember he was posting helheim packages on discord, but I am not sure"}
{"id": "lse45q3", "type": "comment", "parent_id": "t1_lse1czu", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lse45q3/", "author": "Dapper-Profession552", "created_utc": 1729185056, "score": 3, "content": "Works with sites that use the \"cf_clearance\" cookie regardless of the captcha. But this website seems to insert the \"cf_clearance\" cookie differently, I'll try to do what I can to fix it"}
{"id": "ls5h5s0", "type": "comment", "parent_id": "t1_ls5g761", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5h5s0/", "author": "Dapper-Profession552", "created_utc": 1729051497, "score": 2, "content": "Oh, I forgot to put a proxy support Wait"}
{"id": "ls5vkml", "type": "comment", "parent_id": "t1_ls5vcgk", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5vkml/", "author": "Dapper-Profession552", "created_utc": 1729059619, "score": 2, "content": "3 - 5 seconds"}
{"id": "ls8n7mu", "type": "comment", "parent_id": "t1_ls7qpdy", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8n7mu/", "author": "joeyx22lm", "created_utc": 1729103007, "score": 3, "content": "It\u2019s quite easy. Many of these libraries exist. Many scrapers just write it in themselves. You can intercept the cloudflare JavaScript file and hook into the cloudflare turnstile JS. Once you have a nonce token, you can submit the turnstile request in exchange for a validated cf session."}
{"id": "ls8168k", "type": "comment", "parent_id": "t1_ls7qpdy", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8168k/", "author": "Dapper-Profession552", "created_utc": 1729096098, "score": 2, "content": "When a website has bot protection, you must use reverse engineering knowledge to find any vulnerability and use that to bypass it. Well, I don't have much to explain, I just analyzed the cloudflare obfuscated code to look for the function that creates the cf_clearance and export it to my project, as a **vulnerability**, and with that I get the cf_clearance, it seems very simple to me"}
{"id": "ls8jn03", "type": "comment", "parent_id": "t1_ls8j3oy", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8jn03/", "author": "Dapper-Profession552", "created_utc": 1729101879, "score": 2, "content": "try use library or use other HTTP Scraper library, like `tls_client` or `curl_cffi`"}
{"id": "lvjql6r", "type": "comment", "parent_id": "t1_ls8j3oy", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvjql6r/", "author": "Suprem3_bot", "created_utc": 1730825443, "score": 1, "content": "use the same user-agent in the script"}
{"id": "lsabdtu", "type": "comment", "parent_id": "t1_lsab9nx", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsabdtu/", "author": "Dapper-Profession552", "created_utc": 1729123312, "score": 1, "content": "python"}
{"id": "lse6u12", "type": "comment", "parent_id": "t1_lse6azi", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lse6u12/", "author": "Dapper-Profession552", "created_utc": 1729185895, "score": 2, "content": "If you are doing a webscraping project, you can use that cookie in this way ``` import from aqua import CF_Solver client = # rest of the code client.cookies['cf_clearance'] = cookie ```"}
{"id": "lskbl6h", "type": "comment", "parent_id": "t1_lsk77m0", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lskbl6h/", "author": "Dapper-Profession552", "created_utc": 1729275053, "score": 1, "content": "I know that's a cookie extractor. But I called it cf bypass for using cloudflare encryption like an vulnerability and then use that to extract that cookie, since it asks me for 2 parameters that are generated from Cloudflare Javascript. \"wb\" and \"s\" I'm currently looking at how the Cloudflare captcha works, to see if I can create a script locally"}
{"id": "lv6joqg", "type": "comment", "parent_id": "t1_lv5d552", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lv6joqg/", "author": "Dapper-Profession552", "created_utc": 1730643915, "score": 1, "content": "No, it only uses HTTP libraries"}
{"id": "lvj995s", "type": "comment", "parent_id": "t1_lvca9r4", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvj995s/", "author": "Dapper-Profession552", "created_utc": 1730820326, "score": 1, "content": "This doesn't work if the website uses cf turnstile. I will update this bypass soon if possible."}
{"id": "lvj90xg", "type": "comment", "parent_id": "t1_lviusi5", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvj90xg/", "author": "Dapper-Profession552", "created_utc": 1730820257, "score": 1, "content": "Fine, but the only detail is that it will not work with websites protected with cf turnstile. I'm currently trying to bypass cf turnstile and will possibly update this library soon."}
{"id": "ls5qoqi", "type": "comment", "parent_id": "t1_ls5o6h8", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5qoqi/", "author": "Dapper-Profession552", "created_utc": 1729056611, "score": 11, "content": "Well, it's very complex. It took me about 1 hour to analyze and read the cloudflare code and its protection against bots When you enter a website for the first time, cloudflare will add the \"cf\\_clearance\" cookie and this will remain in your web browser's data. If you delete data from a website, and then open DevTools and go to the \"Network\" tab, you will see that cloudflare sent a request called \"[ and this URL returns the cf\\_clearance cookie"}
{"id": "lt79ohj", "type": "comment", "parent_id": "t1_lsaxf6d", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lt79ohj/", "author": "Wise_Environment_185", "created_utc": 1729617179, "score": 2, "content": "well - vtempest: i like your approach. Is this doable - i mean can we put these things togehter!?"}
{"id": "lshaby3", "type": "comment", "parent_id": "t1_lsaxf6d", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lshaby3/", "author": "Munich_tal", "created_utc": 1729226082, "score": 1, "content": "Awesome Idea"}
{"id": "ls4tss1", "type": "comment", "parent_id": "t1_ls4stoe", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls4tss1/", "author": "brianjenkins94", "created_utc": 1729041756, "score": 3, "content": "The JavaScript files are unreadable."}
{"id": "m7hk5bn", "type": "comment", "parent_id": "t1_m7fsgvl", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/m7hk5bn/", "author": "ProgramerAnel", "created_utc": 1737050415, "score": 1, "content": "Not using it for a while."}
{"id": "ls5hnr4", "type": "comment", "parent_id": "t1_ls5h5s0", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5hnr4/", "author": "SUPERMETROMAN", "created_utc": 1729051745, "score": 1, "content": "I see. Cool! Yeah, I saw that it also takes a session so that can be a work around for me. I had a hard time solving cloudflare issues, my go through was to load it in a headless browser to get the cf_clearance. Thanks for sharing your project. This is a great solution. I'll definitely try it and implement it in my scrapers."}
{"id": "ls8ntke", "type": "comment", "parent_id": "t1_ls5vkml", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8ntke/", "author": "RobSm", "created_utc": 1729103203, "score": 4, "content": "So it's a headless browser that does the job?"}
{"id": "ls7i9xh", "type": "comment", "parent_id": "t1_ls5vkml", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls7i9xh/", "author": "Sp4rkiop", "created_utc": 1729090023, "score": 1, "content": "Amazing"}
{"id": "ls8o653", "type": "comment", "parent_id": "t1_ls8n7mu", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8o653/", "author": "Throwawayforgainz99", "created_utc": 1729103313, "score": 1, "content": "Yeah I guess I\u2019m just surprised it\u2019s so easy"}
{"id": "ls8n62h", "type": "comment", "parent_id": "t1_ls8168k", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8n62h/", "author": "Throwawayforgainz99", "created_utc": 1729102994, "score": 2, "content": "How do you analyze it if it is obfuscated?"}
{"id": "ls8lvke", "type": "comment", "parent_id": "t1_ls8jn03", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8lvke/", "author": "M0le5ter", "created_utc": 1729102583, "score": 1, "content": "For what? like I also manually opened a browser having its traffic proxied through my proxy, and then set the cf clearance cookie, but it didn't worked i m not using any library here"}
{"id": "lvjgd1x", "type": "comment", "parent_id": "t1_lvj90xg", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvjgd1x/", "author": "None", "created_utc": 1730822439, "score": 1, "content": "Gotcha. Yeah, thats my use case. Was going to reply back that it seemed ineffective to the one site I wanted to use this one. I'd prefer to make all of my requests using than have to control a chromedriver. Couple of suggestions though. 1) Add a requirements.txt. You'll need to add: PyExecJS==1.5.1 The response when you instantiate it should be stored, as you might want to parse it. So cf.response.json(), etc. Since the cookies would be held within client in the object, I would add notes on how to make follow up requests. Otherwise you'd have to detail out all of the header info a person would need to instantiate their own And it would be silly since turnstile sites can reprompt sporadically. `response = cf.client.get(url=url, timeout=10)` `response = cf.client.post(url=url, data=data, json=json, timeout=10)` I would allow for usage of a full URL. The site I'm trying to tackle doesnt prompt for a cf turnstile until you go further in to the site. So you could use a url.split('/') to grab the base url to use within your self.clientRequest method. Just some ideas."}
{"id": "ls5qqi0", "type": "comment", "parent_id": "t1_ls5qoqi", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5qqi0/", "author": "Dapper-Profession552", "created_utc": 1729056639, "score": 10, "content": ""}
{"id": "m2at43i", "type": "comment", "parent_id": "t1_ls5qoqi", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/m2at43i/", "author": "Kyleweb3", "created_utc": 1734335693, "score": 3, "content": "thank you for your replying, thats very helpful, and inspire me how to solve it myself."}
{"id": "ls4vebj", "type": "comment", "parent_id": "t1_ls4tss1", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls4vebj/", "author": "Dapper-Profession552", "created_utc": 1729042365, "score": 9, "content": "The codes in the JS files are made by cloudflare and are generators that I exported for CF Bypass. Then it looks like unreadable Without that i would not be able to extract the cf_clearance cookie."}
{"id": "ls5j666", "type": "comment", "parent_id": "t1_ls5hnr4", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls5j666/", "author": "Dapper-Profession552", "created_utc": 1729052509, "score": 3, "content": "Thanks, I already implemented proxy support, So: ``` cf = CF_Solver( ' proxy='255.255.255.255' ) ```"}
{"id": "ls8oefz", "type": "comment", "parent_id": "t1_ls8o653", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8oefz/", "author": "joeyx22lm", "created_utc": 1729103387, "score": 1, "content": "There are some extra hoops to jump thru, also there is some level of minification of the JS so it can be harder to make it 100% perfect with just regex."}
{"id": "ls8o09i", "type": "comment", "parent_id": "t1_ls8n62h", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8o09i/", "author": "Dapper-Profession552", "created_utc": 1729103263, "score": 1, "content": "There are some parts of the Cloudflare code that are understandable, for example this one"}
{"id": "ls8nfj4", "type": "comment", "parent_id": "t1_ls8lvke", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8nfj4/", "author": "Dapper-Profession552", "created_utc": 1729103078, "score": 1, "content": "I see that when I enter the site it asks me to solve the captcha only once. You used puppeteer to solve the captcha, but did you see if it returned a cookie after solving it? I saw that it returned the _cfruid cookie to me, when I resolved it"}
{"id": "lu9x9uq", "type": "comment", "parent_id": "t1_ls8lvke", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lu9x9uq/", "author": "SpiritingGiant", "created_utc": 1730163843, "score": 1, "content": "Depending on the site, cloudflare can check against your TLS Fingerprint, if thats the case, you need to use a client that intercepts the original request, spoofs it with an existing fingerprint that may or may not be blocked by cloudflare, then sends it. \"tls\\_client\" and \"curl\\_cffi\" does this."}
{"id": "lvjrl2a", "type": "comment", "parent_id": "t1_lvjgd1x", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvjrl2a/", "author": "Dapper-Profession552", "created_utc": 1730825733, "score": 1, "content": "Thanks for the suggestions, I will try to improve this"}
{"id": "lvjtjyb", "type": "comment", "parent_id": "t1_lvjgd1x", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lvjtjyb/", "author": "Dapper-Profession552", "created_utc": 1730826307, "score": 1, "content": "Could you send me the URL of the website you are working with? Cloudflare typically has no static code and each website implemented with turnstile has different code. And I need to collect protected websites, for my turnstile bypass project"}
{"id": "lt79u05", "type": "comment", "parent_id": "t1_ls5qqi0", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lt79u05/", "author": "Wise_Environment_185", "created_utc": 1729617227, "score": 2, "content": "well - Dapper-Proffession552: i like your approach. Is this doable - i mean can we put these things togehter!?"}
{"id": "ls7h9ed", "type": "comment", "parent_id": "t1_ls4vebj", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls7h9ed/", "author": "GillesQuenot", "created_utc": 1729089690, "score": 3, "content": "So why not just use the JS code on the website? What is the need to store the code on your Github if you copy it from Cloudfare?"}
{"id": "lyqepe6", "type": "comment", "parent_id": "t1_ls5j666", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lyqepe6/", "author": "Noctuuu", "created_utc": 1732452921, "score": 1, "content": "I think I'm in love with you"}
{"id": "ls8o3yq", "type": "comment", "parent_id": "t1_ls8o09i", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8o3yq/", "author": "Throwawayforgainz99", "created_utc": 1729103295, "score": 1, "content": "What does that mean lol"}
{"id": "ls8obpk", "type": "comment", "parent_id": "t1_ls8o09i", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8obpk/", "author": "Throwawayforgainz99", "created_utc": 1729103363, "score": 1, "content": "So was the whole function not obfuscated?"}
{"id": "lx86f0a", "type": "comment", "parent_id": "t1_ls8o09i", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lx86f0a/", "author": "friday305", "created_utc": 1731654405, "score": 1, "content": "What does the \"wp\" value consist of?"}
{"id": "lx7w0d8", "type": "comment", "parent_id": "t1_lvjtjyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lx7w0d8/", "author": "Djkid4lyfe", "created_utc": 1731649097, "score": 1, "content": "Gmgn.ai endpoints"}
{"id": "lxehmbv", "type": "comment", "parent_id": "t1_lvjtjyb", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lxehmbv/", "author": "jamesshamala", "created_utc": 1731743192, "score": 1, "content": "["}
{"id": "lt89b94", "type": "comment", "parent_id": "t1_lt79u05", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lt89b94/", "author": "Dapper-Profession552", "created_utc": 1729628179, "score": 1, "content": "Yes"}
{"id": "ls7kepp", "type": "comment", "parent_id": "t1_ls7h9ed", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls7kepp/", "author": "Dapper-Profession552", "created_utc": 1729090716, "score": 6, "content": "What I'm doing is reverse engineering, using cloudflare generators to get a bot-protected thing I just investigated which generators create \"wb\" and \"s\" and then i use python to send an HTTP request to get cf\\_clearance"}
{"id": "lsngg9b", "type": "comment", "parent_id": "t1_ls7h9ed", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsngg9b/", "author": "donde_waldo", "created_utc": 1729321074, "score": 1, "content": "He simply took the functions from cloudflares js files, which are obfuscated/minified. Why reverse it entirely if you don't need to.. likely not gonna be the same function for long anyway."}
{"id": "lyqfzii", "type": "comment", "parent_id": "t1_lyqepe6", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lyqfzii/", "author": "Noctuuu", "created_utc": 1732453528, "score": 1, "content": "Not working for me, I still get 403 with the given cf\\_clearance :( >>> from aqua import CF_Solver ... cf = CF_Solver(' ... cookie = cf.cookie() ... print(cookie) ... response = cf.client.get(url=\" timeout=10) >>> response <Response [403 Forbidden]> >>> response.text '<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title>"}
{"id": "ls8okfs", "type": "comment", "parent_id": "t1_ls8o3yq", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8okfs/", "author": "Dapper-Profession552", "created_utc": 1729103439, "score": 1, "content": "That is the function that generates the cf\\_clearance cookie xd"}
{"id": "ls8oshx", "type": "comment", "parent_id": "t1_ls8obpk", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8oshx/", "author": "Dapper-Profession552", "created_utc": 1729103511, "score": 1, "content": "This is a little obfuscated"}
{"id": "lxbio2o", "type": "comment", "parent_id": "t1_lx86f0a", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lxbio2o/", "author": "Dapper-Profession552", "created_utc": 1731701053, "score": 1, "content": "`wp` is a token of website and browser information, this will determine if you are a robot or a human and then create a unique cf_clearance"}
{"id": "lsc53hi", "type": "comment", "parent_id": "t1_ls7kepp", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lsc53hi/", "author": "WishIWasOnACatamaran", "created_utc": 1729156285, "score": 3, "content": "You\u2019re not wrong but that doesn\u2019t answer /u/gillesquenot\u2019s question"}
{"id": "lyrexrc", "type": "comment", "parent_id": "t1_lyqfzii", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lyrexrc/", "author": "Dapper-Profession552", "created_utc": 1732466290, "score": 1, "content": "Try use curl_cffi ``` from aqua import CF_Solver from curl_cffi import requests # Rest of the cf code~ cf_clearance = cf.cookie() session = requests.Session(impersonate='chrome124') session.cookies['cf_clearance'] = cf_clearance resp = session.get('url') ```"}
{"id": "ls8ost1", "type": "comment", "parent_id": "t1_ls8okfs", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8ost1/", "author": "Throwawayforgainz99", "created_utc": 1729103513, "score": 1, "content": "It\u2019s just in plain text? It\u2019s that easy?"}
{"id": "ls8p3rw", "type": "comment", "parent_id": "t1_ls8oshx", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8p3rw/", "author": "Throwawayforgainz99", "created_utc": 1729103609, "score": 1, "content": "Why don\u2019t they do the whole thing?"}
{"id": "lyrm5lo", "type": "comment", "parent_id": "t1_lyrexrc", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lyrm5lo/", "author": "Noctuuu", "created_utc": 1732468563, "score": 1, "content": "Am I doing this wrong ? I saw in the github repo issues that this works with websites that don't have turnstile, I guess this DO have turnstile because I remember not having to deal with captchas in the beginning of my project. >>> from aqua import CF_Solver ... from curl_cffi import requests ... cf = CF_Solver(' ... cf_clearance = cf.cookie() ... response = cf.client.get(url=\" timeout=10) ... session = requests.Session(impersonate='chrome124') ... session.cookies['cf_clearance'] = cf_clearance ... resp = session.get(' ... >>> resp <Response [403]> >>> resp.text '<!DOCTYPE html><html lang=\"en-US\"><head><title>Just a moment...</title>"}
{"id": "ls8p3lh", "type": "comment", "parent_id": "t1_ls8ost1", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8p3lh/", "author": "Dapper-Profession552", "created_utc": 1729103607, "score": 1, "content": "Yes, I don't know why everyone asks me how I did it if it's simple"}
{"id": "ls8paq4", "type": "comment", "parent_id": "t1_ls8p3rw", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8paq4/", "author": "Dapper-Profession552", "created_utc": 1729103670, "score": 1, "content": "i dont know, I saw someone who was looking for a bypass like that, and I just did"}
{"id": "lyrporc", "type": "comment", "parent_id": "t1_lyrm5lo", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lyrporc/", "author": "Dapper-Profession552", "created_utc": 1732469673, "score": 1, "content": "Okay, try assigning headers to the `session` instance, Cloudflare probably detected you as a bot because you don't have headers in the request."}
{"id": "lu2c7mr", "type": "comment", "parent_id": "t1_ls8p3lh", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lu2c7mr/", "author": "Apprehensive_Leg6986", "created_utc": 1730060056, "score": 2, "content": "the point is we want to know how you do it, not just some flex word from you mate!"}
{"id": "ls8q1an", "type": "comment", "parent_id": "t1_ls8paq4", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8q1an/", "author": "Throwawayforgainz99", "created_utc": 1729103900, "score": 1, "content": "Can you explain more where to learn this level of scraping ? I\u2019m pretty good with just getting the api from the inspect window and using the cookies, but I\u2019ve never used the \u201csource\u201d tab before"}
{"id": "lyrxi2c", "type": "comment", "parent_id": "t1_lyrporc", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lyrxi2c/", "author": "Noctuuu", "created_utc": 1732472186, "score": 1, "content": "Omg it worked this is insane tysm!!!! Last thing \\^\\^ I struggle with proxies, could you show me the syntax to add http proxies ?"}
{"id": "lu2e8mb", "type": "comment", "parent_id": "t1_lu2c7mr", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/lu2e8mb/", "author": "Dapper-Profession552", "created_utc": 1730060649, "score": 1, "content": "This is `Website Reverse Engineering`, If you search on YouTube you will find videos on how to reverse tokens, cookies and others, from websites or something related"}
{"id": "ls8rkf4", "type": "comment", "parent_id": "t1_ls8q1an", "permalink": "https://www.reddit.com/r/webscraping/comments/1g40qy2/i_made_a_cloudflarebypass/ls8rkf4/", "author": "Dapper-Profession552", "created_utc": 1729104376, "score": 2, "content": "Well, when you want to find an API and you don't see it in the \"Network\" tab You will need to go to the \"Source\" tab and parse the website code and then use the Console to intercept elements of the code, such as APIs, tokens, cookies, etc. The most fundamental thing is to learn how to use Devtools (advanced) and reverse engineering (optional)"}
{"id": "1iosd4a", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1iosd4a/when_you_rebrand_your_web_scrapers_to_ai_agents/", "author": "madredditscientist", "created_utc": 1739477651, "score": 87, "title": "When you rebrand your web scrapers to AI agents", "content": ""}
{"id": "mcpk8rs", "type": "comment", "parent_id": "t3_1iosd4a", "permalink": "https://www.reddit.com/r/webscraping/comments/1iosd4a/when_you_rebrand_your_web_scrapers_to_ai_agents/mcpk8rs/", "author": "Due-Afternoon-5100", "created_utc": 1739528687, "score": 3, "content": "\"sprinkle\" a lil AI"}
{"id": "mcvd74d", "type": "comment", "parent_id": "t3_1iosd4a", "permalink": "https://www.reddit.com/r/webscraping/comments/1iosd4a/when_you_rebrand_your_web_scrapers_to_ai_agents/mcvd74d/", "author": "kobaasama", "created_utc": 1739606030, "score": 2, "content": "So Ai agents were scrapers all along?"}
{"id": "mdnkeyk", "type": "comment", "parent_id": "t3_1iosd4a", "permalink": "https://www.reddit.com/r/webscraping/comments/1iosd4a/when_you_rebrand_your_web_scrapers_to_ai_agents/mdnkeyk/", "author": "None", "created_utc": 1739988431, "score": 1, "content": "[removed]"}
{"id": "mdnr6kk", "type": "comment", "parent_id": "t1_mdnkeyk", "permalink": "https://www.reddit.com/r/webscraping/comments/1iosd4a/when_you_rebrand_your_web_scrapers_to_ai_agents/mdnr6kk/", "author": "webscraping-ModTeam", "created_utc": 1739990243, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "evo1uh", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/", "author": "Gill_Chloet", "created_utc": 1580310407, "score": 89, "title": "Congrats! Web scraping is legal! (US precedent)", "content": "Disputes about whether web scraping is legal have been going on for a long time. And now, a couple of months ago, the scandalous case of web scraping between hiQ v. LinkedIn was completed. You can read about the progress of the case here: [ Finally, the court concludes: \"Giving companies like LinkedIn the freedom to decide who can collect and use data \u2013 data that companies do not own, that is publicly available to everyone, and that these companies themselves collect and use \u2013 creates a risk of information monopolies that will violate the public interest\u201d."}
{"id": "ffwu022", "type": "comment", "parent_id": "t3_evo1uh", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/ffwu022/", "author": "slicknick654", "created_utc": 1580310965, "score": 8, "content": "Read the article, any idea when anti automation will be taken down entirely? Or will small companies have to contact sites we need to scrape to get whitelisted?"}
{"id": "ffyagpz", "type": "comment", "parent_id": "t3_evo1uh", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/ffyagpz/", "author": "albaniax", "created_utc": 1580341792, "score": 5, "content": "Nice! I hope the EU will follow."}
{"id": "jfocmx6", "type": "comment", "parent_id": "t3_evo1uh", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/jfocmx6/", "author": "greatgolem66", "created_utc": 1681118533, "score": 3, "content": "Updated in 2023 - web scraping is truly legal. We wrote about the extensive development of the [court case]( from 2017 - 2022"}
{"id": "fgqwyj4", "type": "comment", "parent_id": "t1_ffwu022", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/fgqwyj4/", "author": "sandalguy89", "created_utc": 1581033238, "score": 2, "content": "Can you clarify for a newb"}
{"id": "fgqytcx", "type": "comment", "parent_id": "t1_fgqwyj4", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/fgqytcx/", "author": "slicknick654", "created_utc": 1581034401, "score": 1, "content": "LinkedIn doesn\u2019t allow bots to scrape. I was asking when they would disable those controls."}
{"id": "fgr97og", "type": "comment", "parent_id": "t1_fgqytcx", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/fgr97og/", "author": "sandalguy89", "created_utc": 1581041226, "score": 1, "content": "I mean about the small companies."}
{"id": "fgrprjj", "type": "comment", "parent_id": "t1_fgr97og", "permalink": "https://www.reddit.com/r/webscraping/comments/evo1uh/congrats_web_scraping_is_legal_us_precedent/fgrprjj/", "author": "slicknick654", "created_utc": 1581053499, "score": 1, "content": "? Will people and companies have to contact LinkedIn to get permission to scrape. I\u2019ve tried scraping before and I was blocked so was curious how the unblock process works"}
{"id": "1l54iee", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/", "author": "dracariz", "created_utc": 1749246618, "score": 91, "title": "Camoufox (Playwright) automatic captcha solving (Cloudflare)", "content": "Built a Python library that extends [camoufox]( (playwright-based anti-detect browser) to automatically solve captchas (currently only Cloudflare: interstitial pages and turnstile widgets). Camoufox makes it possible to bypass closed Shadow DOM with strict CORS, which allows clicking Cloudflare\u2019s checkbox. More technical details on GitHub. Even with a dirty IP, challenges are solved automatically via clicks thanks to Camoufox's anti-detection. Planning to add support for services like 2Captcha and other captcha types (hCaptcha, reCAPTCHA), plus alternative bypass methods where possible (like with Cloudflare now). Github: [ PyPI: ["}
{"id": "mwg6y74", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwg6y74/", "author": "None", "created_utc": 1749277299, "score": 5, "content": "[deleted]"}
{"id": "mwgenab", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwgenab/", "author": "divedave", "created_utc": 1749281836, "score": 1, "content": "Looks great! I'll check it."}
{"id": "mwie77k", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwie77k/", "author": "A4_Ts", "created_utc": 1749313148, "score": 1, "content": "Awesome job, if Camoufox can already pass Cloudflare what\u2019s the benefit of this library? Is it to pass turnstile even with bad proxies? Also its a nice py port"}
{"id": "mwq7uch", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwq7uch/", "author": "None", "created_utc": 1749419445, "score": 1, "content": "[removed]"}
{"id": "mx5hs8h", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mx5hs8h/", "author": "sirf_trivedi", "created_utc": 1749620029, "score": 1, "content": "Hey Thanks for the hard work! This is not working on apnews.com for some reason. Any pointers? Thanks!"}
{"id": "mxccxd6", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxccxd6/", "author": "ElegantPercentage", "created_utc": 1749713296, "score": 1, "content": "How to properly generate and extract browser fingerprints for Camoufox configuration?"}
{"id": "mxhfisx", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxhfisx/", "author": "fluffyboogasuga", "created_utc": 1749776534, "score": 1, "content": "Great tool and realistic mouse movements but after I solve the turnstile it goes to the am I a human where you have to long press and and hold. I\u2019m not sure what I am doing wrong lol"}
{"id": "mxq38r7", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxq38r7/", "author": "tyasar", "created_utc": 1749900466, "score": 1, "content": "What about turnstile?"}
{"id": "n2555sc", "type": "comment", "parent_id": "t3_1l54iee", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/n2555sc/", "author": "Economy-Occasion-489", "created_utc": 1752051604, "score": 1, "content": "is there any for selenium please let me know if you find any for selenium i am doing a personal project it would help a lot"}
{"id": "mwhgrlp", "type": "comment", "parent_id": "t1_mwg6y74", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwhgrlp/", "author": "dracariz", "created_utc": 1749302016, "score": 2, "content": "Thank you. Basically camoufox has a patch to bypass shadow root, I use it here: ["}
{"id": "mwhhg9i", "type": "comment", "parent_id": "t1_mwgenab", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwhhg9i/", "author": "dracariz", "created_utc": 1749302277, "score": 1, "content": "Thank you. I'd love to hear your feedback then."}
{"id": "mwis5xe", "type": "comment", "parent_id": "t1_mwie77k", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwis5xe/", "author": "dracariz", "created_utc": 1749317541, "score": 2, "content": "Thank you! \\> if Camoufox can already pass Cloudflare what\u2019s the benefit of this library Well, there are similar captcha-bypass libs out there for Selenium and other tools, but nothing tailored for Camoufox. I thought that was a missed opportunity, since Camoufox has so much potential. This lib fills that gap - it lets you bypass captchas in your own project with just a single line. And it's not just about Cloudflare either. That\u2019s just the starting point, more captchas and services coming soon. You can check the code, it's built to reliably solve captchas, even under tough conditions. Fully tested, too. And yeah, actually I made this after seeing tons of people struggling with closed shadow DOMs, nested iframes, and all that stuff. The more I looked into it, the more I realized most of them were just trying to solve Cloudflare captchas by clicking the checkbox - and that alone wouldn't really work if not camoufox, especially if you\u2019ve got a bad IP. But camoufox's anti-detection features make a big difference. So this lib builds on that - lets you reliably solve captchas without caring about all that stuff. No need for 2captcha or CapMonster for Cloudflare."}
{"id": "mwqemd1", "type": "comment", "parent_id": "t1_mwq7uch", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwqemd1/", "author": "webscraping-ModTeam", "created_utc": 1749421738, "score": 1, "content": "Please review the sub rules"}
{"id": "mx6v174", "type": "comment", "parent_id": "t1_mx5hs8h", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mx6v174/", "author": "dracariz", "created_utc": 1749645640, "score": 1, "content": "Hey, could you share your code? [ this works for me"}
{"id": "mxcxvlc", "type": "comment", "parent_id": "t1_mxccxd6", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxcxvlc/", "author": "dracariz", "created_utc": 1749725460, "score": 1, "content": "They are already configured. Check the docs if u need custom"}
{"id": "mxhfrk7", "type": "comment", "parent_id": "t1_mxhfisx", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxhfrk7/", "author": "dracariz", "created_utc": 1749776616, "score": 1, "content": "Could you share the website url and a screenshot of that press&hold captcha?"}
{"id": "n26135x", "type": "comment", "parent_id": "t1_n2555sc", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/n26135x/", "author": "dracariz", "created_utc": 1752065611, "score": 1, "content": "Why do you use selenium?"}
{"id": "mwicq1t", "type": "comment", "parent_id": "t1_mwhgrlp", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwicq1t/", "author": "Small-Relation3747", "created_utc": 1749312676, "score": 2, "content": "What do you mean bypass? Shadow DOM is just a feature"}
{"id": "mwtl7vb", "type": "comment", "parent_id": "t1_mwis5xe", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mwtl7vb/", "author": "A4_Ts", "created_utc": 1749472917, "score": 2, "content": "I haven\u2019t tried Camoufox but you\u2019re saying out of the box Camoufox as is without your library can\u2019t pass cloudflare?"}
{"id": "mxbq6zk", "type": "comment", "parent_id": "t1_mx6v174", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxbq6zk/", "author": "sirf_trivedi", "created_utc": 1749701385, "score": 1, "content": "I'm using this scrapy-camoufox library: Example: The exact code in the examples but in parser I use your library to solve the captcha by providing the page object. The library actually clicks the box to verify but it fails to detect the iframe after 3 tries. Any help would be appreciated. Thanks."}
{"id": "n6tups3", "type": "comment", "parent_id": "t1_mwtl7vb", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/n6tups3/", "author": "Harry_Hindsight", "created_utc": 1754290039, "score": 1, "content": "I use playwright+camoufox. It does not evade cloudflare challenges like the one shown in OPs video. I look forward to trying his library today."}
{"id": "mxbqgya", "type": "comment", "parent_id": "t1_mxbq6zk", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxbqgya/", "author": "dracariz", "created_utc": 1749701508, "score": 1, "content": "I believe you didnt add this to the AsyncCamoufox instance: i\\_know\\_what\\_im\\_doing=True,config={'forceScopeAccess': True}, # add this when creating Camoufox instance disable\\_coop=True # add this when creating Camoufox instance"}
{"id": "mxbrj74", "type": "comment", "parent_id": "t1_mxbqgya", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxbrj74/", "author": "sirf_trivedi", "created_utc": 1749701993, "score": 2, "content": "Maybe I am putting the config at the wrong place. Will verify. Thanks"}
{"id": "mxbqku6", "type": "comment", "parent_id": "t1_mxbqgya", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxbqku6/", "author": "sirf_trivedi", "created_utc": 1749701556, "score": 1, "content": "Is i know what I am doing crucial? I thought it was to supress the warnings"}
{"id": "mxbs3hz", "type": "comment", "parent_id": "t1_mxbqku6", "permalink": "https://www.reddit.com/r/webscraping/comments/1l54iee/camoufox_playwright_automatic_captcha_solving/mxbs3hz/", "author": "dracariz", "created_utc": 1749702253, "score": 1, "content": "It is just to remove the warnings"}
{"id": "1hfmul8", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/", "author": "0xReaper", "created_utc": 1734366234, "score": 87, "title": "Big update to Scrapling library!", "content": "Scrapling is Undetectable, Lightning-Fast, and Adaptive Web Scraping Python library Version 0.2.9 has been released now with a lot of new features like async support with better performance and stealth! The last time I talked about Scrapling here was in 0.2 and a lot of updates have been done since then. Check it out and tell me what you think. ["}
{"id": "m2d47rp", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2d47rp/", "author": "Redhawk1230", "created_utc": 1734372727, "score": 6, "content": "Hey I\u2019ve been following the project since I last saw it here when you posted last time (0.2). I liked the auto_match functionality however at that time I believe the documentation was pretty weak. I see it\u2019s improved and adding an Async worker is definitely appreciated. However looking at the code I see it\u2019s essentially a convient layer on top of async client (and the changes to StaticEngine which is responsible for the real asynchronous operations) I still have to manually handle concurrency/task pools (not the worst I just use asyncio_pool and I understand not wanting to add complexity or opinionated code). I would maybe enjoy being able to pass user defined functions to handle delays, concurrency controls and concurrent tasks (trying to avoid making AsyncFetcher a stateful class). Anyway I enjoy the project a lot and enjoy the smart scraping / content based selection. Good work!"}
{"id": "m2d2sck", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2d2sck/", "author": "ghad0265", "created_utc": 1734372283, "score": 2, "content": "How is this comparable to playwright? In terms of speed and performance."}
{"id": "m2x34yu", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2x34yu/", "author": "Djkid4lyfe", "created_utc": 1734659379, "score": 2, "content": "Can this scrape cloudflare protected sites?"}
{"id": "m2e2082", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2e2082/", "author": "eenak", "created_utc": 1734383224, "score": 1, "content": "Very cool. Been looking for something like this. Gonna try it later"}
{"id": "m2fsi95", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2fsi95/", "author": "Over_Discussion3639", "created_utc": 1734405140, "score": 1, "content": "Do you sell it as saas tool?"}
{"id": "m2gbhku", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2gbhku/", "author": "mcpoyles", "created_utc": 1734413354, "score": 1, "content": "Is there a way to render a pages JavaScript to capture content, buttons, or other elements loaded via client side rendering?"}
{"id": "m2vzfvs", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2vzfvs/", "author": "dclets", "created_utc": 1734644963, "score": 1, "content": "Is it free and open source?"}
{"id": "m2x0cbk", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2x0cbk/", "author": "adilanchian", "created_utc": 1734658325, "score": 1, "content": "im fresh in this world, so cool to see ppl excited about this project! i\u2019ve read a ton about how proxies help with being \u201cundetectable\u201d. is this project meant to be used along side a proxy or it actually isn\u2019t necessary? nice work homie :)."}
{"id": "m2ygxfm", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2ygxfm/", "author": "None", "created_utc": 1734684548, "score": 1, "content": "[removed]"}
{"id": "m3p90ge", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m3p90ge/", "author": "None", "created_utc": 1735105795, "score": 1, "content": "[deleted]"}
{"id": "mbypwvc", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/mbypwvc/", "author": "Vegetable_Entrance_4", "created_utc": 1739161089, "score": 1, "content": "I\u2019m facing issues, if I send concurrent requests, it works for a bit and throws ERR 24, Too Many Open Files. I suspect there\u2019s FD leak somewhere. System is tuned to handle 100k open files. Once this is fixed, this is going to be beast."}
{"id": "mk0s2wa", "type": "comment", "parent_id": "t3_1hfmul8", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/mk0s2wa/", "author": "Due-Mechanic-7225", "created_utc": 1743087476, "score": 1, "content": "Hello! I am really interested about your scrapling library, but I am not so expert, so is there some tutorials to start to learn it? thanks in advance"}
{"id": "m2da6d5", "type": "comment", "parent_id": "t1_m2d47rp", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2da6d5/", "author": "None", "created_utc": 1734374580, "score": 0, "content": "[deleted]"}
{"id": "m2dbly0", "type": "comment", "parent_id": "t1_m2d2sck", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2dbly0/", "author": "0xReaper", "created_utc": 1734375029, "score": 2, "content": "Hey mate, there are three main classes here when it comes to fetching websites called Fetchers. One of them is called PlayWrightFetcher, which uses the playwright library directly if you prefer to use Playwright, but the library here makes it easy and adds more options, it's all explained in the table under the PlayWrightFetcher class in the README page here: But if you are talking about the `StealthyFetcher`, then it uses PlayWright API to control a custom browser to bypass protections. This one is different from device to device, but on mine, it's faster than Playwright. I didn't actually compare both fetchers in terms of speed, but both are fast and provide a lot of options. If you can test them on your device, I would love to hear your feedback :D"}
{"id": "m2yigdt", "type": "comment", "parent_id": "t1_m2x34yu", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2yigdt/", "author": "0xReaper", "created_utc": 1734685640, "score": 1, "content": "Yes of course, just use a selector from the website with the argument \u2018wait_selector\u2019 so Scrapling wait for the website after Cloudflare wait page"}
{"id": "m2e2f76", "type": "comment", "parent_id": "t1_m2e2082", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2e2f76/", "author": "0xReaper", "created_utc": 1734383354, "score": 1, "content": "Thanks! I would love to hear your feedback :)"}
{"id": "m2g1oek", "type": "comment", "parent_id": "t1_m2fsi95", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2g1oek/", "author": "0xReaper", "created_utc": 1734408798, "score": 4, "content": "No it\u2019s open source to be used by every one but if you mean you want to use it commercially then you can do it but check out the sponsor button if you are making money from it :)"}
{"id": "m2h3tnp", "type": "comment", "parent_id": "t1_m2gbhku", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2h3tnp/", "author": "0xReaper", "created_utc": 1734431111, "score": 2, "content": "Yes by default both browser fetchers (`PlayWrightFetcher`/`StealthyFetcher`) wait for states 'load' and 'domcontentloaded' to be fulfilled so basically they wait for all javascript to load and execute. The 'network_idle` argument waits till the 'networkidle' state which means waits until there are no network connections for at least 500 ms. If all of that is not enough and for some websites, it is, as a last resort you can use the `wait_selector` which you give a css selector and the Fetcher will wait till the selector appears on the page so for example for a website that uses Cloudflare or similar protection with a 'wait page' you must use a selector from the website itself so the Fetcher will wait till that 'wait page' disappear."}
{"id": "m2wvrrq", "type": "comment", "parent_id": "t1_m2vzfvs", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2wvrrq/", "author": "0xReaper", "created_utc": 1734656586, "score": 2, "content": "Yea of course! The URL is in the post!"}
{"id": "m2yibhk", "type": "comment", "parent_id": "t1_m2x0cbk", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2yibhk/", "author": "0xReaper", "created_utc": 1734685543, "score": 2, "content": "Thanks mate, most of the time it\u2019s not needed but for some stubborn stuff it will. Some websites show captcha even if it doesn\u2019t think that the user is bot etc\u2026"}
{"id": "m2z3x76", "type": "comment", "parent_id": "t1_m2ygxfm", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2z3x76/", "author": "webscraping-ModTeam", "created_utc": 1734699058, "score": 1, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "m3psz2q", "type": "comment", "parent_id": "t1_m3p90ge", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m3psz2q/", "author": "0xReaper", "created_utc": 1735119651, "score": 1, "content": "Yes it works, we don\u2019t use any selenium variant here"}
{"id": "m2dgory", "type": "comment", "parent_id": "t1_m2da6d5", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2dgory/", "author": "None", "created_utc": 1734376591, "score": 1, "content": "[deleted]"}
{"id": "m2fz1zb", "type": "comment", "parent_id": "t1_m2dbly0", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2fz1zb/", "author": "Queasy_Structure1922", "created_utc": 1734407719, "score": 2, "content": "Are you also managing tls handshakes and ja3 fingerprints to circumvent fingerprinting?"}
{"id": "m2i98rb", "type": "comment", "parent_id": "t1_m2dbly0", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2i98rb/", "author": "maxpayne14659", "created_utc": 1734449783, "score": 1, "content": "Can I scrape subreddit with this?"}
{"id": "m38pz81", "type": "comment", "parent_id": "t1_m2dbly0", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m38pz81/", "author": "None", "created_utc": 1734842309, "score": 1, "content": "[removed]"}
{"id": "m2j2rjg", "type": "comment", "parent_id": "t1_m2e2f76", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2j2rjg/", "author": "eenak", "created_utc": 1734459219, "score": 2, "content": "Okay I have a couple questions, and you might already have this info in the docs, but I can't find it. From my understanding, the return type of methods like 'StealthyFetcher().get(<url>)' is an Adaptor. When I use the .find() method on an Adaptor, it also returns an Adaptor (given the content I am finding exists). In order to integrate this project into my current codebase for scraping, I am looking to use your parsing methods (like find, find\\_all etc), but then once I find what I am looking for, and I go to extract the actual text element of a div I have found (without the tags just the text content), I need to be able to get it simply as a string object and not a TextHandler (I understand TextHandler is a subclass of string, but I just need it to be plain str). '.text' on an Adaptor appears to be of type TextHandler, but I can't find any method for TextHandler to just get the content as a string (python builtins like str() don't seem to do the trick either). How can I just get the content? I guess I could just get the raw content from the Adaptor class after fetching, but I want the performance benefits of the scrapling parsing. Besides that, its super good at being stealthy, and thats exactly what I was looking for, so thanks"}
{"id": "m2iay0w", "type": "comment", "parent_id": "t1_m2h3tnp", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2iay0w/", "author": "mcpoyles", "created_utc": 1734450346, "score": 2, "content": "Thank you that is amazing! My current scraping solution always seems to miss YoutTube embeds. Being able to wait for selector is huge, thank you!"}
{"id": "m32tzbj", "type": "comment", "parent_id": "t1_m2yibhk", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m32tzbj/", "author": "adilanchian", "created_utc": 1734748097, "score": 1, "content": "thanks for the response homie :). ya i had one vercel serverless function get block by the site i was scraping (without proxy) so assuming im probably gonna need a proxy haha. tysm!"}
{"id": "m2dk5hv", "type": "comment", "parent_id": "t1_m2dgory", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2dk5hv/", "author": "0xReaper", "created_utc": 1734377654, "score": 3, "content": "Sorry mate I had a bad day which must have caused me to misread your comment this badly! For the Fetcher class which is built on top of the StaticEngine class which is built on top of there are not a lot of tricks to add here as already handles most stuff, unlike the other two Fetchers which has big space to play with and passed some of this Freedom to the user with the `page_action` parameter. In 0.3 I already planned to try to add context managers to handle sessions to all Fetchers so the same Client/session/browser can be used for more than one request. Regarding the smart scraping/content-based selection, have you tried the `find`/`find_all` methods as well? It gives more freedom. And sorry again for misinterpreting your comment I'm really under pressure but it was really bad of me to let it out, I'm already ashamed of my comment and deleted it. I would love to see your full review any time!"}
{"id": "m2fzwqq", "type": "comment", "parent_id": "t1_m2fz1zb", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2fzwqq/", "author": "Queasy_Structure1922", "created_utc": 1734408063, "score": 3, "content": "Just saw the underliying library camoufox seems to deal with that, man I was looking for something like this forever! The only way to do this was writing a custom browser because all the tls stuff is written in c, man thanks for posting this!!! Will give it a try"}
{"id": "m2g0uwt", "type": "comment", "parent_id": "t1_m2fz1zb", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2g0uwt/", "author": "0xReaper", "created_utc": 1734408455, "score": 2, "content": "No mate, the only to do that with normal requests is by using something like curl_impersonate instead of which I already considered but then decided to not use it as it\u2019s compiled so it might cause issues with some devices installation which will hurt Scrapling. Instead you can use browser requests with one of the two Fetchers (StealthyFetcher, PlayWrightFetcher) requests are done through real browsers here so you don\u2019t need to fake anything"}
{"id": "m2idxm9", "type": "comment", "parent_id": "t1_m2i98rb", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2idxm9/", "author": "0xReaper", "created_utc": 1734451316, "score": 1, "content": "Nothing can prevent you from doing that with Scrapling other than your web scraping skills!"}
{"id": "m3ck2vj", "type": "comment", "parent_id": "t1_m38pz81", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m3ck2vj/", "author": "0xReaper", "created_utc": 1734905636, "score": 2, "content": "Class `StealthyFetcher` does that by default in a lot of parts and mostly without configuration"}
{"id": "m2j4g70", "type": "comment", "parent_id": "t1_m2j2rjg", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2j4g70/", "author": "0xReaper", "created_utc": 1734459749, "score": 2, "content": "Hey mate, `TextHandler` is `str` but with added methods, so I don't understand why you would want to do that, but if you insist, then the `str` function is enough to convert it to plain `str` I have just tested it again: ```python >>> from scrapling import TextHandler >>> type(str(TextHandler('string'))) is str True ``` The only usage I found while making the project for converting `TextHandler` to `str` again was while I was using `orjson` because it read the instances of the input, so I was using the `str` function to convert the data as well."}
{"id": "m2idm14", "type": "comment", "parent_id": "t1_m2iay0w", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2idm14/", "author": "0xReaper", "created_utc": 1734451211, "score": 1, "content": "Thanks mate, glad you like it \\^\\_\\^"}
{"id": "m2do951", "type": "comment", "parent_id": "t1_m2dk5hv", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2do951/", "author": "Redhawk1230", "created_utc": 1734378917, "score": 2, "content": "All good I understand I get it, sometimes what other people can say about your codebase can come off very ignorant and entitled (In my own experience people really don't understand how implementing new features/changes really works its not a simple process). A goal of mine personally is to try to minimize this by trying to also read the code itself. I think maybe previously I should have written out code to be more clear: For most of my projects I feel like I have to do this: import asyncio from scrapling.defaults import AsyncFetcher async def fetch_page(url, semaphore, delay_time): async with semaphore: try: page = await AsyncFetcher.get(url) await asyncio.sleep(delay_time) return page except Exception as e: print(f\"Error fetching {url}: {e}\") return None async def main(): urls = [ ' ' ... # hundreds of other urls, ] max_concurrent_tasks = 2 delay_time = 1.0 semaphore = asyncio.Semaphore(max_concurrent_tasks) tasks = [fetch_page(url, semaphore, delay_time) for url in urls] results = await asyncio.gather(*tasks) for result in results: if result: print(result.status) else: print(\"Failed to fetch page.\") if __name__ == \"__main__\": asyncio.run(main()) And I sometimes wish I could do this (ability to pass custom handling of concurrency code but possibly having defaults). \\`\\`\\` fetcher = AsyncFetcher(delay\\_func=custom\\_delay, max\\_concurrent\\_tasks=2, concurrency\\_control\\_func=custom\\_semaphore) \\`\\`\\` To be honest I think I know its not part of the spirit of the project (like you said its a library not a framework and honestly its not quite such a huge issue for me) but hey gotta put myself out there. Sometimes I need/want someone else to let me know whether its a good/bad/silly idea. xD Anyway I'll delete my previous comment too, also I really like the plan to add context managers to to handle sessions (it would be very convenient). Again overall good work honestly impressive understanding your situation further now. Cheers"}
{"id": "m2g17mc", "type": "comment", "parent_id": "t1_m2fzwqq", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2g17mc/", "author": "0xReaper", "created_utc": 1734408604, "score": 2, "content": "Ah great to hear that! I would love to hear your feedback after you test it :) Camoufox is used by one fetcher but the other fetcher is using playwright which might be faster on your device so consider giving it a try"}
{"id": "m3n4nxw", "type": "comment", "parent_id": "t1_m3ck2vj", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m3n4nxw/", "author": "None", "created_utc": 1735071198, "score": 1, "content": "[removed]"}
{"id": "m2jfsvg", "type": "comment", "parent_id": "t1_m2j4g70", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2jfsvg/", "author": "eenak", "created_utc": 1734463310, "score": 3, "content": "My bad, I dug through some of my own code and found that it wasn't the TextHandler type that was giving me problems; it was that I was trying to retrieve an attribute value using the .find() method rather than the .attrib dict, which was returning None, and I mistakenly assumed that the issue was the TextHandler not providing a compatible type to my other string parsing methods rather than the .find() not retrieving actual attribute values (resulting in a NoneType when it can't be found). I appreciate the help!"}
{"id": "m2dqhs2", "type": "comment", "parent_id": "t1_m2do951", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2dqhs2/", "author": "0xReaper", "created_utc": 1734379617, "score": 3, "content": "Oh, nice! now I understand you better! I was thinking a while ago of adding a way that does patch GET requests and maybe call it `patch_get` that takes a list of URLs, now since you brought this up I will add it in 0.3, and for the async version, I will add these options. Till then I guess you will need to do it manually haha :D"}
{"id": "m2jo5jw", "type": "comment", "parent_id": "t1_m2g17mc", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2jo5jw/", "author": "Queasy_Structure1922", "created_utc": 1734465944, "score": 1, "content": "I tested it with the stealth fetcher but the os\\_randomize option does not seem to work the tls handshake params should be randomized or am i missing something?"}
{"id": "m3t35va", "type": "comment", "parent_id": "t1_m3n4nxw", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m3t35va/", "author": "0xReaper", "created_utc": 1735172818, "score": 1, "content": "Those js files are for the stealth mode in PlaywrightFetcher. I was talking about StealthyFetcher which uses Camoufox!"}
{"id": "m2k11s6", "type": "comment", "parent_id": "t1_m2jo5jw", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2k11s6/", "author": "0xReaper", "created_utc": 1734470073, "score": 2, "content": "JA3 is a method for creating SSL/TLS client fingerprints so it has nothing to do with OS fingerprint randomizing."}
{"id": "m2lzs9w", "type": "comment", "parent_id": "t1_m2k11s6", "permalink": "https://www.reddit.com/r/webscraping/comments/1hfmul8/big_update_to_scrapling_library/m2lzs9w/", "author": "Queasy_Structure1922", "created_utc": 1734495793, "score": 2, "content": "Ya the ssl configs are not touched by these scraping browsers:/ I had issues scraping an heavily Akamai protected page and I\u2019m sure they were able to constantly rate limit me heavily due to ja3 fingerprints and the only way to circumvent these mechanics I could think of would to either map ja3 fingerprints to used agents and then intercept tls handshakes with mitm proxy to match the user agent or to build a custom browser that allows to modify the tls handshake to match the user agent spoofs. Some akamai researcher released a paper recently on how they use ja3 and implementation differences in browser / os combinations to detect spoofed user agents, haven\u2019t found any open source tool so far that can beat this. No one else struggling with this?"}
{"id": "1kxab2n", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/", "author": "aaronn2", "created_utc": 1748414468, "score": 85, "title": "Websites provide fake information when detected crawlers", "content": "There are firewall/bot protections websites use when they detect crawling activities on their websites. I started recently dealing with situations when websites instead of blocking you access to the website, they keep you crawling, but they quietly replace the information on the website for fake ones - an example are e-commerce websites. When they detect a bot activity, they change the price of product, so instead of $1,000, it costs $1,300. I don't know how to deal with these situations. One thing is to be completely blocked, another one when you are \"allowed\" to crawl, but you are given false information. Any advice?"}
{"id": "muo1cbe", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muo1cbe/", "author": "ScraperAPI", "created_utc": 1748422391, "score": 33, "content": "We've encountered this a few times before. There's a couple of things you can do: 1. Look for differences in HTML between a \"bad\" page and a \"good\" version of the same page. If you're lucky, you can isolate the difference and ignore \"bad\" pages. 2. Use a good residential proxy - IP address reputation is a big giveaway to cloudflare. 3. Use an actual browser, so the \"signature\" of your request looks as much like a real person browsing as possible. You can use puppeteer or playwright for this, but make sure you use something that explicitly defeats bot detection. You might need to throw in some mouse movements as well. 4. Slow down your requests - it's easy to detect you if you send multiple requests from the same IP address concurrently or too quickly. 5. Don't go directly to the page you need data from - establish a browsing history with the proxy you're using. If you're looking to get a lot of data, you can still do this by sending multiple requests at the same time using multiple proxies."}
{"id": "munrhz1", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/munrhz1/", "author": "MindentMegmondok", "created_utc": 1748416541, "score": 19, "content": "Seems like you're facing with cloudflare's [AI labyrint]( If this is the case, the only solution would be to avoid being detected, which could be pretty tricky as they are using AI not just to generate fake results, but for the detection process too."}
{"id": "munpt1s", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/munpt1s/", "author": "fukato", "created_utc": 1748415561, "score": 8, "content": "Try posing as a real customer and asking about weird price changes. But yeah tough luck for this case."}
{"id": "muozsqr", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muozsqr/", "author": "jinef_john", "created_utc": 1748437898, "score": 4, "content": "I haven\u2019t encountered this situation yet, but I can imagine having some kind of \u201ctrue\u201d reference data \u2014 either before I begin scraping or after a few initial requests \u2014 where I\u2019d visit a known, reliable page and compare it with the scraped results to check for inconsistencies. Or just revisit the same page and see if it matches what's expected with \"true\" data. So that it acts as some form of validation. Ultimately, I believe the main focus should be on avoiding detection. One of the most common \u2014 and often overlooked \u2014 pitfalls is honeypot traps. You should always inspect the page for hidden elements by checking CSS styles and visibility. Bots that interact with these elements can easily get flagged (almost always). So avoid clicking or submitting any hidden fields or links, because falling for a honeypot will just lead to waste of resources or getting blocked too."}
{"id": "mups2dj", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mups2dj/", "author": "Defiant_Alfalfa8848", "created_utc": 1748446355, "score": 4, "content": "Ow wow that is a genius move for whoever came up with it."}
{"id": "munorqu", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/munorqu/", "author": "DutchBytes", "created_utc": 1748414982, "score": 3, "content": "Maybe try crawling using a real browser?"}
{"id": "muprsnb", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muprsnb/", "author": "REDI02", "created_utc": 1748446278, "score": 2, "content": "I am facing same problem. Did you find any solution?"}
{"id": "mvbqtcr", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mvbqtcr/", "author": "JalapenoLemon", "created_utc": 1748733184, "score": 2, "content": "This is called poison pilling and we have been doing it to web scrapers and AI bots."}
{"id": "muqu67w", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muqu67w/", "author": "None", "created_utc": 1748457363, "score": 1, "content": "[removed]"}
{"id": "mv4bflw", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mv4bflw/", "author": "welcome_to_milliways", "created_utc": 1748632085, "score": 1, "content": "We discovered a certain well known website doing this some years ago. You\u2019d scrape the first dozen profiles and anything after that was fictitious. We didn\u2019t notice for weeks"}
{"id": "mv7xl8l", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mv7xl8l/", "author": "TheDiamondCG", "created_utc": 1748684817, "score": 1, "content": "If someone is doing this, make sure you are respecting the robots.txt on their website. You are probably costing them a lot of money (especially when it comes to dynamic content). I\u2019m active in the free open-source software space and web scrapers have become an ***actual scourge*** on so, so many of the providers for source code. Scrapers hit these really expensive endpoints and end up multiplying the costs of running things by up to 60x or even more in the worst cases. If a website is doing this to you, then stop scraping them. They tried blocking scrapers the normal way, so then instead of respecting the host\u2019s wishes, the scrapers decided to circumvent those measures. You might think that you\u2019re just one person/organization, but the scale and quantity of people scraping means that there can be 600+ bot visits for every 1 human visiting. TL;DR: Providing fake info is a desperate last measure, nobody goes there unless they *really* have to go there. Stop scraping those websites. Respect robots.txt"}
{"id": "muodqgr", "type": "comment", "parent_id": "t3_1kxab2n", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muodqgr/", "author": "pauldm7", "created_utc": 1748429222, "score": -1, "content": "I second the post above. Make some fake emails and email the company every few days from different customers, ask them why the price keeps changing and it\u2019s unprofessional and you\u2019re not willing to buy at the higher price. Maybe they disable it, maybe they don\u2019t."}
{"id": "mur460x", "type": "comment", "parent_id": "t1_muo1cbe", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mur460x/", "author": "ColoRadBro69", "created_utc": 1748460212, "score": 7, "content": "> Use an actual browser, so the \"signature\" of your request looks as much like a real person browsing as possible. If I was running a website and wanted to \"poison the results\" for scrapers like this instead of just blocking them. I would need a way to identify which is which. If somebody was always requesting the HTML where all the info is, but never the CSS and scripts and images and all the things a real user needs to see the page, that would be a dead give away. I'm posting to clarify for others who aren't sure what you mean."}
{"id": "murrk8v", "type": "comment", "parent_id": "t1_muo1cbe", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/murrk8v/", "author": "Atomic1221", "created_utc": 1748466811, "score": 4, "content": "We do 5 but I don\u2019t think it explicitly has to be using your proxy. Your proxy may be bad sure, any you can test for that right away, but rather the browsing on your specific browser session is what\u2019s important. I say this because you\u2019ll be wasting a lot of bandwidth by building trust score on your proxy when it can be done without. You can even import the browsing history and then just do one or two new searches and you\u2019re in decent shape."}
{"id": "muns5yr", "type": "comment", "parent_id": "t1_munrhz1", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muns5yr/", "author": "aaronn2", "created_utc": 1748416935, "score": 1, "content": "Interesting - thanks, I'll have a read."}
{"id": "muq1hga", "type": "comment", "parent_id": "t1_munrhz1", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muq1hga/", "author": "Klutzy_Cup_3542", "created_utc": 1748449029, "score": 1, "content": "I came across this in cloud flare on my SEO site audit software and I was told it is only for bots not respecting the robot.txt. Is this the case? My SEO software found it via a footer."}
{"id": "mv6h9vv", "type": "comment", "parent_id": "t1_mups2dj", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mv6h9vv/", "author": "carbon_splinters", "created_utc": 1748657966, "score": 1, "content": "Cloudflare has been killing it lately."}
{"id": "munp9fc", "type": "comment", "parent_id": "t1_munorqu", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/munp9fc/", "author": "aaronn2", "created_utc": 1748415255, "score": 1, "content": "That is very short-lived. It works only for the first couple of pages and then it starts feeding fake data."}
{"id": "mur0kz5", "type": "comment", "parent_id": "t1_muqu67w", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mur0kz5/", "author": "webscraping-ModTeam", "created_utc": 1748459193, "score": 1, "content": "Welcome to the r/webscraping community. This sub is focused on addressing the technical aspects of implementing and operating scrapers. We're not a marketplace, nor are we a platform for selling services or datasets. You're welcome to post in the [monthly thread]( or try your request on Fiverr or Upwork. For anything else, please contact the mod team."}
{"id": "mv4qat0", "type": "comment", "parent_id": "t1_mv4bflw", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mv4qat0/", "author": "aaronn2", "created_utc": 1748636522, "score": 1, "content": "How did you eventually resolve this?"}
{"id": "muomuts", "type": "comment", "parent_id": "t1_muodqgr", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muomuts/", "author": "UnnamedRealities", "created_utc": 1748433189, "score": 1, "content": "Companies that implement deception technology typically do very extensive testing and tuning before initial deployment and after feature/config changes to ensure that it is highly unlikely that legitimate non-malicious human activity is impacted. They also typically maintain extensive analytics so they can assess the efficacy of the deployment and investigate if customers report issues. The company OP whose site OP is scraping could be an exception, but I suspect it would be a better use of OP's time to determine how to fly under the radar and how to identify when the deception controls have been triggered."}
{"id": "mv7y3z3", "type": "comment", "parent_id": "t1_muodqgr", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mv7y3z3/", "author": "TheDiamondCG", "created_utc": 1748685130, "score": -1, "content": "You guys have no shame. Why do companies even use deception in the first place? 1. Individual sets up robots.txt to tell scrapers NOT to touch really expensive endpoints 2. Scrapers do not respect this, so individual blocks common scrapers 3. Scrapers circumvent this, so individual (who is now losing a lot of money), is forced to use deception tactics 4. \u2026 now you want to\u2026 cost them even more?? It\u2019s not just big corporations who can take the loss anyways that use deception. There are lots of grass-roots organizations (especially software freedom initiatives) that get financially hurt really badly by what you\u2019re trying to do. Please respect robots.txt."}
{"id": "mvx3eyi", "type": "comment", "parent_id": "t1_mur460x", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mvx3eyi/", "author": "ScraperAPI", "created_utc": 1749026678, "score": 1, "content": "thank you so much for that clairification!"}
{"id": "mvx3crq", "type": "comment", "parent_id": "t1_murrk8v", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mvx3crq/", "author": "ScraperAPI", "created_utc": 1749026642, "score": 1, "content": "fair point."}
{"id": "mur4lpq", "type": "comment", "parent_id": "t1_muq1hga", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mur4lpq/", "author": "ColoRadBro69", "created_utc": 1748460336, "score": 3, "content": "> My SEO software found it via a footer. The way it works is by hiding a link (apparently in the footer) that's prohibited in the robots file. It's a trap, in other words. It's invisible and a human won't click because they won't see it. Only a bot that ignores robots.txt will find it. That's what they're doing."}
{"id": "munpmmy", "type": "comment", "parent_id": "t1_munp9fc", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/munpmmy/", "author": "amazingbanana", "created_utc": 1748415459, "score": 5, "content": "you might be crawling too fast if it works for a few pages and then stops"}
{"id": "muo7vk7", "type": "comment", "parent_id": "t1_munp9fc", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/muo7vk7/", "author": "DutchBytes", "created_utc": 1748426191, "score": 1, "content": "Find out how many you can crawl and then use different IP adresses. Slowing down might help too"}
{"id": "mupb2dp", "type": "comment", "parent_id": "t1_muomuts", "permalink": "https://www.reddit.com/r/webscraping/comments/1kxab2n/websites_provide_fake_information_when_detected/mupb2dp/", "author": "OkTry9715", "created_utc": 1748441488, "score": 1, "content": "Cloudfare will throw you captcha if you are using extensions that block tackers like Ghostery."}
{"id": "1lp3mf0", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/", "author": "madredditscientist", "created_utc": 1751381367, "score": 83, "title": "Cloudflare to introduce pay-per-crawl for AI bots", "content": ""}
{"id": "n0rplzy", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0rplzy/", "author": "Hour_Analyst_7765", "created_utc": 1751382132, "score": 31, "content": "If you can't beat them, join them. That is what it sounds like to me. Data=money, so as Cloudflare provides bot/ddos protection, they are the gatekeepers which revenue stream is extracted by which kind of visitor. Human=ads, bots=hassle free access"}
{"id": "n0sjytt", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0sjytt/", "author": "Purkinje90", "created_utc": 1751390531, "score": 25, "content": "By playing both sides, they always come out on top."}
{"id": "n0slqd5", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0slqd5/", "author": "None", "created_utc": 1751391003, "score": 10, "content": "Surely the people who used pirated content to train their AI will respect this and pay up instead of just using selenium or similar /s"}
{"id": "n0t2jrk", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0t2jrk/", "author": "Directive31", "created_utc": 1751395654, "score": 5, "content": "Nice. DRM v2025. You can read this page, you can coy & paste it, you can screenshot it.. you can even download it, but crawl it? f no. I get some of the intent.. If you're a pub and you have ads on your site, having this content resurface on a different site, and monetized there (chatgpt) tastes bitter. This will harm smaller pubs and favor larger ones and drive more content consolidation / balkanization. Cool... Thanks for making the internet that much better cf while extracting from it. I think we know who the real middle man is.."}
{"id": "n0rpwnl", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0rpwnl/", "author": "amemingfullife", "created_utc": 1751382220, "score": 3, "content": "I hope they have a license fee with the companies they\u2019re protecting."}
{"id": "n0txl4h", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0txl4h/", "author": "kuta2599", "created_utc": 1751404547, "score": 3, "content": "Webscrapers have abused internet freedoms."}
{"id": "n0rx0qn", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0rx0qn/", "author": "Classic-Dependent517", "created_utc": 1751384185, "score": 1, "content": "Hmmm so crawlers also need to sign up for cloudflare\u2026 who will?"}
{"id": "n0s37t4", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0s37t4/", "author": "BotBarrier", "created_utc": 1751385893, "score": 1, "content": "What rights are conveyed to the AI vendors that pay the fee? For the record, I have a conflicted interest with Cloudflare as my company is a competitor."}
{"id": "n1gh64c", "type": "comment", "parent_id": "t3_1lp3mf0", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n1gh64c/", "author": "hmnguyen87", "created_utc": 1751716704, "score": 1, "content": "Wouldn\u2019t this just deter companies from using them for protection?"}
{"id": "n0v03xa", "type": "comment", "parent_id": "t1_n0rpwnl", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0v03xa/", "author": "Directive31", "created_utc": 1751417220, "score": 2, "content": "duh making money on both sides and squeezing where they can (or will) - what s a monopoly and vertically integrated business about otherwise?"}
{"id": "n0uzrm1", "type": "comment", "parent_id": "t1_n0txl4h", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0uzrm1/", "author": "Directive31", "created_utc": 1751417100, "score": 2, "content": "can you expand on what you mean? want to better understand the thought process not that \"webscraping bad\" is hard to grasp as a message but... maybe one layer deeper. new here, and genuinely curious."}
{"id": "n0thn5c", "type": "comment", "parent_id": "t1_n0s37t4", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0thn5c/", "author": "Directive31", "created_utc": 1751400002, "score": 3, "content": "Good for you. Are you going after a different tier of publishers (they pretty much own all large publishers + bandwagon mentality in that tier)? Or on a specialized feature set?"}
{"id": "n0t0kgp", "type": "comment", "parent_id": "t1_n0s37t4", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0t0kgp/", "author": "outceptionator", "created_utc": 1751395095, "score": 2, "content": "Man I think Cloudflare is so good as a dev focused company (don't use them). What do you guys do?"}
{"id": "n1notpx", "type": "comment", "parent_id": "t1_n1gh64c", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n1notpx/", "author": "Prospector2", "created_utc": 1751819339, "score": 1, "content": "It depends on which side Cloudflare is on."}
{"id": "n0wps2y", "type": "comment", "parent_id": "t1_n0v03xa", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0wps2y/", "author": "amemingfullife", "created_utc": 1751444588, "score": 4, "content": "Yeah it\u2019s basically a mafia protection racket at this point. \u201cAwful nice website you got here mate, would be an awful shame if some AI crawlers were to get a hold of your data and use it to train LLMs. Listen, I\u2019ve got a little idea - why don\u2019t I help you out here. Why don\u2019t I help you on your feet? I\u2019ll handle the nasty AI scrapers and you and the wife can rest easy at night. I\u2019ll take a small fee, of course, someone\u2019s got to pay our developers, they\u2019re so young and talented and poor, someone\u2019s got to help them, right? There\u2019s a good lad, you wouldn\u2019t want to hurt the developers of course. Now, there might be a day, and that day may never come, where we will need to scrape your data too. I might need a few of my friends and associates to get involved too. But rest easy, your website is always protected.\u201d"}
{"id": "n0wsz1l", "type": "comment", "parent_id": "t1_n0uzrm1", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0wsz1l/", "author": "kuta2599", "created_utc": 1751446495, "score": 4, "content": "For decades search bots crawled the web without creating major problems and it was tolerated. AI powered webscrapers are literally hammering web sites far beyond what traditional search engine bots did for decades. Never mind the issue of vacuuming up content & reselling it to consumers without permission or payment. As a web site creator the issue of being virtually ddoss'ed by ai scrapers is by far the most pressing issue."}
{"id": "n0u2vq4", "type": "comment", "parent_id": "t1_n0thn5c", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0u2vq4/", "author": "None", "created_utc": 1751406143, "score": 1, "content": "[removed]"}
{"id": "n0tw7z8", "type": "comment", "parent_id": "t1_n0t0kgp", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0tw7z8/", "author": "None", "created_utc": 1751404145, "score": 2, "content": "[removed]"}
{"id": "n0xknws", "type": "comment", "parent_id": "t1_n0wps2y", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0xknws/", "author": "Directive31", "created_utc": 1751459686, "score": 1, "content": "I understood the first sentence which yes, I agree it is the biz model essentially. Kinda lost me on the rest. They are a good infra provider. They are great at selling it to publishers. That's all good. Now when they reach both ways to the consumers of what comes thru their pipes is when it gets dicey. Good reason why the guy owning the power lines is almost NEVER the guy billing the end users almost anywhere in the reasonable world. Can't wait to see what side the regulators will take."}
{"id": "n0xnsfz", "type": "comment", "parent_id": "t1_n0wsz1l", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0xnsfz/", "author": "Directive31", "created_utc": 1751460802, "score": 1, "content": "I mean if you're one of these companies it makes very little sense to harass site owners. it costs them money, reputation, legal headaches, etc... list goes on. That eng that fucked the crawler rate limit? gone. It's not like openai is anxious to spend their money on retraining their whole model daily.... so other than anecdotes I'm not readily buying the naive headline here. certainly not coming from the folks SELLING the \"protection \" (lol) If your site can't take a few extra pings across its pages monthly.. idk what to tell you"}
{"id": "n0wz8y5", "type": "comment", "parent_id": "t1_n0wsz1l", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0wz8y5/", "author": "Warguy387", "created_utc": 1751450134, "score": 0, "content": "scrapers aren't the same as crawlers imo but yeah I mean its your fault for not calculating for your backend requests idk what to say"}
{"id": "n0wzert", "type": "comment", "parent_id": "t1_n0wsz1l", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0wzert/", "author": "Warguy387", "created_utc": 1751450222, "score": 0, "content": "also I don't know how economical these \"ai web crawlers\" are to be honest. I don't really see them as a problem for high frequency/large volume scraping"}
{"id": "n0u5lln", "type": "comment", "parent_id": "t1_n0u2vq4", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0u5lln/", "author": "Directive31", "created_utc": 1751406967, "score": 3, "content": "aw shit.. mod not gonna like that one :) but interesting positioning. tough market. small pubs have no money to spare. unless you make them make money hand over fist it's probably a hard sell."}
{"id": "n0ulbk0", "type": "comment", "parent_id": "t1_n0u2vq4", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0ulbk0/", "author": "webscraping-ModTeam", "created_utc": 1751412086, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "n0ulmpf", "type": "comment", "parent_id": "t1_n0tw7z8", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0ulmpf/", "author": "webscraping-ModTeam", "created_utc": 1751412187, "score": 1, "content": "Welcome to r/webscraping! Referencing paid products or services is not permitted, and your post has been removed. Please take a moment to review the [promotion guide]( You may also wish to re-submit your post to the [monthly thread]("}
{"id": "n0xp3du", "type": "comment", "parent_id": "t1_n0xnsfz", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0xp3du/", "author": "Directive31", "created_utc": 1751461254, "score": 1, "content": "though i'm sure there's a host of wannabes attempting it. Yeah they can be a nuisance. Still none of them want to recrawl your site 100times over. The harder you make it and force them to scrape all the heavier loading items, the costlier it gets for you? also it costs $100M+ (or much much more) to train a proper large scale LLM... Just saying..."}
{"id": "n0xx0fe", "type": "comment", "parent_id": "t1_n0wzert", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0xx0fe/", "author": "kuta2599", "created_utc": 1751463876, "score": 1, "content": "For example: \"Amazon's AI crawler is making my git server unstable\""}
{"id": "n0up08v", "type": "comment", "parent_id": "t1_n0u5lln", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0up08v/", "author": "BotBarrier", "created_utc": 1751413306, "score": 1, "content": "It is a tough market. And yup, looks like the mods didn\u2019t like my posts. Sorry all."}
{"id": "n0ux1ma", "type": "comment", "parent_id": "t1_n0up08v", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0ux1ma/", "author": "Directive31", "created_utc": 1751416120, "score": 2, "content": "I think they are just doing their jobs. I'm new here but tbh prefer that over what i saw in other sub even tho (and esp since) many on here have a project or company to promote..."}
{"id": "n0v9kst", "type": "comment", "parent_id": "t1_n0ux1ma", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0v9kst/", "author": "BotBarrier", "created_utc": 1751420581, "score": 1, "content": "Absolutely. I should have read the rules before posting\u2026"}
{"id": "n0vaz3b", "type": "comment", "parent_id": "t1_n0v9kst", "permalink": "https://www.reddit.com/r/webscraping/comments/1lp3mf0/cloudflare_to_introduce_paypercrawl_for_ai_bots/n0vaz3b/", "author": "Directive31", "created_utc": 1751421059, "score": 1, "content": "you and me was my very first move... on the whole of reddit as a matter if fact. I'm okay with accepting the mods have me in their \"what a shithead\" list - praise be the mods for their watchful eye"}
{"id": "1ktd870", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/", "author": "musaspacecadet", "created_utc": 1747983747, "score": 83, "title": "It's not even my repo, it's a fork!", "content": "This should confirm all the fears I had, if you write a new bypass for any bot detection or captcha wall, don't make it public they scan the internet to find and patch them, let's make it harder"}
{"id": "mtsn3r7", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtsn3r7/", "author": "divided_capture_bro", "created_utc": 1747984024, "score": 40, "content": "Just post all the code here raw."}
{"id": "mtsntn7", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtsntn7/", "author": "Rungk4d", "created_utc": 1747984439, "score": 19, "content": "post code raw"}
{"id": "mtt5wkz", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtt5wkz/", "author": "InterestingStick", "created_utc": 1747995226, "score": 8, "content": "Interesting, I checked the actual notice at (wonder who made that typo in cloudflare, github or the party representing cloudflare lol) Seems like they target the source as well as all the forks, which makes sense cause each fork contains the source code of the original in one way or another The basis of the takedown is not the circumvention of cloudflares bot detection, it is (as it states) because the repository contains code that is under the copyright of cloudflare I do not know and do not have the means to check if what they allege is true, I'm just trying to interpret the notice itself"}
{"id": "mtsv35e", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtsv35e/", "author": "ToreGore", "created_utc": 1747988803, "score": 8, "content": "Code raw please"}
{"id": "mtvluny", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtvluny/", "author": "viciousDellicious", "created_utc": 1748023968, "score": 3, "content": "I actually had a fork on this project and i had implemented more features: API/docker/etc i havent received a dmca notice yet xD"}
{"id": "mub18oe", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mub18oe/", "author": "Dapper-Profession552", "created_utc": 1748244645, "score": 3, "content": "I'm LOBYXLYX, I didn't expect that Cloudflare employee to report my repository if it's literally just a simple cf_clearance extractor lol"}
{"id": "mtsza2h", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtsza2h/", "author": "Dreamin0904", "created_utc": 1747991374, "score": 5, "content": "Did someone say raw code?"}
{"id": "mtt9ff3", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtt9ff3/", "author": "thatsbutters", "created_utc": 1747997086, "score": 4, "content": "You can write them all day and no one cares.... This is about publishing source code under copyright. Also, forking isn't some magic trick to remove licensing."}
{"id": "mttb4j1", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mttb4j1/", "author": "_iamhamza_", "created_utc": 1747997903, "score": 2, "content": "The fact that Cloudflare contacted GitHub to take down your repository..."}
{"id": "mttz9p8", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mttz9p8/", "author": "JohnnyOmmm", "created_utc": 1748007114, "score": 2, "content": "Can we have a copy of the code bro?"}
{"id": "mtul1b3", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtul1b3/", "author": "InformalWar8615", "created_utc": 1748013566, "score": 2, "content": "self host git alternativetime"}
{"id": "mtufehv", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtufehv/", "author": "SoleymanOfficial", "created_utc": 1748011969, "score": 1, "content": "Got the same one"}
{"id": "mtw2pr1", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtw2pr1/", "author": "anotherucfstudent", "created_utc": 1748029068, "score": 1, "content": "Just paste the code raw bro"}
{"id": "mu4bcst", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mu4bcst/", "author": "elixon", "created_utc": 1748146015, "score": 1, "content": "The Streisand effect in the making?"}
{"id": "mtu23z7", "type": "comment", "parent_id": "t3_1ktd870", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtu23z7/", "author": "ProfessionIntrepid96", "created_utc": 1748008025, "score": 1, "content": "where"}
{"id": "mtxjd29", "type": "comment", "parent_id": "t1_mtt5wkz", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtxjd29/", "author": "HillaryPutin", "created_utc": 1748046695, "score": 2, "content": "Rehost it with patches of cloudflare source code. Also this repo was seriously legendary. He basically plucked out all the client-side js from cloudflare and rehosted it in an emulated js DOM. Glad I saved a recent copy of it."}
{"id": "mtxvpvn", "type": "comment", "parent_id": "t1_mtvluny", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtxvpvn/", "author": "Nasa1423", "created_utc": 1748051430, "score": 1, "content": "Any luck to share a repo url?"}
{"id": "mwgka02", "type": "comment", "parent_id": "t1_mub18oe", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mwgka02/", "author": "Melodic_Choice_1594", "created_utc": 1749285273, "score": 1, "content": "is it possible to share source code here, please"}
{"id": "n91ad33", "type": "comment", "parent_id": "t1_mub18oe", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/n91ad33/", "author": "BeforeICry", "created_utc": 1755363343, "score": 1, "content": "Can we look into this further if you still got the repo?"}
{"id": "nbqh7dq", "type": "comment", "parent_id": "t1_mub18oe", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/nbqh7dq/", "author": "LeNRPC", "created_utc": 1756676981, "score": 1, "content": "Interested as well for a re-up. You might want to dig into GETting the js file somehow, as other people suggested instead of storing it in your repo. This makes a good reason for them to take it down as their code should be protected by copyright (other similar repos do not store CF code and don't seem to have this kind of issues). Thanks for your work anyway!"}
{"id": "mtwnueg", "type": "comment", "parent_id": "t1_mttb4j1", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtwnueg/", "author": "Typical-Armadillo340", "created_utc": 1748035570, "score": 1, "content": "They didnt initally contact to take down his repo but the main one. They probably wrote a script to list also all forks that were created at that time. This is like almost little to no effort."}
{"id": "mtxwlj9", "type": "comment", "parent_id": "t1_mtxvpvn", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtxwlj9/", "author": "viciousDellicious", "created_utc": 1748051755, "score": 7, "content": "["}
{"id": "nbqiq0f", "type": "comment", "parent_id": "t1_nbqh7dq", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/nbqiq0f/", "author": "Dapper-Profession552", "created_utc": 1756677478, "score": 1, "content": "I already did this, I made a new cloudflare bypass that doesn't use cloudflare's js code, it's all done in py, I would upload it but I don't want to get another DMCA"}
{"id": "mu9l0jr", "type": "comment", "parent_id": "t1_mtwnueg", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mu9l0jr/", "author": "tankerkiller125real", "created_utc": 1748221001, "score": 1, "content": "Github just has a checkbox the copyright owner can use to also take down all the forks. Given that Github as a very simple DB link between forks and the original repo it's not that hard for them to take all of them out at once. The only hard work for a copyright owner is finding forks that didn't use the fork repo button."}
{"id": "mtxz8up", "type": "comment", "parent_id": "t1_mtxwlj9", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtxz8up/", "author": "InterestingStick", "created_utc": 1748052758, "score": 2, "content": "jesus that really is a carbon copy of cloudflares code, no wonder they took it down"}
{"id": "mtxwthx", "type": "comment", "parent_id": "t1_mtxwlj9", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mtxwthx/", "author": "Nasa1423", "created_utc": 1748051838, "score": 1, "content": "Thanks a lot!"}
{"id": "nbqlekc", "type": "comment", "parent_id": "t1_nbqiq0f", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/nbqlekc/", "author": "None", "created_utc": 1756678392, "score": 1, "content": "[removed]"}
{"id": "mty081s", "type": "comment", "parent_id": "t1_mtxz8up", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mty081s/", "author": "None", "created_utc": 1748053139, "score": 3, "content": "[deleted]"}
{"id": "nbqngno", "type": "comment", "parent_id": "t1_nbqlekc", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/nbqngno/", "author": "Dapper-Profession552", "created_utc": 1756679111, "score": 1, "content": "Alright"}
{"id": "nbqy56c", "type": "comment", "parent_id": "t1_nbqlekc", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/nbqy56c/", "author": "webscraping-ModTeam", "created_utc": 1756683024, "score": 1, "content": "Please review the sub rules"}
{"id": "much20a", "type": "comment", "parent_id": "t1_mty081s", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/much20a/", "author": "devils-advocacy", "created_utc": 1748269265, "score": 1, "content": "Commenting so I can follow up if necessary. Thank you for sharing your repo!"}
{"id": "mugyxpe", "type": "comment", "parent_id": "t1_mty081s", "permalink": "https://www.reddit.com/r/webscraping/comments/1ktd870/its_not_even_my_repo_its_a_fork/mugyxpe/", "author": "Pale_Ad_6029", "created_utc": 1748325480, "score": 1, "content": "commenting same reason \\^"}
{"id": "1j395h5", "type": "submission", "permalink": "https://www.reddit.com/r/webscraping/comments/1j395h5/detecting_proxies_serverside_using_tcp_handshake/", "author": "CoinsHost", "created_utc": 1741089513, "score": 82, "title": "Detecting proxies server-side using TCP handshake latency?", "content": "I've recently came across this concept that detects proxies and VPNs by comparing the TCP handshake time and RTT using Websocket. If these two times do not match up, it could mean that a proxy is being used. Here's the concept: [ Most VPN and proxy detection APIs rely on IP databases, but here's the two real-world implementations of the concept that I found: * [ (original concept - check the \"Latency Test\") * [ (seems to use the very same detection idea) From my tests, both tests are pretty accurate when it comes to detecting proxies (100% detection rate actually) but not so precise when it comes to VPNs. It may also spawn false-positives even on direct connection some times, I guess due to networking glitches. I am curious if others have tried this approach or have any thoughts on its reliability when detecting proxied requests based on TCP handshake latency, or have your proxied scrapers ever been detected and blocked supposedly using this approach? Do you think this method is worth putting into consideration?"}
